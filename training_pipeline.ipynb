{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3eaf3d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, f1_score, roc_auc_score,\n",
    "    mean_absolute_error, mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, OneHotEncoder, LabelEncoder, TargetEncoder\n",
    ")\n",
    "\n",
    "# Deep learning imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "\n",
    "# Model-specific imports\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from GRANDE import GRANDE\n",
    "from tabpfn import TabPFNClassifier, TabPFNRegressor\n",
    "\n",
    "# Optimization imports\n",
    "import optuna\n",
    "\n",
    "# Data source imports\n",
    "import openml\n",
    "\n",
    "# Abstract base class imports\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Literal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae3463",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf7565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    Set random seeds for reproducibility across Python, NumPy, PyTorch, and TensorFlow.\n",
    "\n",
    "    Args:\n",
    "        seed: Integer seed for random number generators\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "def setup_logging() -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Set up logging configuration.\n",
    "    This function configures the logging settings for the application, including\n",
    "    the logging level and format. It returns a logger instance that can be used\n",
    "    throughout the application.\n",
    "    Returns:\n",
    "        logging.Logger: Configured logger instance.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def check_GPU_availability():\n",
    "    \"\"\"\n",
    "    Checks if a GPU is available and configures TensorFlow to use it appropriately.\n",
    "\n",
    "    Sets up memory growth for TensorFlow GPU usage to avoid allocating all GPU memory at once.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: A torch device object ('cuda' if GPU is available, 'cpu' otherwise)\n",
    "    \"\"\"\n",
    "    logger = setup_logging()\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"Using GPU: {torch.cuda.get_device_name()}\")\n",
    "        device = torch.device(\"cuda\")\n",
    "        gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        logger.info(\"Using CPU for training.\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14164da",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3635ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target(y):\n",
    "    \"\"\"\n",
    "    Encode the target variable using LabelEncoder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array\n",
    "        The target variable to encode.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_encoded : np.array\n",
    "        The encoded target variable.\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    return y_encoded\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_val: pd.DataFrame,\n",
    "    cat_cols: list,\n",
    "    config: dict,\n",
    "    preprocessing_type: str,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Preprocess the training and validation datasets based on the specified model type.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : pandas.DataFrame\n",
    "        The training dataset features.\n",
    "    y_train : pandas.Series or np.array\n",
    "        The target variable for the training dataset.\n",
    "    X_val : pandas.DataFrame\n",
    "        The validation dataset features.\n",
    "    cat_cols : list or array of bool\n",
    "        Boolean mask indicating which columns in X_train and X_val are categorical (True).\n",
    "    config : dict\n",
    "        Configuration dictionary containing model and preprocessing details.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train : np.array\n",
    "        The preprocessed training dataset features.\n",
    "    X_val : np.array\n",
    "        The preprocessed validation dataset features.\n",
    "    \"\"\"\n",
    "    X_train, preprocessor_inner = _minimal_preprocess_train(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cat_cols,\n",
    "        preprocessing_type,\n",
    "        config,\n",
    "    )\n",
    "    X_val = _minimal_preprocess_test(X_val, preprocessor_inner)\n",
    "\n",
    "    return X_train, X_val\n",
    "\n",
    "\n",
    "def _minimal_preprocess_train(X, y, categorical_features, model_type, config):\n",
    "    \"\"\"\n",
    "    Perform minimal preprocessing on the input features for training.\n",
    "\n",
    "    This function applies preprocessing to both numeric and categorical features\n",
    "    based on the model type specified in the configuration. For Neural Networks,\n",
    "    it standardizes numeric features and one-hot encodes categorical features. For\n",
    "    tree-based models, it does not scale numeric features and applies different\n",
    "    encoding strategies for low- and high-cardinality categorical features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pandas.DataFrame\n",
    "        The input features.\n",
    "\n",
    "    categorical_features : list or array of bool\n",
    "        Boolean mask indicating which columns in X are categorical (True) and\n",
    "        which are numeric (False).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_preprocessed : np.array\n",
    "        The preprocessed features.\n",
    "    preprocessor : ColumnTransformer\n",
    "        The fitted preprocessor, which can be used to transform future datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the boolean mask to a NumPy array (if not already) for fast indexing.\n",
    "    cat_mask = np.asarray(categorical_features)\n",
    "\n",
    "    # Check that the mask length matches the number of columns in X.\n",
    "    if cat_mask.shape[0] != X.shape[1]:\n",
    "        raise ValueError(\n",
    "            \"Length of categorical_features mask must match the number of columns in X.\"\n",
    "        )\n",
    "\n",
    "    # Extract column names using boolean indexing.\n",
    "    categorical_feature_names = X.columns[cat_mask].tolist()\n",
    "    numeric_feature_names = X.columns[~cat_mask].tolist()\n",
    "\n",
    "    # Select pipeline based on config.\n",
    "    logger = setup_logging()\n",
    "\n",
    "    # Neural Networks: standardize numeric features and one-hot encode categoricals.\n",
    "    if model_type == \"nn\":\n",
    "\n",
    "        # Define the preprocessing pipeline for numeric features.\n",
    "        numeric_pipeline = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "                (\"scaler\", StandardScaler()),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Define the preprocessing pipeline for categorical features.\n",
    "        categorical_pipeline = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\n",
    "                    \"onehot\",\n",
    "                    OneHotEncoder(\n",
    "                        drop=\"first\", sparse_output=False, handle_unknown=\"ignore\"\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        transformers = [\n",
    "            (\"num\", numeric_pipeline, numeric_feature_names),\n",
    "            (\"cat\", categorical_pipeline, categorical_feature_names),\n",
    "        ]\n",
    "\n",
    "        logger.info(\"Preprocessing pipeline for Neural Network model.\")\n",
    "\n",
    "    elif model_type == \"tree\":\n",
    "\n",
    "        # Tree-based models: leave numeric features unscaled, and differentiate encoding for categoricals.\n",
    "        numeric_pipeline = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            ]\n",
    "        )\n",
    "        # Split categorical features into low- and high-cardinality groups.\n",
    "        threshold = config[\"preprocessing\"][\"threshold_high_cardinality\"]\n",
    "        low_card_features = []\n",
    "        high_card_features = []\n",
    "        for feature in categorical_feature_names:\n",
    "            if X[feature].nunique() <= threshold:\n",
    "                low_card_features.append(feature)\n",
    "            else:\n",
    "                high_card_features.append(feature)\n",
    "\n",
    "        # For low-cardinality categoricals, use one-hot encoding.\n",
    "        low_card_pipeline = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\n",
    "                    \"onehot\",\n",
    "                    OneHotEncoder(\n",
    "                        drop=\"first\", sparse_output=False, handle_unknown=\"ignore\"\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # For high-cardinality categoricals, use target encoding.\n",
    "        high_card_pipeline = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"target_encode\", TargetEncoder()),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        transformers = [\n",
    "            (\"num\", numeric_pipeline, numeric_feature_names),\n",
    "            (\"low_card\", low_card_pipeline, low_card_features),\n",
    "            (\"high_card\", high_card_pipeline, high_card_features),\n",
    "        ]\n",
    "\n",
    "        logger.info(\"Preprocessing pipeline for Decision Tree model.\")\n",
    "\n",
    "    elif model_type == \"grande\":\n",
    "        logger.info(\"No preprocessing required for Grande model.\")\n",
    "        return X, None\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Unknown teacher type. Must be 'nn' for Neural Network, 'tree' for Decision Tree, or 'grande' for Gradient Based Tree.\"\n",
    "        )\n",
    "\n",
    "    # Create a ColumnTransformer to apply the transformations to the appropriate columns.\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        n_jobs=-2,\n",
    "    )\n",
    "\n",
    "    # Fit the preprocessor on the training data and transform it.\n",
    "    X_preprocessed = preprocessor.fit_transform(X, y)\n",
    "\n",
    "    return X_preprocessed, preprocessor\n",
    "\n",
    "\n",
    "def _minimal_preprocess_test(X, preprocessor):\n",
    "    \"\"\"\n",
    "    Transform the input features according to the preprocessor fitted on the training data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pandas.DataFrame\n",
    "        The input features.\n",
    "    preprocessor : ColumnTransformer\n",
    "        The preprocessor fitted on the training data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_preprocessed : np.array\n",
    "        The preprocessed features.\n",
    "    \"\"\"\n",
    "    if preprocessor is None:\n",
    "        return X\n",
    "    return preprocessor.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d700d2",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f642a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_id: int, config: dict):\n",
    "    \"\"\"\n",
    "    Loads an OpenML dataset based on its ID and processes it according to the task type.\n",
    "\n",
    "    This function fetches the dataset using the _fetch_dataset helper, then processes\n",
    "    the target variable based on whether the task is binary classification or regression.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_id (int): The ID of the dataset on OpenML.\n",
    "        config (dict): Configuration dictionary containing data paths and settings.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - X: The feature data (pandas DataFrame)\n",
    "            - y: The processed target variable (encoded for binary classification or numpy array for regression)\n",
    "            - cat_cols: List of booleans indicating which features are categorical\n",
    "            - attribute_names: List of attribute names\n",
    "            - task_type: String indicating the task type ('binary' or 'regression')\n",
    "    \"\"\"\n",
    "    X, y, cat_cols, attribute_names, task_type = fetch_dataset(\n",
    "        dataset_id=dataset_id,\n",
    "        cache_dir=config[\"data\"][\"cache_dir_path\"],\n",
    "    )\n",
    "    if task_type == \"binary\":\n",
    "        # Encode the target variable if it's binary\n",
    "        y = encode_target(y)\n",
    "    else:\n",
    "        # Transform to np.array for regression\n",
    "        y = np.array(y, dtype=float)\n",
    "\n",
    "    return X, y, cat_cols, attribute_names, task_type\n",
    "\n",
    "\n",
    "def fetch_dataset(dataset_id: int, cache_dir: str = \"data/cache/\"):\n",
    "    \"\"\"\n",
    "    Downloads an OpenML dataset based on its id, caches the data locally, and returns the data\n",
    "    along with its metadata.\n",
    "\n",
    "    The five returned objects are:\n",
    "        - X: The feature data (typically a pandas DataFrame).\n",
    "        - y: The target variable.\n",
    "        - categorical_indicator: A list of booleans indicating which features are categorical.\n",
    "        - attribute_names: A list of attribute names.\n",
    "        - task_type: A string indicating the type of task ('binary' or 'regression').\n",
    "\n",
    "    If the dataset has been previously downloaded and stored in the cache directory,\n",
    "    it will be loaded from the local file instead of re-downloading.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_id (int): The id of the dataset on OpenML.\n",
    "        cache_dir (str): The directory to store the downloaded dataset. Defaults to \"openml_cache\".\n",
    "\n",
    "    Returns:\n",
    "        X, y, categorical_indicator, attribute_names, task_type\n",
    "    \"\"\"\n",
    "    logger = setup_logging()\n",
    "\n",
    "    # Ensure the cache directory exists\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    # Define a cache file name that is unique to the dataset id\n",
    "    cache_file = os.path.join(cache_dir, f\"openml_dataset_{dataset_id}.pkl\")\n",
    "\n",
    "    # If the cache file exists, load the data from the file.\n",
    "    if os.path.exists(cache_file):\n",
    "        logger.info(\n",
    "            f\"Loading dataset {dataset_id} from cache at '{cache_file}'...\"\n",
    "        )\n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        X = data[\"X\"]\n",
    "        y = data[\"y\"]\n",
    "        categorical_indicator = data[\"categorical_indicator\"]\n",
    "        attribute_names = data[\"attribute_names\"]\n",
    "        task_type = data[\"task_type\"]\n",
    "    else:\n",
    "        # Download the dataset from OpenML.\n",
    "        logger.info(f\"Downloading dataset {dataset_id} from OpenML...\")\n",
    "        dataset = openml.datasets.get_dataset(dataset_id)\n",
    "\n",
    "        # Use the default target attribute (if defined) when retrieving the data.\n",
    "        X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "            target=dataset.default_target_attribute, dataset_format=\"dataframe\"\n",
    "        )\n",
    "\n",
    "        # Determine the task type (binary classification or regression)\n",
    "        task_type = _determine_task_type(y)\n",
    "\n",
    "        # Store the data and metadata in a dictionary.\n",
    "        data = {\n",
    "            \"X\": X,\n",
    "            \"y\": y,\n",
    "            \"categorical_indicator\": categorical_indicator,\n",
    "            \"attribute_names\": attribute_names,\n",
    "            \"task_type\": task_type,\n",
    "        }\n",
    "\n",
    "        # Save the dictionary to a local file using pickle.\n",
    "        with open(cache_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        logger.info(f\"Dataset {dataset_id} stored locally at '{cache_file}'.\")\n",
    "\n",
    "    logger.info(f\"Dataset {dataset_id} loaded successfully with task type: {task_type}\")\n",
    "\n",
    "    return X, y, categorical_indicator, attribute_names, task_type\n",
    "\n",
    "\n",
    "def _determine_task_type(y):\n",
    "    \"\"\"\n",
    "    Determines if the target variable is for binary classification, or regression.\n",
    "\n",
    "    Parameters:\n",
    "        y: The target variable (pandas dataframe).\n",
    "\n",
    "    Returns:\n",
    "        str: 'binary', or 'regression'\n",
    "    \"\"\"\n",
    "\n",
    "    # Get unique values\n",
    "    unique_values = pd.unique(y)\n",
    "\n",
    "    # Check if it's binary (2 unique values)\n",
    "    if len(unique_values) == 2:\n",
    "        return \"binary\"\n",
    "    else:\n",
    "        return \"regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4579812",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ee7681de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(y_prob, y_true):\n",
    "    \"\"\"\n",
    "    Evaluate classification performance using balanced accuracy, F1 score, and ROC AUC.\n",
    "\n",
    "    Parameters:\n",
    "        y_prob (np.array): Predicted probabilities for the positive class.\n",
    "        y_true (np.array): True labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    logger = setup_logging()\n",
    "    y_pred = (y_prob[:, 1] > 0.5).astype(int)\n",
    "\n",
    "    acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    roc_auc = roc_auc_score(y_true, y_prob[:, 1])\n",
    "\n",
    "    logger.info(f\"\\t Balanced Accuracy: {acc:.4f}, F1 Score: {f1:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"roc\": roc_auc,\n",
    "    }\n",
    "\n",
    "def evaluate_regression(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Evaluate regression performance using R^2 score.\n",
    "\n",
    "    Parameters:\n",
    "        y_pred (np.array): Predicted values.\n",
    "        y_true (np.array): True values.\n",
    "\n",
    "    Returns:\n",
    "        float: R^2 score.\n",
    "    \"\"\"\n",
    "    logger = setup_logging()\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    logger.info(f\"\\t MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R^2: {r2:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"mae\": mae,\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\": r2,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8af1a5",
   "metadata": {},
   "source": [
    "# Model Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f9825c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherModelBase(ABC):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.params = kwargs\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model on the given data.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.ndarray): The input features with shape (n_samples, n_features).\n",
    "            y (np.ndarray): The target labels with shape (n_samples,).\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Makes predictions on the provided data.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.ndarray): The input features with shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: An array of predicted probabilities with shape (n_samples, n_classes).\n",
    "                        (If the model returns logits, these should be post-processed to probabilities.)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, y_pred: np.ndarray, y_true: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluates the model's predictions against the true labels.\n",
    "\n",
    "        Parameters:\n",
    "            y_pred (np.ndarray): The predicted probabilities or values.\n",
    "            y_true (np.ndarray): The true labels.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: A dictionary containing evaluation metrics.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            metrics = evaluate_classification(y_pred, y_true)\n",
    "            metrics['parameters'] = self.count_parameters()\n",
    "            return metrics\n",
    "        \n",
    "        elif self.task_type == \"regression\":\n",
    "            metrics = evaluate_regression(y_pred, y_true)\n",
    "            metrics['parameters'] = self.count_parameters()\n",
    "            return metrics\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown task type: {self.task_type}. Must be 'binary' or 'regression'.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def count_parameters(self) -> int:\n",
    "        \"\"\"\n",
    "        Counts the number of trainable parameters in the model.\n",
    "\n",
    "        Returns:\n",
    "            int: The total number of trainable parameters.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class TabPFNTeacherModel(TeacherModelBase):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a TabPFN teacher model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments to pass to the base\n",
    "            class constructor.\n",
    "        \"\"\"\n",
    "        self.device = kwargs.pop(\"device\")\n",
    "        self.config = kwargs.pop(\"config\")\n",
    "        self.task_type = kwargs.pop(\"task_type\")\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def train(self, X, y, **training_params):\n",
    "        if self.task_type == \"binary\":\n",
    "            self.model = TabPFNClassifier()\n",
    "        else:\n",
    "            self.model = TabPFNRegressor()\n",
    "        # If X is bigger than 10000 samples, reduce data size to 10000\n",
    "        if X.shape[0] > 10000:\n",
    "            X = X[:10000]\n",
    "            y = y[:10000]\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.task_type == \"binary\":\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            return self.model.predict(X)\n",
    "\n",
    "    def count_parameters(self) -> int:\n",
    "        \"\"\"\n",
    "        Counts the number of trainable parameters in the underlying TabPFN model.\n",
    "\n",
    "        Returns:\n",
    "            int: Total number of trainable parameters.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the TabPFN model has not been fitted yet.\n",
    "        \"\"\"\n",
    "        if not hasattr(self.model, \"model_\"):\n",
    "            raise ValueError(\n",
    "                \"The TabPFN model is not fitted yet. Please call fit() first.\"\n",
    "            )\n",
    "        return sum(p.numel() for p in self.model.model_.parameters() if p.requires_grad)\n",
    "    \n",
    "\n",
    "class CatBoostTeacherModel(TeacherModelBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a CatBoost teacher model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments to pass to the base class constructor.\n",
    "        \"\"\"\n",
    "        self.device = kwargs.pop(\"device\")\n",
    "        self.config = kwargs.pop(\"config\")\n",
    "        self.task_type = kwargs.pop(\"task_type\")\n",
    "        self.hyperparams = kwargs.pop(\"hyperparams\")\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the CatBoost model using the provided data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target data.\n",
    "        **training_params : dict\n",
    "            Additional training parameters to pass to the CatBoost model's fit method.\n",
    "        \"\"\"\n",
    "        config = self.config\n",
    "        random_state = config[\"training\"][\"random_state\"]\n",
    "\n",
    "        # Initialize the CatBoost model\n",
    "        if self.task_type == \"binary\":\n",
    "            self.model = CatBoostClassifier(\n",
    "                random_seed=random_state,\n",
    "                thread_count=-1,  # Use all available CPU cores\n",
    "                logging_level=\"Silent\",  # Suppress CatBoost output\n",
    "                task_type=(\"GPU\" if str(self.device).lower() == \"cuda\" else \"CPU\"),\n",
    "                **self.hyperparams,  # Include the sampled hyperparameters\n",
    "            )\n",
    "        else:\n",
    "            self.model = CatBoostRegressor(\n",
    "                random_seed=random_state,\n",
    "                thread_count=-1,  # Use all available CPU cores\n",
    "                logging_level=\"Silent\",  # Suppress CatBoost output\n",
    "                task_type=(\"GPU\" if str(self.device).lower() == \"cuda\" else \"CPU\"),\n",
    "                **self.hyperparams,  # Include the sampled hyperparameters\n",
    "            )\n",
    "\n",
    "        # Split data into training and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=random_state\n",
    "        )\n",
    "        # Train the model\n",
    "        self.model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            early_stopping_rounds=20, \n",
    "        )\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on the given data using the trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array-like of shape (n_samples, n_classes)\n",
    "            The predicted probabilities for each class.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            return self.model.predict(X)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"\n",
    "        Count the number of parameters in the trained CatBoost model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The number of parameters in the model.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"model\") or self.model is None:\n",
    "            raise ValueError(\"The model has not been trained yet.\")\n",
    "\n",
    "        # Get the number of trees in the model\n",
    "        tree_count = self.model.tree_count_\n",
    "\n",
    "        # For each tree, count:\n",
    "        # - split values (one per non-leaf node)\n",
    "        # - leaf values (one per leaf node)\n",
    "        # - feature indices (one per non-leaf node)\n",
    "        # For a binary tree with depth d, there are 2^d - 1 non-leaf nodes and 2^d leaf nodes\n",
    "        # This is a simplified approximation\n",
    "        approx_depth = self.hyperparams.get(\"max_depth\", 6)  # Default depth in CatBoost is 6\n",
    "        non_leaf_nodes = 2**approx_depth - 1\n",
    "        leaf_nodes = 2**approx_depth\n",
    "\n",
    "        # Total parameters per tree\n",
    "        params_per_tree = (\n",
    "            non_leaf_nodes * 2 + leaf_nodes\n",
    "        )  # split values + feature indices + leaf values\n",
    "        return tree_count * params_per_tree\n",
    "    \n",
    "\n",
    "class GRANDETeacherModel(TeacherModelBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a GRANDE teacher model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments. The `device` parameter is popped from kwargs and\n",
    "            defaults to 'cpu' if not provided.\n",
    "        \"\"\"\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "        os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "        self.device = kwargs.pop(\"device\")\n",
    "        self.config = kwargs.pop(\"config\")\n",
    "        self.task_type = kwargs.pop(\"task_type\")\n",
    "        self.hyperparams = kwargs.pop(\"hyperparams\")\n",
    "        self.cat_cols = kwargs.pop(\"cat_cols\")\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the GRANDE teacher model using the provided data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target data.\n",
    "        **training_params : dict\n",
    "            Additional training parameters to pass to the GRANDE model's fit method.\n",
    "        \"\"\"\n",
    "        config = self.config\n",
    "        random_state = config[\"training\"][\"random_state\"]\n",
    "\n",
    "        # Determine categorical indices\n",
    "        cat_indices = [i for i, is_cat in enumerate(self.cat_cols) if is_cat]\n",
    "\n",
    "        training_params = {\n",
    "            \"epochs\": 1000,\n",
    "            \"early_stopping_epochs\": 25,\n",
    "            \"batch_size\": 64,\n",
    "            \"cat_idx\": cat_indices,\n",
    "            \"random_seed\": random_state,\n",
    "        }\n",
    "\n",
    "        if self.task_type == \"binary\":\n",
    "            training_params[\"objective\"] = \"binary\"\n",
    "        else:\n",
    "            training_params[\"objective\"] = \"regression\"\n",
    "\n",
    "        # Instantiate the GRANDE teacher model.\n",
    "        self.model = GRANDE(params=self.hyperparams, args=training_params)\n",
    "\n",
    "        # Split the data into training and validation sets.\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=random_state\n",
    "        )\n",
    "        self.model.fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on the given input data using the trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array-like\n",
    "            The predicted target values.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            return self.model.predict(X)\n",
    "        else:\n",
    "            return self.model.predict(X).squeeze()\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"\n",
    "        Count the number of trainable parameters in the GRANDE teacher model based on the\n",
    "        dense representation of a single tree and the number of trees in the ensemble.\n",
    "\n",
    "        The number of parameters for a single tree is computed as:\n",
    "            leaf parameters: 2^d\n",
    "            split thresholds: (2^d - 1) * n\n",
    "            feature selection (one-hot): (2^d - 1) * n\n",
    "        so that:\n",
    "            params_per_tree = 2^d + 2 * n * (2^d - 1)\n",
    "        The total is then:\n",
    "            total_params = n_estimators * params_per_tree\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"model\"):\n",
    "            raise ValueError(\"The model has not been trained/initialized yet.\")\n",
    "\n",
    "        # These attributes must be set when the GRANDE model is instantiated.\n",
    "        # Adjust the attribute names if they are different in your GRANDE implementation.\n",
    "        d = self.model.depth  # Depth of each decision tree.\n",
    "        n = self.model.number_of_variables  # Number of features used in the tree.\n",
    "        E = self.model.n_estimators  # Number of trees in the ensemble.\n",
    "\n",
    "        # Calculate the number of parameters for one tree.\n",
    "        params_per_tree = (2**d) + 2 * n * ((2**d) - 1)\n",
    "        total_params = E * params_per_tree\n",
    "\n",
    "        return total_params\n",
    "    \n",
    "def get_teacher_model(config: dict, task_type: Literal[\"binary\", \"regression\"], device, hyperparams, cat_cols=None) -> TeacherModelBase:\n",
    "    \"\"\"\n",
    "    Returns an instance of the teacher model based on the configuration.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict\n",
    "        Configuration dictionary containing model and dataset details.\n",
    "    device : torch.device\n",
    "        The device to use for training (CPU or GPU).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TeacherModelBase\n",
    "        An instance of the teacher model.\n",
    "    \"\"\"\n",
    "    model_type = config[\"model\"][\"teacher_model\"]\n",
    "    if model_type == \"tabpfn\":\n",
    "        return TabPFNTeacherModel(config=config, task_type=task_type, device=device, hyperparams=hyperparams)\n",
    "    elif model_type == \"catboost\":\n",
    "        return CatBoostTeacherModel(config=config, task_type=task_type, device=device, hyperparams=hyperparams)\n",
    "    elif model_type == \"grande\":\n",
    "        return GRANDETeacherModel(config=config, task_type=task_type, device=device, hyperparams=hyperparams, cat_cols=cat_cols)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown teacher type: {config['model']['teacher_model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725d7a3c",
   "metadata": {},
   "source": [
    "# HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ab890760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_hyperparameters(trial: optuna.Trial, model_type: str, task_type: str, use_hpo: bool) -> dict:\n",
    "    \"\"\"\n",
    "    Suggest hyperparameters for the given model type using Optuna.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    trial : optuna.Trial\n",
    "        Optuna trial object for suggesting hyperparameters.\n",
    "    model_type : str\n",
    "        Type of model ('tabpfn', 'catboost', etc.).\n",
    "    task_type : str\n",
    "        Type of task ('binary', 'regression').\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of suggested hyperparameters.\n",
    "    \"\"\"\n",
    "    hyperparameter = {}\n",
    "    \n",
    "    if model_type == \"catboost\":\n",
    "        if use_hpo:\n",
    "            hyperparameter = {\n",
    "                \"iterations\": trial.suggest_int(\"iterations\", 512, 4096),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 2, 12),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "                \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 0.5, 30),\n",
    "                \"boosting_type\": \"Plain\",\n",
    "            }\n",
    "        else:\n",
    "            hyperparameter = {\n",
    "                \"iterations\": 1000,\n",
    "                \"max_depth\": 6,\n",
    "                \"l2_leaf_reg\": 3,\n",
    "                \"boosting_type\": \"Plain\",\n",
    "            }\n",
    "\n",
    "        if task_type == \"binary\":\n",
    "            hyperparameter[\"loss_function\"] = \"Logloss\"\n",
    "        else:\n",
    "            hyperparameter[\"loss_function\"] = \"RMSE\"\n",
    "    \n",
    "    elif model_type == \"grande\":\n",
    "        if use_hpo:\n",
    "            hyperparameter = {\n",
    "                \"depth\": trial.suggest_int(\"depth\", 3, 7),\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 512, 4096),\n",
    "                \"learning_rate_weights\": trial.suggest_float(\"learning_rate_weights\", 0.0001, 0.05, log=True),\n",
    "                \"learning_rate_index\": trial.suggest_float(\"learning_rate_index\", 0.001, 0.2, log=True),\n",
    "                \"learning_rate_values\": trial.suggest_float(\"learning_rate_values\", 0.001, 0.2, log=True),\n",
    "                \"learning_rate_leaf\": trial.suggest_float(\"learning_rate_leaf\", 0.001, 0.2, log=True),\n",
    "                \"optimizer\": \"adam\",\n",
    "                \"cosine_decay_steps\": trial.suggest_categorical(\"cosine_decay_steps\", [0.0, 0.1, 1.0, 100.0, 1000.0]),\n",
    "                \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.75),\n",
    "                \"selected_variables\": trial.suggest_float(\"selected_variables\", 0.0, 1.0),\n",
    "                \"data_subset_fraction\": trial.suggest_float(\"data_subset_fraction\", 0.1, 1.0),\n",
    "                \"focal_loss\": False,\n",
    "                \"temperature\": 0.0,\n",
    "                \"from_logits\": True,\n",
    "                \"use_class_weights\": True,\n",
    "            }\n",
    "        else:\n",
    "            hyperparameter = {\n",
    "                \"depth\": 5,\n",
    "                \"n_estimators\": 2048,\n",
    "                \"learning_rate_weights\": 0.005,\n",
    "                \"learning_rate_index\": 0.01,\n",
    "                \"learning_rate_values\": 0.01,\n",
    "                \"learning_rate_leaf\": 0.01,\n",
    "                \"optimizer\": \"adam\",\n",
    "                \"cosine_decay_steps\": 0,\n",
    "                \"dropout\": 0.0,\n",
    "                \"selected_variables\": 0.8,\n",
    "                \"data_subset_fraction\": 1.0,\n",
    "                \"focal_loss\": False,\n",
    "                \"temperature\": 0.0,\n",
    "                \"from_logits\": True,\n",
    "                \"use_class_weights\": True,\n",
    "            }\n",
    "        \n",
    "        if task_type == \"binary\":\n",
    "            hyperparameter[\"loss\"] = \"crossentropy\"\n",
    "        else:\n",
    "            hyperparameter[\"loss\"] = \"mse\"\n",
    "    \n",
    "    return hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02857a",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "59b406d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data\": {\n",
    "        \"datasets\": [\n",
    "                    #  23381, # Binary classification datasets\n",
    "                       197, # Regression datasets\n",
    "                     ],  \n",
    "        \"cache_dir_path\": \"data/cache/\",\n",
    "        \"fold_indices_path\": \"data/fold_indices/\",\n",
    "        \"optuna_db_path\": \"data/optuna_db/\",\n",
    "        \"output_dir_path\": \"data/output/\",\n",
    "        \"outer_folds_path\": \"data/outer_fold/\",\n",
    "        \"results_dir_path\": \"results/\",\n",
    "    },\n",
    "\n",
    "    \"preprocessing\": {\n",
    "        \"threshold_high_cardinality\": 10,  \n",
    "    },\n",
    "\n",
    "    \"model\": {\n",
    "        \"teacher_model\": \"grande\",   # Options: 'tabpfn', \n",
    "        \"student_model\": \"catboost\", # Options: 'catboost', \n",
    "    },\n",
    "\n",
    "    \"training\": {\n",
    "        \"use_hpo\": False,\n",
    "        \"trials\": 30,\n",
    "        \"random_state\": 42,\n",
    "        \"outer_folds\": 5,\n",
    "        \"inner_folds\": 2,\n",
    "    },\n",
    "\n",
    "    \"teacher_models\": {\n",
    "        \"tabpfn\": {\n",
    "            \"preprocessing\": \"nn\",  \n",
    "        },\n",
    "        \"catboost\": {\n",
    "            \"preprocessing\": \"tree\", \n",
    "        },\n",
    "        \"grande\": {\n",
    "            \"preprocessing\": \"grande\",  \n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17729927",
   "metadata": {},
   "source": [
    "# Train Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b9dce1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 20:03:34,815 - __main__ - INFO - Loading configuration...\n",
      "2025-06-04 20:03:34,816 - __main__ - INFO - Loading dataset 197 from cache at 'data/cache/openml_dataset_197.pkl'...\n",
      "2025-06-04 20:03:34,818 - __main__ - INFO - Dataset 197 loaded successfully with task type: regression\n",
      "2025-06-04 20:03:34,819 - __main__ - INFO - Using GPU: NVIDIA RTX A6000\n",
      "2025-06-04 20:03:34,820 - __main__ - INFO - Random seed set to 42\n",
      "2025-06-04 20:03:34,821 - __main__ - INFO - -------------------- Outer Fold 1 --------------------\n",
      "2025-06-04 20:03:34,823 - __main__ - INFO - Starting hyperparameter optimization for outer fold 1...\n",
      "[I 2025-06-04 20:03:34,823] A new study created in memory with name: 197.1.grande\n",
      "2025-06-04 20:03:34,825 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-04 20:03:34,825 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "2025-06-04 20:03:49,144 - __main__ - INFO - \t MAE: 1.7982, MSE: 6.2230, RMSE: 2.4946, R^2: 0.9824\n",
      "2025-06-04 20:03:49,146 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-04 20:03:49,146 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "2025-06-04 20:04:04,259 - __main__ - INFO - \t MAE: 1.7874, MSE: 5.9603, RMSE: 2.4414, R^2: 0.9826\n",
      "[I 2025-06-04 20:04:04,260] Trial 0 finished with value: -1.7928274365637344 and parameters: {}. Best is trial 0 with value: -1.7928274365637344.\n",
      "2025-06-04 20:04:04,261 - __main__ - INFO - Best hyperparameters for fold 1: {}\n",
      "2025-06-04 20:04:04,261 - __main__ - INFO - Best score: -1.7928\n",
      "2025-06-04 20:04:04,262 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-04 20:04:04,262 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-04 20:04:04,262 - __main__ - INFO - Retraining Model on Outer Fold 1\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "2025-06-04 20:04:27,477 - __main__ - INFO - \t Inference Time: 1.01537 seconds\n",
      "2025-06-04 20:04:27,479 - __main__ - INFO - \t MAE: 1.7074, MSE: 5.4027, RMSE: 2.3244, R^2: 0.9820\n",
      "2025-06-04 20:04:27,480 - __main__ - INFO - -------------------- Outer Fold 2 --------------------\n",
      "2025-06-04 20:04:27,482 - __main__ - INFO - Starting hyperparameter optimization for outer fold 2...\n",
      "[I 2025-06-04 20:04:27,482] A new study created in memory with name: 197.2.grande\n",
      "2025-06-04 20:04:27,484 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-04 20:04:27,485 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression (1639,) [87.69193 91.31391 79.06036 88.52079 92.86063]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "2025-06-04 20:04:42,235 - __main__ - INFO - \t MAE: 1.7625, MSE: 6.0730, RMSE: 2.4643, R^2: 0.9822\n",
      "2025-06-04 20:04:42,237 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-04 20:04:42,237 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "2025-06-04 20:04:55,506 - __main__ - INFO - \t MAE: 1.7820, MSE: 6.2264, RMSE: 2.4953, R^2: 0.9819\n",
      "[I 2025-06-04 20:04:55,507] Trial 0 finished with value: -1.7722405835974873 and parameters: {}. Best is trial 0 with value: -1.7722405835974873.\n",
      "2025-06-04 20:04:55,508 - __main__ - INFO - Best hyperparameters for fold 2: {}\n",
      "2025-06-04 20:04:55,508 - __main__ - INFO - Best score: -1.7722\n",
      "2025-06-04 20:04:55,508 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-04 20:04:55,509 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-04 20:04:55,509 - __main__ - INFO - Retraining Model on Outer Fold 2\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "2025-06-04 20:05:14,822 - __main__ - INFO - \t Inference Time: 1.00449 seconds\n",
      "2025-06-04 20:05:14,824 - __main__ - INFO - \t MAE: 1.6684, MSE: 5.4310, RMSE: 2.3304, R^2: 0.9833\n",
      "2025-06-04 20:05:14,825 - __main__ - INFO - -------------------- Outer Fold 3 --------------------\n",
      "2025-06-04 20:05:14,827 - __main__ - INFO - Starting hyperparameter optimization for outer fold 3...\n",
      "[I 2025-06-04 20:05:14,827] A new study created in memory with name: 197.3.grande\n",
      "2025-06-04 20:05:14,829 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-04 20:05:14,830 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression (1639,) [90.22914   84.74084   85.646866   1.3436966 88.123474 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "2025-06-04 20:05:31,731 - __main__ - INFO - \t MAE: 1.7849, MSE: 5.9781, RMSE: 2.4450, R^2: 0.9830\n",
      "2025-06-04 20:05:31,733 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-04 20:05:31,734 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "2025-06-04 20:05:44,998 - __main__ - INFO - \t MAE: 1.7897, MSE: 6.1641, RMSE: 2.4828, R^2: 0.9821\n",
      "[I 2025-06-04 20:05:44,999] Trial 0 finished with value: -1.787284404648707 and parameters: {}. Best is trial 0 with value: -1.787284404648707.\n",
      "2025-06-04 20:05:44,999 - __main__ - INFO - Best hyperparameters for fold 3: {}\n",
      "2025-06-04 20:05:45,000 - __main__ - INFO - Best score: -1.7873\n",
      "2025-06-04 20:05:45,000 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-04 20:05:45,001 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-04 20:05:45,001 - __main__ - INFO - Retraining Model on Outer Fold 3\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "2025-06-04 20:06:02,603 - __main__ - INFO - \t Inference Time: 1.12992 seconds\n",
      "2025-06-04 20:06:02,605 - __main__ - INFO - \t MAE: 1.6560, MSE: 5.0959, RMSE: 2.2574, R^2: 0.9832\n",
      "2025-06-04 20:06:02,606 - __main__ - INFO - -------------------- Outer Fold 4 --------------------\n",
      "2025-06-04 20:06:02,607 - __main__ - INFO - Starting hyperparameter optimization for outer fold 4...\n",
      "[I 2025-06-04 20:06:02,608] A new study created in memory with name: 197.4.grande\n",
      "2025-06-04 20:06:02,610 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-04 20:06:02,610 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression (1638,) [83.564476 85.4327   94.31128  82.32708  91.757385]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "2025-06-04 20:06:18,211 - __main__ - INFO - \t MAE: 1.7391, MSE: 5.7822, RMSE: 2.4046, R^2: 0.9810\n",
      "2025-06-04 20:06:18,213 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-04 20:06:18,213 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "2025-06-04 20:06:31,247 - __main__ - INFO - \t MAE: 1.7924, MSE: 6.3300, RMSE: 2.5160, R^2: 0.9824\n",
      "[I 2025-06-04 20:06:31,248] Trial 0 finished with value: -1.7657379386699303 and parameters: {}. Best is trial 0 with value: -1.7657379386699303.\n",
      "2025-06-04 20:06:31,249 - __main__ - INFO - Best hyperparameters for fold 4: {}\n",
      "2025-06-04 20:06:31,249 - __main__ - INFO - Best score: -1.7657\n",
      "2025-06-04 20:06:31,250 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-04 20:06:31,250 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-04 20:06:31,250 - __main__ - INFO - Retraining Model on Outer Fold 4\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "2025-06-04 20:06:54,104 - __main__ - INFO - \t Inference Time: 1.00246 seconds\n",
      "2025-06-04 20:06:54,106 - __main__ - INFO - \t MAE: 1.6791, MSE: 5.2708, RMSE: 2.2958, R^2: 0.9855\n",
      "2025-06-04 20:06:54,107 - __main__ - INFO - -------------------- Outer Fold 5 --------------------\n",
      "2025-06-04 20:06:54,109 - __main__ - INFO - Starting hyperparameter optimization for outer fold 5...\n",
      "[I 2025-06-04 20:06:54,109] A new study created in memory with name: 197.5.grande\n",
      "2025-06-04 20:06:54,111 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-04 20:06:54,111 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression (1638,) [88.42782  82.3329   90.9486   90.02148  92.932884]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "2025-06-04 20:07:09,516 - __main__ - INFO - \t MAE: 1.8167, MSE: 6.4584, RMSE: 2.5413, R^2: 0.9809\n",
      "2025-06-04 20:07:09,518 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-04 20:07:09,518 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "2025-06-04 20:07:22,415 - __main__ - INFO - \t MAE: 1.7864, MSE: 6.1911, RMSE: 2.4882, R^2: 0.9799\n",
      "[I 2025-06-04 20:07:22,416] Trial 0 finished with value: -1.8015217127725778 and parameters: {}. Best is trial 0 with value: -1.8015217127725778.\n",
      "2025-06-04 20:07:22,416 - __main__ - INFO - Best hyperparameters for fold 5: {}\n",
      "2025-06-04 20:07:22,417 - __main__ - INFO - Best score: -1.8015\n",
      "2025-06-04 20:07:22,417 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-04 20:07:22,417 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-04 20:07:22,417 - __main__ - INFO - Retraining Model on Outer Fold 5\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "2025-06-04 20:07:41,434 - __main__ - INFO - \t Inference Time: 1.00555 seconds\n",
      "2025-06-04 20:07:41,436 - __main__ - INFO - \t MAE: 1.7249, MSE: 5.3020, RMSE: 2.3026, R^2: 0.9868\n",
      "2025-06-04 20:07:41,439 - __main__ - INFO - Outer fold metrics saved to: data/outer_fold/teacher/default/197_grande.csv\n",
      "2025-06-04 20:07:41,440 - __main__ - INFO - === FINAL RESULTS FOR DATASET 197 ===\n",
      "2025-06-04 20:07:41,441 - __main__ - INFO - MAE: 1.6872  0.0284\n",
      "2025-06-04 20:07:41,441 - __main__ - INFO - MSE: 5.3005  0.1325\n",
      "2025-06-04 20:07:41,441 - __main__ - INFO - RMSE: 2.3021  0.0289\n",
      "2025-06-04 20:07:41,441 - __main__ - INFO - R2: 0.9841  0.0019\n",
      "2025-06-04 20:07:41,442 - __main__ - INFO - Mean Inference Time: 1.0316  0.0552\n",
      "2025-06-04 20:07:41,442 - __main__ - INFO - Mean Parameters: 2732032.0000  0.0000\n",
      "2025-06-04 20:07:41,443 - __main__ - INFO - Summary statistics saved to: results/197_results.json\n",
      "2025-06-04 20:07:41,444 - __main__ - INFO - Fold indices file already exists: data/fold_indices/dataset_197.json\n",
      "2025-06-04 20:07:41,452 - __main__ - INFO - grande outputs saved to: data/output/default/teacher/197_grande.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression (1638,) [79.50667 76.1699  90.92571 85.76621 81.97447]\n"
     ]
    }
   ],
   "source": [
    "# Get list of datasets to process from configuration\n",
    "datasets = config[\"data\"][\"datasets\"]\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINING LOOP - Process each dataset independently\n",
    "# =============================================================================\n",
    "for dataset_id in datasets:\n",
    "# -------------------------------------------------------------------------\n",
    "    # SETUP AND INITIALIZATION\n",
    "    # -------------------------------------------------------------------------\n",
    "    logger = setup_logging()\n",
    "    logger.info(\"Loading configuration...\")\n",
    "\n",
    "    # Extract model configuration for this run\n",
    "    model_type = config[\"model\"][\"teacher_model\"]\n",
    "    preprocessing_type = config[\"teacher_models\"][model_type][\"preprocessing\"]\n",
    "    use_hpo = config[\"training\"][\"use_hpo\"]\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 0: DATA LOADING\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Load dataset from OpenML with caching for efficiency\n",
    "    X, y, cat_cols, _, task_type = load_dataset(\n",
    "        dataset_id=dataset_id,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 1: INFRASTRUCTURE SETUP\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Note: Checking for existing results is placeholder for future implementation\n",
    "    summary_file = os.path.join(config[\"data\"][\"results_dir_path\"], f\"{dataset_id}_results.json\")\n",
    "    if os.path.exists(summary_file):\n",
    "        with open(summary_file, 'r') as f:\n",
    "            existing_results = json.load(f)\n",
    "        \n",
    "        # Check if we already have results with the same configuration\n",
    "        config_exists = False\n",
    "        for key, result in existing_results.items():\n",
    "            if (result.get('model_type') == model_type and \n",
    "                result.get('use_hpo') == use_hpo and \n",
    "                result.get('seed') == config[\"training\"][\"random_state\"]):\n",
    "                config_exists = True\n",
    "                logger.info(f\"Results already exist for dataset {dataset_id} with model {model_type}, HPO: {use_hpo}, seed: {config['training']['random_state']}\")\n",
    "                break\n",
    "        \n",
    "        if config_exists:\n",
    "            logger.info(f\"Skipping dataset {dataset_id} - results already computed\")\n",
    "            continue\n",
    "\n",
    "    # Configure GPU/CPU usage for training\n",
    "    device = check_GPU_availability()\n",
    "\n",
    "    # Set random seed for reproducibility across all libraries\n",
    "    set_seed(config[\"training\"][\"random_state\"])\n",
    "    logger.info(f\"Random seed set to {config['training']['random_state']}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 2: INITIALIZE DATA STRUCTURES\n",
    "    # -------------------------------------------------------------------------\n",
    "    # List to store predictions from each outer fold\n",
    "    output_dfs = []\n",
    "\n",
    "    outer_fold_scores = []\n",
    "    \n",
    "    # Dictionary to store all fold indices for reproducibility and student training\n",
    "    # Structure: {\"outer_folds\": {fold_id: {train_idx, test_idx}},\n",
    "    #            \"inner_folds\": {outer_fold_id: {inner_fold_id: {train_idx, val_idx}}}}\n",
    "    fold_indices = {\n",
    "        \"outer_folds\": {},\n",
    "        \"inner_folds\": {}\n",
    "    }\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 3: OUTER CROSS-VALIDATION SETUP\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Choose appropriate CV strategy based on task type to maintain class balance\n",
    "    if task_type == \"binary\":\n",
    "        outer_cv = StratifiedKFold(\n",
    "            n_splits=config[\"training\"][\"outer_folds\"],\n",
    "            shuffle=True,\n",
    "            random_state=config[\"training\"][\"random_state\"],\n",
    "        )\n",
    "    else:\n",
    "        outer_cv = KFold(\n",
    "            n_splits=config[\"training\"][\"outer_folds\"],\n",
    "            shuffle=True,\n",
    "            random_state=config[\"training\"][\"random_state\"],\n",
    "        )\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 4: OUTER CROSS-VALIDATION LOOP\n",
    "    # =========================================================================\n",
    "    # Each iteration provides one unbiased performance estimate\n",
    "    for outer_fold, (train_idx, test_idx) in enumerate(outer_cv.split(X, y), start=1):\n",
    "\n",
    "        logger.info(f\"-------------------- Outer Fold {outer_fold} --------------------\")\n",
    "        \n",
    "        # ---------------------------------------------------------------------\n",
    "        # FOLD INDEX MANAGEMENT\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Store outer fold indices for later use in student training and validation\n",
    "        fold_indices[\"outer_folds\"][f\"fold_{outer_fold}\"] = {\n",
    "            \"train_idx\": train_idx.tolist(),\n",
    "            \"test_idx\": test_idx.tolist()\n",
    "        }\n",
    "\n",
    "        # Split data according to current outer fold\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # INNER CROSS-VALIDATION SETUP (for model validation)\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Choose appropriate CV strategy for inner folds\n",
    "        if task_type == \"binary\":\n",
    "            inner_cv = StratifiedKFold(\n",
    "                n_splits=config[\"training\"][\"inner_folds\"],\n",
    "                shuffle=True,\n",
    "                random_state=config[\"training\"][\"random_state\"],\n",
    "            )\n",
    "        else:\n",
    "            inner_cv = KFold(\n",
    "                n_splits=config[\"training\"][\"inner_folds\"],\n",
    "                shuffle=True,\n",
    "                random_state=config[\"training\"][\"random_state\"],\n",
    "            )\n",
    "        \n",
    "        # Initialize storage for inner fold indices within this outer fold\n",
    "        fold_indices[\"inner_folds\"][f\"outer_fold_{outer_fold}\"] = {}\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 5: INNER CROSS-VALIDATION LOOP (hyperparameter validation) \n",
    "        # =====================================================================\n",
    "        def objective(trial):\n",
    "\n",
    "            hyperparams = suggest_hyperparameters(\n",
    "                trial=trial,\n",
    "                model_type=model_type,\n",
    "                task_type=task_type,\n",
    "                use_hpo=use_hpo,\n",
    "            )\n",
    "\n",
    "            inner_fold_scores = []\n",
    "            val_metrics_list = []\n",
    "\n",
    "            # This loop would typically be used for hyperparameter optimization\n",
    "            for inner_fold, (inner_train_index, inner_val_index) in enumerate(inner_cv.split(X_train, y_train), start=1):\n",
    "                \n",
    "                # -----------------------------------------------------------------\n",
    "                # INDEX MANAGEMENT (Critical for avoiding data leakage)\n",
    "                # -----------------------------------------------------------------\n",
    "                # Convert relative indices (within outer training set) to absolute indices\n",
    "                absolute_inner_train_idx = train_idx[inner_train_index]\n",
    "                absolute_inner_val_idx = train_idx[inner_val_index]\n",
    "                \n",
    "                # Store inner fold indices using absolute indices for consistency\n",
    "                # Only store indices for the first trial\n",
    "                if trial.number == 0:  \n",
    "                    fold_indices[\"inner_folds\"][f\"outer_fold_{outer_fold}\"][f\"inner_fold_{inner_fold}\"] = {\n",
    "                        \"train_idx\": absolute_inner_train_idx.tolist(),\n",
    "                        \"val_idx\": absolute_inner_val_idx.tolist()\n",
    "                    }\n",
    "\n",
    "                # Split inner training data using relative indices\n",
    "                X_inner_train, X_inner_val = X_train.iloc[inner_train_index], X_train.iloc[inner_val_index]\n",
    "                y_inner_train, y_inner_val = y_train[inner_train_index], y_train[inner_val_index]\n",
    "\n",
    "                # -----------------------------------------------------------------\n",
    "                # PREPROCESSING: Apply model-specific data transformations\n",
    "                # -----------------------------------------------------------------\n",
    "                X_inner_train, X_inner_val = preprocess(\n",
    "                    X_inner_train,\n",
    "                    y_inner_train,\n",
    "                    X_inner_val, \n",
    "                    cat_cols,\n",
    "                    config,\n",
    "                    preprocessing_type=preprocessing_type,\n",
    "                )\n",
    "\n",
    "                # -----------------------------------------------------------------\n",
    "                # MODEL TRAINING: Train teacher model on inner training data\n",
    "                # -----------------------------------------------------------------\n",
    "                model = get_teacher_model(config=config, task_type=task_type, device=device, hyperparams=hyperparams, cat_cols=cat_cols)\n",
    "                logger.info(f\"Training Model on Outer Fold {outer_fold}, Inner Fold {inner_fold}...\")\n",
    "                model.train(X_inner_train, y_inner_train)\n",
    "\n",
    "                # -----------------------------------------------------------------\n",
    "                # VALIDATION: Evaluate model performance on inner validation set\n",
    "                # -----------------------------------------------------------------\n",
    "                val_preds = model.predict(X_inner_val)\n",
    "                val_metrics = model.evaluate(val_preds, y_inner_val)\n",
    "\n",
    "                # Store metrics from this fold for later mean calculation\n",
    "                val_metrics_list.append(val_metrics)\n",
    "\n",
    "                if task_type == \"binary\":\n",
    "                    inner_fold_scores.append(val_metrics[\"f1\"])\n",
    "                    trial.report(np.mean(inner_fold_scores), step=inner_fold)\n",
    "                else:\n",
    "                    inner_fold_scores.append(-val_metrics[\"mae\"])\n",
    "                    trial.report(np.mean(inner_fold_scores), step=inner_fold)\n",
    "\n",
    "                # Check if the trial should be pruned (With 2 inner folds, pruning not recommended)\n",
    "                # if trial.should_prune():\n",
    "                #     logger.info(\"Trial pruned.\")\n",
    "                #     raise optuna.TrialPruned()\n",
    "\n",
    "            # Calculate and set mean metrics as user attributes after all inner folds\n",
    "            if val_metrics_list:\n",
    "                # Get all metric keys from the first fold\n",
    "                metric_keys = val_metrics_list[0].keys()\n",
    "                \n",
    "                for metric_key in metric_keys:\n",
    "                    # Calculate mean across all inner folds for this metric\n",
    "                    metric_values = [fold_metrics[metric_key] for fold_metrics in val_metrics_list]\n",
    "                    mean_metric = np.mean(metric_values)\n",
    "                    trial.set_user_attr(f\"mean_{metric_key}\", mean_metric)\n",
    "\n",
    "            return np.mean(inner_fold_scores)\n",
    "            \n",
    "        logger.info(f\"Starting hyperparameter optimization for outer fold {outer_fold}...\")\n",
    "\n",
    "        study_kwargs = dict(\n",
    "            direction=\"maximize\",\n",
    "            study_name=f\"{dataset_id}.{outer_fold}.{model_type}\",\n",
    "            load_if_exists=True,\n",
    "            sampler=optuna.samplers.TPESampler(seed=config[\"training\"][\"random_state\"]),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps=1),\n",
    "        )\n",
    "\n",
    "        if use_hpo:\n",
    "            os.makedirs(config[\"data\"][\"optuna_db_path\"], exist_ok=True)\n",
    "            study_kwargs[\"storage\"] = f\"sqlite:///{config['data']['optuna_db_path']}/optuna.db\"\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            **study_kwargs,\n",
    "        )\n",
    "\n",
    "        # Optimize\n",
    "        completed_trials = len(study.trials)\n",
    "        remaining_trials = config[\"training\"][\"trials\"] - completed_trials\n",
    "\n",
    "        if remaining_trials > 0:\n",
    "            default_hyperparams = suggest_hyperparameters(None, model_type, task_type, False)\n",
    "            study.enqueue_trial(default_hyperparams)\n",
    "            study.optimize(\n",
    "                objective, \n",
    "                n_trials=remaining_trials if use_hpo else 1,\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\"The study has already reached the maximum number of trials.\")\n",
    "\n",
    "        # Get best hyperparameters\n",
    "        best_hyperparams = study.best_params\n",
    "        logger.info(f\"Best hyperparameters for fold {outer_fold}: {best_hyperparams}\")\n",
    "        logger.info(f\"Best score: {study.best_value:.4f}\")\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 6: FINAL MODEL TRAINING (on complete outer training set)\n",
    "        # =====================================================================        \n",
    "        # Apply same preprocessing to outer training and test sets\n",
    "        X_train, X_test = preprocess(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            cat_cols,\n",
    "            config,\n",
    "            preprocessing_type=preprocessing_type,  # Use 'nn' for TabPFN preprocessing\n",
    "        )\n",
    "        \n",
    "        # Train final model on complete outer training set\n",
    "        logger.info(\"------------------------------------------------------\")\n",
    "        logger.info(f\"Retraining Model on Outer Fold {outer_fold}\")\n",
    "\n",
    "        model = get_teacher_model(\n",
    "            config=config,\n",
    "            task_type=task_type,\n",
    "            device=device,\n",
    "            hyperparams=best_hyperparams,  # Use best hyperparameters from HPO\n",
    "            cat_cols=cat_cols,  \n",
    "        )\n",
    "        model.train(X_train, y_train)\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 7: FINAL EVALUATION (unbiased performance on outer test set)\n",
    "        # =====================================================================\n",
    "        # This provides the unbiased performance estimate for this fold\n",
    "        start_time = time.time()\n",
    "        test_preds = model.predict(X_test)\n",
    "        end_time = time.time() - start_time\n",
    "        logger.info(f\"\\t Inference Time: {end_time:.5f} seconds\")\n",
    "        test_metrics = model.evaluate(test_preds, y_test)\n",
    "\n",
    "        # Store outer fold score for later analysis\n",
    "        outer_fold_results = {\n",
    "            \"fold\": outer_fold,\n",
    "            \"seed\": config[\"training\"][\"random_state\"],\n",
    "            \"inference_time\": end_time,\n",
    "            **test_metrics,\n",
    "        }\n",
    "        outer_fold_scores.append(outer_fold_results)\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 8: STORE PREDICTIONS FOR STUDENT TRAINING\n",
    "        # =====================================================================\n",
    "        # Save predictions with their corresponding dataset indices\n",
    "        # These will be used as targets for training student models\n",
    "        output_dfs.append(pd.DataFrame({\n",
    "            \"index\": test_idx,\n",
    "            \"output\": test_preds[:, 1] if task_type == \"binary\" else test_preds\n",
    "        }))\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 9: SAVE RESULTS AND METADATA\n",
    "    # =========================================================================\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # SAVE OUTER FOLD METRICS\n",
    "    # -------------------------------------------------------------------------\n",
    "    outer_fold_df = pd.DataFrame(outer_fold_scores)\n",
    "    sub_folder = \"hpo\" if use_hpo else \"default\"\n",
    "    output_dir = os.path.join(config[\"data\"][\"outer_folds_path\"], \"teacher\", sub_folder)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    metrics_file = os.path.join(output_dir, f\"{dataset_id}_{model_type}.csv\")\n",
    "    outer_fold_df.to_csv(metrics_file, index=False)\n",
    "    logger.info(f\"Outer fold metrics saved to: {metrics_file}\")\n",
    "\n",
    "    # Calculate and log overall performance across all folds\n",
    "    mean_inference_time = outer_fold_df['inference_time'].mean()\n",
    "    std_inference_time = outer_fold_df['inference_time'].std()\n",
    "    mean_parameters = outer_fold_df['parameters'].mean()\n",
    "    std_parameters = outer_fold_df['parameters'].std()\n",
    "\n",
    "    if task_type == \"binary\":\n",
    "        mean_acc = outer_fold_df['acc'].mean()\n",
    "        std_acc = outer_fold_df['acc'].std()\n",
    "        mean_f1 = outer_fold_df['f1'].mean()\n",
    "        std_f1 = outer_fold_df['f1'].std()\n",
    "        mean_roc = outer_fold_df['roc'].mean()\n",
    "        std_roc = outer_fold_df['roc'].std()\n",
    "        \n",
    "        logger.info(f\"=== FINAL RESULTS FOR DATASET {dataset_id} ===\")\n",
    "        logger.info(f\"Balanced Accuracy: {mean_acc:.4f}  {std_acc:.4f}\")\n",
    "        logger.info(f\"F1 Score: {mean_f1:.4f}  {std_f1:.4f}\")\n",
    "        logger.info(f\"ROC AUC: {mean_roc:.4f}  {std_roc:.4f}\")\n",
    "    else:\n",
    "        mean_mae = outer_fold_df['mae'].mean()\n",
    "        std_mae = outer_fold_df['mae'].std()\n",
    "        mean_mse = outer_fold_df['mse'].mean()\n",
    "        std_mse = outer_fold_df['mse'].std()\n",
    "        mean_rmse = outer_fold_df['rmse'].mean()\n",
    "        std_rmse = outer_fold_df['rmse'].std()\n",
    "        mean_r2 = outer_fold_df['r2'].mean()\n",
    "        std_r2 = outer_fold_df['r2'].std()\n",
    "\n",
    "        \n",
    "        logger.info(f\"=== FINAL RESULTS FOR DATASET {dataset_id} ===\")\n",
    "        logger.info(f\"MAE: {mean_mae:.4f}  {std_mae:.4f}\")\n",
    "        logger.info(f\"MSE: {mean_mse:.4f}  {std_mse:.4f}\")\n",
    "        logger.info(f\"RMSE: {mean_rmse:.4f}  {std_rmse:.4f}\")\n",
    "        logger.info(f\"R2: {mean_r2:.4f}  {std_r2:.4f}\")\n",
    "    \n",
    "    logger.info(f\"Mean Inference Time: {mean_inference_time:.4f}  {std_inference_time:.4f}\")\n",
    "    logger.info(f\"Mean Parameters: {mean_parameters:.4f}  {std_parameters:.4f}\")\n",
    "\n",
    "    # Save summary statistics as well\n",
    "    summary_stats = {\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"model_type\": model_type,\n",
    "        \"task_type\": task_type,\n",
    "        \"seed\": config[\"training\"][\"random_state\"],\n",
    "        \"use_hpo\": config[\"training\"][\"use_hpo\"],\n",
    "        \"mean_inference_time\": mean_inference_time,\n",
    "        \"std_inference_time\": std_inference_time,\n",
    "        \"mean_parameters\": mean_parameters,\n",
    "        \"std_parameters\": std_parameters,\n",
    "    }\n",
    "\n",
    "    if task_type == \"binary\":\n",
    "        summary_stats.update({\n",
    "            \"mean_acc\": mean_acc,\n",
    "            \"std_acc\": std_acc,\n",
    "            \"mean_f1\": mean_f1,\n",
    "            \"std_f1\": std_f1,\n",
    "            \"mean_roc\": mean_roc,\n",
    "            \"std_roc\": std_roc,\n",
    "        })\n",
    "    else:\n",
    "        summary_stats.update({\n",
    "            \"mean_mae\": mean_mae,\n",
    "            \"std_mae\": std_mae,\n",
    "            \"mean_mse\": mean_mse,\n",
    "            \"std_mse\": std_mse,\n",
    "            \"mean_rmse\": mean_rmse,\n",
    "            \"std_rmse\": std_rmse,\n",
    "            \"mean_r2\": mean_r2,\n",
    "            \"std_r2\": std_r2,\n",
    "        })\n",
    "\n",
    "    # Load existing summary file if it exists, otherwise create new\n",
    "    summary_file = os.path.join(config[\"data\"][\"results_dir_path\"], f\"{dataset_id}_results.json\")\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(config[\"data\"][\"results_dir_path\"], exist_ok=True)\n",
    "    if os.path.exists(summary_file):\n",
    "        with open(summary_file, 'r') as f:\n",
    "            all_results = json.load(f)\n",
    "    else:\n",
    "        all_results = {}\n",
    "\n",
    "    # Add current model results to the dataset summary\n",
    "    # Use simple incremental numbering\n",
    "    next_num = len(all_results) + 1\n",
    "    model_key = str(next_num)\n",
    "\n",
    "    # Add current model results to the dataset summary\n",
    "    all_results[model_key] = summary_stats\n",
    "\n",
    "    # Save updated summary\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    logger.info(f\"Summary statistics saved to: {summary_file}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # SAVE FOLD INDICES (for reproducibility and student training)\n",
    "    # -------------------------------------------------------------------------\n",
    "    fold_indices_file = os.path.join(config[\"data\"][\"fold_indices_path\"], f\"dataset_{dataset_id}.json\")\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(config[\"data\"][\"fold_indices_path\"], exist_ok=True)\n",
    "    if not os.path.exists(fold_indices_file):\n",
    "        with open(fold_indices_file, 'w') as f:\n",
    "            json.dump(fold_indices, f, indent=2)\n",
    "        logger.info(f\"Fold indices saved to: {fold_indices_file}\")\n",
    "    else:\n",
    "        logger.info(f\"Fold indices file already exists: {fold_indices_file}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # SAVE TEACHER PREDICTIONS (targets for student training)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Combine predictions from all outer folds\n",
    "    if output_dfs:  # Check if we have any DataFrames to concatenate\n",
    "        output_df = pd.concat(output_dfs, ignore_index=True)\n",
    "    else:\n",
    "        output_df = pd.DataFrame(columns=[\"index\", \"output\"])\n",
    "    output_df = output_df.sort_values(by=\"index\")\n",
    "    \n",
    "    # Create the full directory structure\n",
    "    sub_folder = \"hpo\" if use_hpo else \"default\"\n",
    "    output_dir = os.path.join(config[\"data\"][\"output_dir_path\"], sub_folder, \"teacher\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    output_file = os.path.join(output_dir, f\"{dataset_id}_{model_type}.csv\")\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "    logger.info(f\"{config['model']['teacher_model']} outputs saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c1e150",
   "metadata": {},
   "source": [
    "# Train Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1289fc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 18:26:50,330 - __main__ - INFO - Loading dataset 23381 from cache at 'data/cache/openml_dataset_23381.pkl'...\n",
      "2025-06-04 18:26:50,331 - __main__ - INFO - Dataset 23381 loaded successfully with task type: binary\n",
      "2025-06-04 18:26:50,332 - __main__ - INFO - Using GPU: NVIDIA RTX A6000\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/cache/dataset_23381_fold_indices.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load the saved fold indices \u001b[39;00m\n\u001b[1;32m     14\u001b[0m fold_indices_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir_path\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fold_indices.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfold_indices_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     16\u001b[0m     fold_indices \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded fold indices from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_indices_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/cache/dataset_23381_fold_indices.json'"
     ]
    }
   ],
   "source": [
    "datasets = config[\"data\"][\"datasets\"]\n",
    "\n",
    "for dataset_id in datasets:\n",
    "    X, y, cat_cols, _, task_type = load_dataset(\n",
    "        dataset_id=dataset_id,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    device = check_GPU_availability()\n",
    "\n",
    "    set_seed(config[\"training\"][\"random_state\"])\n",
    "\n",
    "    # Load the saved fold indices \n",
    "    fold_indices_file = os.path.join(config[\"data\"][\"cache_dir_path\"], f\"dataset_{dataset_id}_fold_indices.json\")\n",
    "    with open(fold_indices_file, 'r') as f:\n",
    "        fold_indices = json.load(f)\n",
    "    print(f\"Loaded fold indices from: {fold_indices_file}\") \n",
    "\n",
    "    # Load the TabPFN outputs (teacher predictions)\n",
    "    tabpfn_outputs_file = os.path.join(config[\"data\"][\"cache_dir_path\"], f\"dataset_{dataset_id}_tabpfn_outputs.csv\")\n",
    "    teacher_outputs_df = pd.read_csv(tabpfn_outputs_file)\n",
    "    print(f\"Loaded TabPFN outputs from: {tabpfn_outputs_file}\")\n",
    "\n",
    "    # Convert probabilities to logits\n",
    "    # Clip probabilities to avoid log(0) or log(1)\n",
    "    eps = 1e-7\n",
    "    teacher_probs = np.clip(teacher_outputs_df['output'].values, eps, 1 - eps)\n",
    "    teacher_logits = np.log(teacher_probs / (1 - teacher_probs))\n",
    "    \n",
    "    # Create a mapping from index to logits for easy lookup\n",
    "    index_to_logits = dict(zip(teacher_outputs_df['index'].values, teacher_logits))\n",
    "\n",
    "    output_df = pd.DataFrame(columns=[\"index\", \"output\"])\n",
    "\n",
    "    for fold_key, fold_data in fold_indices[\"outer_folds\"].items():\n",
    "        outer_fold = int(fold_key.split('_')[1])\n",
    "        train_idx = np.array(fold_data[\"train_idx\"])\n",
    "        test_idx = np.array(fold_data[\"test_idx\"])\n",
    "\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        inner_folds_data = fold_indices[\"inner_folds\"][f\"outer_fold_{outer_fold}\"]\n",
    "\n",
    "        for inner_fold_key, inner_fold_data in inner_folds_data.items():\n",
    "            inner_fold = int(inner_fold_key.split('_')[2])\n",
    "            \n",
    "            # Get absolute indices from saved data\n",
    "            absolute_inner_train_idx = np.array(inner_fold_data[\"train_idx\"])\n",
    "            absolute_inner_val_idx = np.array(inner_fold_data[\"val_idx\"])\n",
    "            \n",
    "            # Convert absolute indices to relative indices for the current outer training set\n",
    "            inner_train_relative = np.where(np.isin(train_idx, absolute_inner_train_idx))[0]\n",
    "            inner_val_relative = np.where(np.isin(train_idx, absolute_inner_val_idx))[0]\n",
    "\n",
    "            X_inner_train, X_inner_val = X_train.iloc[inner_train_relative], X_train.iloc[inner_val_relative]\n",
    "            y_inner_train, y_inner_val = y_train[inner_train_relative], y_train[inner_val_relative]\n",
    "\n",
    "            # Get teacher logits for inner training and validation sets\n",
    "            teacher_logits_inner_train = np.array([index_to_logits[idx] for idx in absolute_inner_train_idx])\n",
    "            teacher_logits_inner_val = np.array([index_to_logits[idx] for idx in absolute_inner_val_idx])\n",
    "\n",
    "            # Preprocess the data for tree-based model\n",
    "            X_inner_train, X_inner_val = preprocess(\n",
    "                X_inner_train,\n",
    "                y_inner_train,\n",
    "                X_inner_val, \n",
    "                cat_cols,\n",
    "                config,\n",
    "                preprocessing_type=\"tree\",\n",
    "            )\n",
    "\n",
    "            # Train CatBoost regressor to predict teacher logits\n",
    "            catboost = CatBoostRegressor(verbose=False)\n",
    "            catboost.fit(X_inner_train, teacher_logits_inner_train)\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            val_pred_logits = catboost.predict(X_inner_val)\n",
    "            # Convert predicted logits back to probabilities for evaluation\n",
    "            val_probs = 1 / (1 + np.exp(-val_pred_logits))\n",
    "            val_preds = (val_probs > 0.5).astype(int)\n",
    "            \n",
    "            val_acc = balanced_accuracy_score(y_inner_val, val_preds)\n",
    "            val_f1 = f1_score(y_inner_val, val_preds, average=\"macro\")\n",
    "            val_roc_auc = roc_auc_score(y_inner_val, val_probs)\n",
    "            print(f\"Outer Fold {outer_fold}, Inner Fold {inner_fold} - Accuracy: {val_acc:.4f}, F1 Score: {val_f1:.4f}, ROC AUC: {val_roc_auc:.4f}\")\n",
    "\n",
    "        # Get teacher logits for outer training set\n",
    "        teacher_logits_train = np.array([index_to_logits[idx] for idx in train_idx])\n",
    "\n",
    "        # Preprocess the outer training and test sets\n",
    "        X_train, X_test = preprocess(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            cat_cols,\n",
    "            config,\n",
    "            preprocessing_type=\"tree\",\n",
    "        )\n",
    "        \n",
    "        # Retrain CatBoost regressor on the full training set for the outer fold\n",
    "        catboost = CatBoostRegressor(verbose=False)\n",
    "        catboost.fit(X_train, teacher_logits_train)\n",
    "\n",
    "        # Evaluate on the outer test set\n",
    "        test_pred_logits = catboost.predict(X_test)\n",
    "        # Convert predicted logits back to probabilities for evaluation\n",
    "        test_probs = 1 / (1 + np.exp(-test_pred_logits))\n",
    "        test_preds = (test_probs > 0.5).astype(int)\n",
    "        \n",
    "        test_acc = balanced_accuracy_score(y_test, test_preds)\n",
    "        test_f1 = f1_score(y_test, test_preds, average=\"macro\")\n",
    "        test_roc_auc = roc_auc_score(y_test, test_probs)\n",
    "        print(f\"Outer Fold {outer_fold} - Test Accuracy: {test_acc:.4f}, Test F1 Score: {test_f1:.4f}, Test ROC AUC: {test_roc_auc:.4f}\")\n",
    "        \n",
    "        # Save student probabilities with indices\n",
    "        output_df = pd.concat(\n",
    "            [\n",
    "                output_df,\n",
    "                pd.DataFrame({\n",
    "                    \"index\": test_idx,\n",
    "                    \"output\": test_probs  # Student probabilities (converted from predicted logits)\n",
    "                })\n",
    "            ],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "    # Export the output DataFrame to a CSV file\n",
    "    output_df = output_df.sort_values(by=\"index\")\n",
    "    output_file = os.path.join(config[\"data\"][\"cache_dir_path\"], f\"dataset_{dataset_id}_catboost_outputs_student.csv\")\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "    print(f\"CatBoost student outputs saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23cc6db",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b141aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Validating Fold Indices ===\n",
      "Total samples in dataset: 500\n",
      "Total test samples across all folds: 500\n",
      "Unique test samples: 500\n",
      " All samples appear exactly once in test sets across folds\n",
      " Teacher outputs cover all dataset indices\n",
      " Fold 1: No overlap between train/test\n",
      " Fold 1, Inner fold inner_fold_1: Indices are subset of outer train\n",
      " Fold 1, Inner fold inner_fold_2: Indices are subset of outer train\n",
      " Fold 1: Inner folds cover all outer training data\n",
      " Fold 2: No overlap between train/test\n",
      " Fold 2, Inner fold inner_fold_1: Indices are subset of outer train\n",
      " Fold 2, Inner fold inner_fold_2: Indices are subset of outer train\n",
      " Fold 2: Inner folds cover all outer training data\n",
      " Fold 3: No overlap between train/test\n",
      " Fold 3, Inner fold inner_fold_1: Indices are subset of outer train\n",
      " Fold 3, Inner fold inner_fold_2: Indices are subset of outer train\n",
      " Fold 3: Inner folds cover all outer training data\n",
      " Fold 4: No overlap between train/test\n",
      " Fold 4, Inner fold inner_fold_1: Indices are subset of outer train\n",
      " Fold 4, Inner fold inner_fold_2: Indices are subset of outer train\n",
      " Fold 4: Inner folds cover all outer training data\n",
      " Fold 5: No overlap between train/test\n",
      " Fold 5, Inner fold inner_fold_1: Indices are subset of outer train\n",
      " Fold 5, Inner fold inner_fold_2: Indices are subset of outer train\n",
      " Fold 5: Inner folds cover all outer training data\n",
      "=== End Validation ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add this validation code after loading the fold indices and before the main loop\n",
    "print(\"=== Validating Fold Indices ===\")\n",
    "\n",
    "# 1. Check that all indices are within valid range\n",
    "total_samples = len(X)\n",
    "print(f\"Total samples in dataset: {total_samples}\")\n",
    "\n",
    "# 2. Collect all test indices across outer folds\n",
    "all_test_indices = []\n",
    "for fold_key, fold_data in fold_indices[\"outer_folds\"].items():\n",
    "    test_idx = np.array(fold_data[\"test_idx\"])\n",
    "    all_test_indices.extend(test_idx)\n",
    "\n",
    "all_test_indices = np.array(all_test_indices)\n",
    "print(f\"Total test samples across all folds: {len(all_test_indices)}\")\n",
    "print(f\"Unique test samples: {len(np.unique(all_test_indices))}\")\n",
    "\n",
    "# 3. Check if all samples appear exactly once in test sets\n",
    "if len(all_test_indices) == len(np.unique(all_test_indices)) == total_samples:\n",
    "    print(\" All samples appear exactly once in test sets across folds\")\n",
    "else:\n",
    "    print(\" Issue with test set coverage!\")\n",
    "\n",
    "# 4. Check teacher outputs coverage\n",
    "teacher_indices = set(teacher_outputs_df['index'].values)\n",
    "dataset_indices = set(range(total_samples))\n",
    "if teacher_indices == dataset_indices:\n",
    "    print(\" Teacher outputs cover all dataset indices\")\n",
    "else:\n",
    "    missing = dataset_indices - teacher_indices\n",
    "    extra = teacher_indices - dataset_indices\n",
    "    print(f\" Teacher outputs mismatch - Missing: {missing}, Extra: {extra}\")\n",
    "\n",
    "# 5. Validate fold structure\n",
    "for fold_key, fold_data in fold_indices[\"outer_folds\"].items():\n",
    "    outer_fold = int(fold_key.split('_')[1])\n",
    "    train_idx = np.array(fold_data[\"train_idx\"])\n",
    "    test_idx = np.array(fold_data[\"test_idx\"])\n",
    "    \n",
    "    # Check no overlap between train and test\n",
    "    overlap = np.intersect1d(train_idx, test_idx)\n",
    "    if len(overlap) == 0:\n",
    "        print(f\" Fold {outer_fold}: No overlap between train/test\")\n",
    "    else:\n",
    "        print(f\" Fold {outer_fold}: Found {len(overlap)} overlapping indices!\")\n",
    "    \n",
    "    # Check inner folds\n",
    "    inner_folds_data = fold_indices[\"inner_folds\"][f\"outer_fold_{outer_fold}\"]\n",
    "    inner_train_indices = []\n",
    "    inner_val_indices = []\n",
    "    \n",
    "    for inner_fold_key, inner_fold_data in inner_folds_data.items():\n",
    "        inner_train_idx = np.array(inner_fold_data[\"train_idx\"])\n",
    "        inner_val_idx = np.array(inner_fold_data[\"val_idx\"])\n",
    "        \n",
    "        # Check inner indices are subset of outer train indices\n",
    "        if np.all(np.isin(inner_train_idx, train_idx)) and np.all(np.isin(inner_val_idx, train_idx)):\n",
    "            print(f\" Fold {outer_fold}, Inner fold {inner_fold_key}: Indices are subset of outer train\")\n",
    "        else:\n",
    "            print(f\" Fold {outer_fold}, Inner fold {inner_fold_key}: Indices not subset of outer train!\")\n",
    "        \n",
    "        inner_train_indices.extend(inner_train_idx)\n",
    "        inner_val_indices.extend(inner_val_idx)\n",
    "    \n",
    "    # Check inner folds cover all outer training data\n",
    "    inner_all = np.unique(np.concatenate([inner_train_indices, inner_val_indices]))\n",
    "    if len(np.setdiff1d(train_idx, inner_all)) == 0:\n",
    "        print(f\" Fold {outer_fold}: Inner folds cover all outer training data\")\n",
    "    else:\n",
    "        missing_in_inner = np.setdiff1d(train_idx, inner_all)\n",
    "        print(f\" Fold {outer_fold}: {len(missing_in_inner)} samples missing from inner folds\")\n",
    "\n",
    "print(\"=== End Validation ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc9aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validating Teacher Logits for Fold 5 ===\n",
      " All 400 training indices have teacher outputs\n",
      " WARNING: 100 test indices found in teacher outputs - possible data leakage!\n",
      "Sample teacher logits: [-0.5883296581435397, -0.79494873035179, -0.49716914708297155, -0.6096161248627843, -0.4098342731792005]\n",
      "Converted to probabilities: [0.3570182  0.31110707 0.37820616 0.35214677 0.39895186]\n",
      "=== End Teacher Logits Validation ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add this inside the main loop, after creating index_to_logits mapping\n",
    "print(f\"\\n=== Validating Teacher Logits for Fold {outer_fold} ===\")\n",
    "\n",
    "# Check if all training indices have corresponding teacher outputs\n",
    "missing_teacher_outputs = []\n",
    "for idx in train_idx:\n",
    "    if idx not in index_to_logits:\n",
    "        missing_teacher_outputs.append(idx)\n",
    "\n",
    "if len(missing_teacher_outputs) == 0:\n",
    "    print(f\" All {len(train_idx)} training indices have teacher outputs\")\n",
    "else:\n",
    "    print(f\" Missing teacher outputs for {len(missing_teacher_outputs)} indices: {missing_teacher_outputs[:10]}...\")\n",
    "\n",
    "# Check if test indices have teacher outputs (they shouldn't for proper validation)\n",
    "test_in_teacher = []\n",
    "for idx in test_idx:\n",
    "    if idx in index_to_logits:\n",
    "        test_in_teacher.append(idx)\n",
    "\n",
    "if len(test_in_teacher) == 0:\n",
    "    print(f\" No test indices found in teacher outputs (proper data leakage prevention)\")\n",
    "else:\n",
    "    print(f\" WARNING: {len(test_in_teacher)} test indices found in teacher outputs - possible data leakage!\")\n",
    "\n",
    "# Sample a few logits to check they're reasonable\n",
    "sample_indices = train_idx[:5]\n",
    "sample_logits = [index_to_logits[idx] for idx in sample_indices]\n",
    "sample_probs = 1 / (1 + np.exp(-np.array(sample_logits)))\n",
    "print(f\"Sample teacher logits: {sample_logits}\")\n",
    "print(f\"Converted to probabilities: {sample_probs}\")\n",
    "print(\"=== End Teacher Logits Validation ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b55ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Validation ===\n",
      " Student output file created with 500 predictions\n",
      " Student predictions cover exactly the expected test indices\n",
      " All student outputs are valid probabilities [0,1]\n",
      "  Range: [0.2589, 0.7412]\n",
      "  Mean: 0.4127\n",
      "=== End Final Validation ===\n"
     ]
    }
   ],
   "source": [
    "# Add this at the very end to verify your student outputs match expectations\n",
    "print(\"=== Final Validation ===\")\n",
    "\n",
    "# Check output file was created and has expected structure\n",
    "if os.path.exists(output_file):\n",
    "    student_df = pd.read_csv(output_file)\n",
    "    print(f\" Student output file created with {len(student_df)} predictions\")\n",
    "    \n",
    "    # Check all test indices are covered\n",
    "    predicted_indices = set(student_df['index'].values)\n",
    "    expected_test_indices = set(all_test_indices)\n",
    "    \n",
    "    if predicted_indices == expected_test_indices:\n",
    "        print(\" Student predictions cover exactly the expected test indices\")\n",
    "    else:\n",
    "        missing = expected_test_indices - predicted_indices\n",
    "        extra = predicted_indices - expected_test_indices\n",
    "        print(f\" Prediction coverage mismatch - Missing: {len(missing)}, Extra: {len(extra)}\")\n",
    "    \n",
    "    # Check output values are reasonable probabilities\n",
    "    outputs = student_df['output'].values\n",
    "    if np.all((outputs >= 0) & (outputs <= 1)):\n",
    "        print(f\" All student outputs are valid probabilities [0,1]\")\n",
    "        print(f\"  Range: [{outputs.min():.4f}, {outputs.max():.4f}]\")\n",
    "        print(f\"  Mean: {outputs.mean():.4f}\")\n",
    "    else:\n",
    "        invalid_count = np.sum((outputs < 0) | (outputs > 1))\n",
    "        print(f\" {invalid_count} invalid probability values found!\")\n",
    "        \n",
    "else:\n",
    "    print(\" Student output file was not created!\")\n",
    "\n",
    "print(\"=== End Final Validation ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
