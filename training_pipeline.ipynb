{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf3d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, f1_score, roc_auc_score,\n",
    "    mean_absolute_error, mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, OneHotEncoder, LabelEncoder, TargetEncoder\n",
    ")\n",
    "\n",
    "# Deep learning imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "\n",
    "# Model-specific imports\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from GRANDE import GRANDE\n",
    "from tabpfn import TabPFNClassifier, TabPFNRegressor\n",
    "\n",
    "# Optimization imports\n",
    "import optuna\n",
    "\n",
    "# Data source imports\n",
    "import openml\n",
    "\n",
    "# Abstract base class imports\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Literal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae3463",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bf7565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    Set random seeds for reproducibility across Python, NumPy, PyTorch, and TensorFlow.\n",
    "\n",
    "    Args:\n",
    "        seed: Integer seed for random number generators\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "def setup_logging() -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Set up logging configuration.\n",
    "    This function configures the logging settings for the application, including\n",
    "    the logging level and format. It returns a logger instance that can be used\n",
    "    throughout the application.\n",
    "    Returns:\n",
    "        logging.Logger: Configured logger instance.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def check_GPU_availability():\n",
    "    \"\"\"\n",
    "    Checks if a GPU is available and configures TensorFlow to use it appropriately.\n",
    "\n",
    "    Sets up memory growth for TensorFlow GPU usage to avoid allocating all GPU memory at once.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: A torch device object ('cuda' if GPU is available, 'cpu' otherwise)\n",
    "    \"\"\"\n",
    "    logger = setup_logging()\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"Using GPU: {torch.cuda.get_device_name()}\")\n",
    "        device = torch.device(\"cuda\")\n",
    "        gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        logger.info(\"Using CPU for training.\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14164da",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3635ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target(y):\n",
    "    \"\"\"\n",
    "    Encode the target variable using LabelEncoder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array\n",
    "        The target variable to encode.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_encoded : np.array\n",
    "        The encoded target variable.\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    return y_encoded\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_val: pd.DataFrame,\n",
    "    cat_cols: list,\n",
    "    config: dict,\n",
    "    preprocessing_type: str,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Preprocess the training and validation datasets based on the specified model type.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : pandas.DataFrame\n",
    "        The training dataset features.\n",
    "    y_train : pandas.Series or np.array\n",
    "        The target variable for the training dataset.\n",
    "    X_val : pandas.DataFrame\n",
    "        The validation dataset features.\n",
    "    cat_cols : list or array of bool\n",
    "        Boolean mask indicating which columns in X_train and X_val are categorical (True).\n",
    "    config : dict\n",
    "        Configuration dictionary containing model and preprocessing details.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train : np.array\n",
    "        The preprocessed training dataset features.\n",
    "    X_val : np.array\n",
    "        The preprocessed validation dataset features.\n",
    "    \"\"\"\n",
    "    X_train, preprocessor_inner = _minimal_preprocess_train(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cat_cols,\n",
    "        preprocessing_type,\n",
    "        config,\n",
    "    )\n",
    "    X_val = _minimal_preprocess_test(X_val, preprocessor_inner)\n",
    "\n",
    "    return X_train, X_val\n",
    "\n",
    "\n",
    "def _minimal_preprocess_train(X, y, categorical_features, model_type, config):\n",
    "    \"\"\"\n",
    "    Perform minimal preprocessing on the input features for training.\n",
    "\n",
    "    This function applies preprocessing to both numeric and categorical features\n",
    "    based on the model type specified in the configuration. For Neural Networks,\n",
    "    it standardizes numeric features and one-hot encodes categorical features. For\n",
    "    tree-based models, it does not scale numeric features and applies different\n",
    "    encoding strategies for low- and high-cardinality categorical features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pandas.DataFrame\n",
    "        The input features.\n",
    "\n",
    "    categorical_features : list or array of bool\n",
    "        Boolean mask indicating which columns in X are categorical (True) and\n",
    "        which are numeric (False).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_preprocessed : np.array\n",
    "        The preprocessed features.\n",
    "    preprocessor : ColumnTransformer\n",
    "        The fitted preprocessor, which can be used to transform future datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the boolean mask to a NumPy array (if not already) for fast indexing.\n",
    "    cat_mask = np.asarray(categorical_features)\n",
    "\n",
    "    # Check that the mask length matches the number of columns in X.\n",
    "    if cat_mask.shape[0] != X.shape[1]:\n",
    "        raise ValueError(\n",
    "            \"Length of categorical_features mask must match the number of columns in X.\"\n",
    "        )\n",
    "\n",
    "    # Extract column names using boolean indexing.\n",
    "    categorical_feature_names = X.columns[cat_mask].tolist()\n",
    "    numeric_feature_names = X.columns[~cat_mask].tolist()\n",
    "\n",
    "    # Select pipeline based on config.\n",
    "    logger = setup_logging()\n",
    "\n",
    "    # Neural Networks: standardize numeric features and one-hot encode categoricals.\n",
    "    if model_type == \"nn\":\n",
    "\n",
    "        # Define the preprocessing pipeline for numeric features.\n",
    "        numeric_pipeline = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "                (\"scaler\", StandardScaler()),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Define the preprocessing pipeline for categorical features.\n",
    "        categorical_pipeline = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\n",
    "                    \"onehot\",\n",
    "                    OneHotEncoder(\n",
    "                        drop=\"first\", sparse_output=False, handle_unknown=\"ignore\"\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        transformers = [\n",
    "            (\"num\", numeric_pipeline, numeric_feature_names),\n",
    "            (\"cat\", categorical_pipeline, categorical_feature_names),\n",
    "        ]\n",
    "\n",
    "        logger.info(\"Preprocessing pipeline for Neural Network model.\")\n",
    "\n",
    "    elif model_type == \"tree\":\n",
    "\n",
    "        # Tree-based models: leave numeric features unscaled, and differentiate encoding for categoricals.\n",
    "        numeric_pipeline = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            ]\n",
    "        )\n",
    "        # Split categorical features into low- and high-cardinality groups.\n",
    "        threshold = config[\"preprocessing\"][\"threshold_high_cardinality\"]\n",
    "        low_card_features = []\n",
    "        high_card_features = []\n",
    "        for feature in categorical_feature_names:\n",
    "            if X[feature].nunique() <= threshold:\n",
    "                low_card_features.append(feature)\n",
    "            else:\n",
    "                high_card_features.append(feature)\n",
    "\n",
    "        # For low-cardinality categoricals, use one-hot encoding.\n",
    "        low_card_pipeline = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\n",
    "                    \"onehot\",\n",
    "                    OneHotEncoder(\n",
    "                        drop=\"first\", sparse_output=False, handle_unknown=\"ignore\"\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # For high-cardinality categoricals, use target encoding.\n",
    "        high_card_pipeline = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"target_encode\", TargetEncoder()),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        transformers = [\n",
    "            (\"num\", numeric_pipeline, numeric_feature_names),\n",
    "            (\"low_card\", low_card_pipeline, low_card_features),\n",
    "            (\"high_card\", high_card_pipeline, high_card_features),\n",
    "        ]\n",
    "\n",
    "        logger.info(\"Preprocessing pipeline for Decision Tree model.\")\n",
    "\n",
    "    elif model_type == \"grande\":\n",
    "        logger.info(\"No preprocessing required for Grande model.\")\n",
    "        return X, None\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Unknown teacher type. Must be 'nn' for Neural Network, 'tree' for Decision Tree, or 'grande' for Gradient Based Tree.\"\n",
    "        )\n",
    "\n",
    "    # Create a ColumnTransformer to apply the transformations to the appropriate columns.\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        n_jobs=-2,\n",
    "    )\n",
    "\n",
    "    # Fit the preprocessor on the training data and transform it.\n",
    "    X_preprocessed = preprocessor.fit_transform(X, y)\n",
    "\n",
    "    return X_preprocessed, preprocessor\n",
    "\n",
    "\n",
    "def _minimal_preprocess_test(X, preprocessor):\n",
    "    \"\"\"\n",
    "    Transform the input features according to the preprocessor fitted on the training data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pandas.DataFrame\n",
    "        The input features.\n",
    "    preprocessor : ColumnTransformer\n",
    "        The preprocessor fitted on the training data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_preprocessed : np.array\n",
    "        The preprocessed features.\n",
    "    \"\"\"\n",
    "    if preprocessor is None:\n",
    "        return X\n",
    "    return preprocessor.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d700d2",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f642a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_id: int, config: dict):\n",
    "    \"\"\"\n",
    "    Loads an OpenML dataset based on its ID and processes it according to the task type.\n",
    "\n",
    "    This function fetches the dataset using the _fetch_dataset helper, then processes\n",
    "    the target variable based on whether the task is binary classification or regression.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_id (int): The ID of the dataset on OpenML.\n",
    "        config (dict): Configuration dictionary containing data paths and settings.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - X: The feature data (pandas DataFrame)\n",
    "            - y: The processed target variable (encoded for binary classification or numpy array for regression)\n",
    "            - cat_cols: List of booleans indicating which features are categorical\n",
    "            - attribute_names: List of attribute names\n",
    "            - task_type: String indicating the task type ('binary' or 'regression')\n",
    "    \"\"\"\n",
    "    X, y, cat_cols, attribute_names, task_type = fetch_dataset(\n",
    "        dataset_id=dataset_id,\n",
    "        cache_dir=config[\"data\"][\"cache_dir_path\"],\n",
    "    )\n",
    "    if task_type == \"binary\":\n",
    "        # Encode the target variable if it's binary\n",
    "        y = encode_target(y)\n",
    "    else:\n",
    "        # Transform to np.array for regression\n",
    "        y = np.array(y, dtype=float)\n",
    "\n",
    "    return X, y, cat_cols, attribute_names, task_type\n",
    "\n",
    "\n",
    "def fetch_dataset(dataset_id: int, cache_dir: str = \"data/cache/\"):\n",
    "    \"\"\"\n",
    "    Downloads an OpenML dataset based on its id, caches the data locally, and returns the data\n",
    "    along with its metadata.\n",
    "\n",
    "    The five returned objects are:\n",
    "        - X: The feature data (typically a pandas DataFrame).\n",
    "        - y: The target variable.\n",
    "        - categorical_indicator: A list of booleans indicating which features are categorical.\n",
    "        - attribute_names: A list of attribute names.\n",
    "        - task_type: A string indicating the type of task ('binary' or 'regression').\n",
    "\n",
    "    If the dataset has been previously downloaded and stored in the cache directory,\n",
    "    it will be loaded from the local file instead of re-downloading.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_id (int): The id of the dataset on OpenML.\n",
    "        cache_dir (str): The directory to store the downloaded dataset. Defaults to \"openml_cache\".\n",
    "\n",
    "    Returns:\n",
    "        X, y, categorical_indicator, attribute_names, task_type\n",
    "    \"\"\"\n",
    "    logger = setup_logging()\n",
    "\n",
    "    # Ensure the cache directory exists\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    # Define a cache file name that is unique to the dataset id\n",
    "    cache_file = os.path.join(cache_dir, f\"openml_dataset_{dataset_id}.pkl\")\n",
    "\n",
    "    # If the cache file exists, load the data from the file.\n",
    "    if os.path.exists(cache_file):\n",
    "        logger.info(\n",
    "            f\"Loading dataset {dataset_id} from cache at '{cache_file}'...\"\n",
    "        )\n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        X = data[\"X\"]\n",
    "        y = data[\"y\"]\n",
    "        categorical_indicator = data[\"categorical_indicator\"]\n",
    "        attribute_names = data[\"attribute_names\"]\n",
    "        task_type = data[\"task_type\"]\n",
    "    else:\n",
    "        # Download the dataset from OpenML.\n",
    "        logger.info(f\"Downloading dataset {dataset_id} from OpenML...\")\n",
    "        dataset = openml.datasets.get_dataset(dataset_id)\n",
    "\n",
    "        # Use the default target attribute (if defined) when retrieving the data.\n",
    "        X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "            target=dataset.default_target_attribute, dataset_format=\"dataframe\"\n",
    "        )\n",
    "\n",
    "        # Determine the task type (binary classification or regression)\n",
    "        task_type = _determine_task_type(y)\n",
    "\n",
    "        # Store the data and metadata in a dictionary.\n",
    "        data = {\n",
    "            \"X\": X,\n",
    "            \"y\": y,\n",
    "            \"categorical_indicator\": categorical_indicator,\n",
    "            \"attribute_names\": attribute_names,\n",
    "            \"task_type\": task_type,\n",
    "        }\n",
    "\n",
    "        # Save the dictionary to a local file using pickle.\n",
    "        with open(cache_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        logger.info(f\"Dataset {dataset_id} stored locally at '{cache_file}'.\")\n",
    "\n",
    "    logger.info(f\"Dataset {dataset_id} loaded successfully with task type: {task_type}\")\n",
    "\n",
    "    return X, y, categorical_indicator, attribute_names, task_type\n",
    "\n",
    "\n",
    "def _determine_task_type(y):\n",
    "    \"\"\"\n",
    "    Determines if the target variable is for binary classification, or regression.\n",
    "\n",
    "    Parameters:\n",
    "        y: The target variable (pandas dataframe).\n",
    "\n",
    "    Returns:\n",
    "        str: 'binary', or 'regression'\n",
    "    \"\"\"\n",
    "\n",
    "    # Get unique values\n",
    "    unique_values = pd.unique(y)\n",
    "\n",
    "    # Check if it's binary (2 unique values)\n",
    "    if len(unique_values) == 2:\n",
    "        return \"binary\"\n",
    "    else:\n",
    "        return \"regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4579812",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee7681de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(y_prob, y_true):\n",
    "    \"\"\"\n",
    "    Evaluate classification performance using balanced accuracy, F1 score, and ROC AUC.\n",
    "\n",
    "    Parameters:\n",
    "        y_prob (np.array): Predicted probabilities for the positive class.\n",
    "        y_true (np.array): True labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    logger = setup_logging()\n",
    "    y_pred = (y_prob[:, 1] > 0.5).astype(int)\n",
    "\n",
    "    acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    roc_auc = roc_auc_score(y_true, y_prob[:, 1])\n",
    "\n",
    "    logger.info(f\"\\t Balanced Accuracy: {acc:.4f}, F1 Score: {f1:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"roc\": roc_auc,\n",
    "    }\n",
    "\n",
    "def evaluate_regression(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Evaluate regression performance using R^2 score.\n",
    "\n",
    "    Parameters:\n",
    "        y_pred (np.array): Predicted values.\n",
    "        y_true (np.array): True values.\n",
    "\n",
    "    Returns:\n",
    "        float: R^2 score.\n",
    "    \"\"\"\n",
    "    logger = setup_logging()\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    logger.info(f\"\\t MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R^2: {r2:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"mae\": mae,\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\": r2,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8af1a5",
   "metadata": {},
   "source": [
    "# Model Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9825c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherModelBase(ABC):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.params = kwargs\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model on the given data.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.ndarray): The input features with shape (n_samples, n_features).\n",
    "            y (np.ndarray): The target labels with shape (n_samples,).\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Makes predictions on the provided data.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.ndarray): The input features with shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: An array of predicted probabilities with shape (n_samples, n_classes).\n",
    "                        (If the model returns logits, these should be post-processed to probabilities.)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, y_pred: np.ndarray, y_true: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluates the model's predictions against the true labels.\n",
    "\n",
    "        Parameters:\n",
    "            y_pred (np.ndarray): The predicted probabilities or values.\n",
    "            y_true (np.ndarray): The true labels.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: A dictionary containing evaluation metrics.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            metrics = evaluate_classification(y_pred, y_true)\n",
    "            metrics['parameters'] = self.count_parameters()\n",
    "            return metrics\n",
    "        \n",
    "        elif self.task_type == \"regression\":\n",
    "            metrics = evaluate_regression(y_pred, y_true)\n",
    "            metrics['parameters'] = self.count_parameters()\n",
    "            return metrics\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown task type: {self.task_type}. Must be 'binary' or 'regression'.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def count_parameters(self) -> int:\n",
    "        \"\"\"\n",
    "        Counts the number of trainable parameters in the model.\n",
    "\n",
    "        Returns:\n",
    "            int: The total number of trainable parameters.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class TabPFNTeacherModel(TeacherModelBase):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a TabPFN teacher model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments to pass to the base\n",
    "            class constructor.\n",
    "        \"\"\"\n",
    "        self.device = kwargs.pop(\"device\")\n",
    "        self.config = kwargs.pop(\"config\")\n",
    "        self.task_type = kwargs.pop(\"task_type\")\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def train(self, X, y, **training_params):\n",
    "        if self.task_type == \"binary\":\n",
    "            self.model = TabPFNClassifier()\n",
    "        else:\n",
    "            self.model = TabPFNRegressor()\n",
    "        # If X is bigger than 10000 samples, reduce data size to 10000\n",
    "        if X.shape[0] > 10000:\n",
    "            X = X[:10000]\n",
    "            y = y[:10000]\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.task_type == \"binary\":\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            return self.model.predict(X)\n",
    "\n",
    "    def count_parameters(self) -> int:\n",
    "        \"\"\"\n",
    "        Counts the number of trainable parameters in the underlying TabPFN model.\n",
    "\n",
    "        Returns:\n",
    "            int: Total number of trainable parameters.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the TabPFN model has not been fitted yet.\n",
    "        \"\"\"\n",
    "        if not hasattr(self.model, \"model_\"):\n",
    "            raise ValueError(\n",
    "                \"The TabPFN model is not fitted yet. Please call fit() first.\"\n",
    "            )\n",
    "        return sum(p.numel() for p in self.model.model_.parameters() if p.requires_grad)\n",
    "    \n",
    "\n",
    "class CatBoostTeacherModel(TeacherModelBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a CatBoost teacher model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments to pass to the base class constructor.\n",
    "        \"\"\"\n",
    "        self.device = kwargs.pop(\"device\")\n",
    "        self.config = kwargs.pop(\"config\")\n",
    "        self.task_type = kwargs.pop(\"task_type\")\n",
    "        self.hyperparams = kwargs.pop(\"hyperparams\")\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the CatBoost model using the provided data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target data.\n",
    "        **training_params : dict\n",
    "            Additional training parameters to pass to the CatBoost model's fit method.\n",
    "        \"\"\"\n",
    "        config = self.config\n",
    "        random_state = config[\"training\"][\"random_state\"]\n",
    "\n",
    "        # Initialize the CatBoost model\n",
    "        if self.task_type == \"binary\":\n",
    "            self.model = CatBoostClassifier(\n",
    "                random_seed=random_state,\n",
    "                thread_count=-1,  # Use all available CPU cores\n",
    "                logging_level=\"Silent\",  # Suppress CatBoost output\n",
    "                task_type=(\"GPU\" if str(self.device).lower() == \"cuda\" else \"CPU\"),\n",
    "                **self.hyperparams,  # Include the sampled hyperparameters\n",
    "            )\n",
    "        else:\n",
    "            self.model = CatBoostRegressor(\n",
    "                random_seed=random_state,\n",
    "                thread_count=-1,  # Use all available CPU cores\n",
    "                logging_level=\"Silent\",  # Suppress CatBoost output\n",
    "                task_type=(\"GPU\" if str(self.device).lower() == \"cuda\" else \"CPU\"),\n",
    "                **self.hyperparams,  # Include the sampled hyperparameters\n",
    "            )\n",
    "\n",
    "        # Split data into training and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=random_state\n",
    "        )\n",
    "        # Train the model\n",
    "        self.model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            early_stopping_rounds=20, \n",
    "        )\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on the given data using the trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array-like of shape (n_samples, n_classes)\n",
    "            The predicted probabilities for each class.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            return self.model.predict(X)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"\n",
    "        Count the number of parameters in the trained CatBoost model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The number of parameters in the model.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"model\") or self.model is None:\n",
    "            raise ValueError(\"The model has not been trained yet.\")\n",
    "\n",
    "        # Get the number of trees in the model\n",
    "        tree_count = self.model.tree_count_\n",
    "\n",
    "        # For each tree, count:\n",
    "        # - split values (one per non-leaf node)\n",
    "        # - leaf values (one per leaf node)\n",
    "        # - feature indices (one per non-leaf node)\n",
    "        # For a binary tree with depth d, there are 2^d - 1 non-leaf nodes and 2^d leaf nodes\n",
    "        # This is a simplified approximation\n",
    "        approx_depth = self.hyperparams.get(\"max_depth\", 6)  # Default depth in CatBoost is 6\n",
    "        non_leaf_nodes = 2**approx_depth - 1\n",
    "        leaf_nodes = 2**approx_depth\n",
    "\n",
    "        # Total parameters per tree\n",
    "        params_per_tree = (\n",
    "            non_leaf_nodes * 2 + leaf_nodes\n",
    "        )  # split values + feature indices + leaf values\n",
    "        return tree_count * params_per_tree\n",
    "    \n",
    "\n",
    "class GRANDETeacherModel(TeacherModelBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a GRANDE teacher model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments. The `device` parameter is popped from kwargs and\n",
    "            defaults to 'cpu' if not provided.\n",
    "        \"\"\"\n",
    "        self.device = kwargs.pop(\"device\")\n",
    "        self.config = kwargs.pop(\"config\")\n",
    "        self.task_type = kwargs.pop(\"task_type\")\n",
    "        self.hyperparams = kwargs.pop(\"hyperparams\")\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the GRANDE teacher model using the provided data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target data.\n",
    "        **training_params : dict\n",
    "            Additional training parameters to pass to the GRANDE model's fit method.\n",
    "        \"\"\"\n",
    "        config = self.config\n",
    "        random_state = config[\"training\"][\"random_state\"]\n",
    "        training_params = {}\n",
    "        training_params[\"random_seed\"] = random_state\n",
    "\n",
    "        # Instantiate the GRANDE teacher model.\n",
    "        self.model = GRANDE(params=self.hyperparams, args=training_params)\n",
    "\n",
    "        # Split the data into training and validation sets.\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=random_state\n",
    "        )\n",
    "        self.model.fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on the given input data using the trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array-like\n",
    "            The predicted target values.\n",
    "        \"\"\"\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"\n",
    "        Count the number of trainable parameters in the GRANDE teacher model based on the\n",
    "        dense representation of a single tree and the number of trees in the ensemble.\n",
    "\n",
    "        The number of parameters for a single tree is computed as:\n",
    "            leaf parameters: 2^d\n",
    "            split thresholds: (2^d - 1) * n\n",
    "            feature selection (one-hot): (2^d - 1) * n\n",
    "        so that:\n",
    "            params_per_tree = 2^d + 2 * n * (2^d - 1)\n",
    "        The total is then:\n",
    "            total_params = n_estimators * params_per_tree\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"model\"):\n",
    "            raise ValueError(\"The model has not been trained/initialized yet.\")\n",
    "\n",
    "        # These attributes must be set when the GRANDE model is instantiated.\n",
    "        # Adjust the attribute names if they are different in your GRANDE implementation.\n",
    "        d = self.model.depth  # Depth of each decision tree.\n",
    "        n = self.model.number_of_variables  # Number of features used in the tree.\n",
    "        E = self.model.n_estimators  # Number of trees in the ensemble.\n",
    "\n",
    "        # Calculate the number of parameters for one tree.\n",
    "        params_per_tree = (2**d) + 2 * n * ((2**d) - 1)\n",
    "        total_params = E * params_per_tree\n",
    "\n",
    "        return total_params\n",
    "    \n",
    "def get_teacher_model(config: dict, task_type: Literal[\"binary\", \"regression\"], device, hyperparams) -> TeacherModelBase:\n",
    "    \"\"\"\n",
    "    Returns an instance of the teacher model based on the configuration.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict\n",
    "        Configuration dictionary containing model and dataset details.\n",
    "    device : torch.device\n",
    "        The device to use for training (CPU or GPU).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TeacherModelBase\n",
    "        An instance of the teacher model.\n",
    "    \"\"\"\n",
    "    model_type = config[\"model\"][\"teacher_model\"]\n",
    "    if model_type == \"tabpfn\":\n",
    "        return TabPFNTeacherModel(config=config, task_type=task_type, device=device, hyperparams=hyperparams)\n",
    "    elif model_type == \"catboost\":\n",
    "        return CatBoostTeacherModel(config=config, task_type=task_type, device=device, hyperparams=hyperparams)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown teacher type: {config['model']['teacher_model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725d7a3c",
   "metadata": {},
   "source": [
    "# HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab890760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_hyperparameters(trial: optuna.Trial, model_type: str, task_type: str, use_hpo: bool) -> dict:\n",
    "    \"\"\"\n",
    "    Suggest hyperparameters for the given model type using Optuna.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    trial : optuna.Trial\n",
    "        Optuna trial object for suggesting hyperparameters.\n",
    "    model_type : str\n",
    "        Type of model ('tabpfn', 'catboost', etc.).\n",
    "    task_type : str\n",
    "        Type of task ('binary', 'regression').\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of suggested hyperparameters.\n",
    "    \"\"\"\n",
    "    hyperparameter = {}\n",
    "    \n",
    "    if model_type == \"catboost\":\n",
    "        if use_hpo:\n",
    "            hyperparameter = {\n",
    "                \"iterations\": trial.suggest_int(\"iterations\", 512, 4096),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 2, 12),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "                \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 0.5, 30),\n",
    "                \"boosting_type\": \"Plain\",\n",
    "            }\n",
    "        else:\n",
    "            hyperparameter = {\n",
    "                \"iterations\": 1000,\n",
    "                \"max_depth\": 6,\n",
    "                \"l2_leaf_reg\": 3,\n",
    "                \"boosting_type\": \"Plain\",\n",
    "            }\n",
    "\n",
    "        if task_type == \"binary\":\n",
    "            hyperparameter[\"loss_function\"] = \"Logloss\"\n",
    "        else:\n",
    "            hyperparameter[\"loss_function\"] = \"RMSE\"\n",
    "    \n",
    "    elif model_type == \"grande\":\n",
    "        if use_hpo:\n",
    "            hyperparameter = {\n",
    "                \"depth\": trial.suggest_int(\"depth\", 3, 7),\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 512, 4096),\n",
    "                \"learning_rate_weights\": trial.suggest_float(\"learning_rate_weights\", 0.0001, 0.05, log=True),\n",
    "                \"learning_rate_index\": trial.suggest_float(\"learning_rate_index\", 0.001, 0.2, log=True),\n",
    "                \"learning_rate_values\": trial.suggest_float(\"learning_rate_values\", 0.001, 0.2, log=True),\n",
    "                \"learning_rate_leaf\": trial.suggest_float(\"learning_rate_leaf\", 0.001, 0.2, log=True),\n",
    "                \"optimizer\": \"adam\",\n",
    "                \"cosine_decay_steps\": trial.suggest_categorical(\"cosine_decay_steps\", [0.0, 0.1, 1.0, 100.0, 1000.0]),\n",
    "                \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.75),\n",
    "                \"selected_variables\": trial.suggest_float(\"selected_variables\", 0.0, 1.0),\n",
    "                \"data_subset_fraction\": trial.suggest_float(\"data_subset_fraction\", 0.1, 1.0),\n",
    "                \"focal_loss\": False,\n",
    "                \"temperature\": 0.0,\n",
    "                \"from_logits\": True,\n",
    "                \"use_class_weights\": True,\n",
    "            }\n",
    "        else:\n",
    "            hyperparameter = {\n",
    "                \"depth\": 5,\n",
    "                \"n_estimators\": 2048,\n",
    "                \"learning_rate_weights\": 0.005,\n",
    "                \"learning_rate_index\": 0.01,\n",
    "                \"learning_rate_values\": 0.01,\n",
    "                \"learning_rate_leaf\": 0.01,\n",
    "                \"optimizer\": \"adam\",\n",
    "                \"cosine_decay_steps\": 0,\n",
    "                \"dropout\": 0.0,\n",
    "                \"selected_variables\": 0.8,\n",
    "                \"data_subset_fraction\": 1.0,\n",
    "                \"focal_loss\": False,\n",
    "                \"temperature\": 0.0,\n",
    "                \"from_logits\": True,\n",
    "                \"use_class_weights\": True,\n",
    "            }\n",
    "        \n",
    "        if task_type == \"binary\":\n",
    "            hyperparameter[\"loss\"] = \"crossentropy\"\n",
    "        else:\n",
    "            hyperparameter[\"loss\"] = \"mse\"\n",
    "    \n",
    "    return hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02857a",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b406d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data\": {\n",
    "        \"datasets\": [\n",
    "                    #  23381, # Binary classification datasets\n",
    "                       197, # Regression datasets\n",
    "                     ],  \n",
    "        \"cache_dir_path\": \"data/cache/\",\n",
    "        \"fold_indices_path\": \"data/fold_indices/\",\n",
    "        \"optuna_db_path\": \"data/optuna_db/\",\n",
    "        \"output_dir_path\": \"data/output/\",\n",
    "        \"outer_folds_path\": \"data/outer_fold/\",\n",
    "        \"results_dir_path\": \"results/\",\n",
    "    },\n",
    "\n",
    "    \"preprocessing\": {\n",
    "        \"threshold_high_cardinality\": 10,  \n",
    "    },\n",
    "\n",
    "    \"model\": {\n",
    "        \"teacher_model\": \"catboost\",   # Options: 'tabpfn', \n",
    "        \"student_model\": \"catboost\", # Options: 'catboost', \n",
    "    },\n",
    "\n",
    "    \"training\": {\n",
    "        \"use_hpo\": True,\n",
    "        \"trials\": 30,\n",
    "        \"random_state\": 42,\n",
    "        \"outer_folds\": 5,\n",
    "        \"inner_folds\": 2,\n",
    "    },\n",
    "\n",
    "    \"teacher_models\": {\n",
    "        \"tabpfn\": {\n",
    "            \"preprocessing\": \"nn\",  \n",
    "        },\n",
    "        \"catboost\": {\n",
    "            \"preprocessing\": \"tree\", \n",
    "        },\n",
    "        \"grande\": {\n",
    "            \"preprocessing\": \"grande\",  \n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17729927",
   "metadata": {},
   "source": [
    "# Train Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9dce1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 18:48:17,101 - __main__ - INFO - Loading configuration...\n",
      "2025-06-04 18:48:17,102 - __main__ - INFO - Loading dataset 197 from cache at 'data/cache/openml_dataset_197.pkl'...\n",
      "2025-06-04 18:48:17,104 - __main__ - INFO - Dataset 197 loaded successfully with task type: regression\n",
      "2025-06-04 18:48:17,105 - __main__ - INFO - Using GPU: NVIDIA RTX A6000\n",
      "2025-06-04 18:48:17,106 - __main__ - INFO - Random seed set to 42\n",
      "2025-06-04 18:48:17,108 - __main__ - INFO - -------------------- Outer Fold 1 --------------------\n",
      "2025-06-04 18:48:17,109 - __main__ - INFO - Starting hyperparameter optimization for outer fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 18:48:17,153] A new study created in RDB with name: 197.1.catboost\n",
      "2025-06-04 18:48:17,237 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:48:17,261 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:48:18,564 - __main__ - INFO - \t MAE: 1.7327, MSE: 6.2273, RMSE: 2.4954, R^2: 0.9824\n",
      "2025-06-04 18:48:18,576 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:48:18,600 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:48:20,744 - __main__ - INFO - \t MAE: 1.7108, MSE: 7.0259, RMSE: 2.6506, R^2: 0.9795\n",
      "[I 2025-06-04 18:48:20,834] Trial 0 finished with value: -1.7217622848878316 and parameters: {'iterations': 1000, 'max_depth': 6, 'learning_rate': 0.03574712922600244, 'l2_leaf_reg': 3.0}. Best is trial 0 with value: -1.7217622848878316.\n",
      "2025-06-04 18:48:20,891 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:48:20,915 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:48:23,433 - __main__ - INFO - \t MAE: 1.8891, MSE: 8.9455, RMSE: 2.9909, R^2: 0.9747\n",
      "2025-06-04 18:48:23,445 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:48:23,470 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:48:25,877 - __main__ - INFO - \t MAE: 1.8785, MSE: 11.1664, RMSE: 3.3416, R^2: 0.9674\n",
      "[I 2025-06-04 18:48:25,959] Trial 1 finished with value: -1.8838117314118694 and parameters: {'iterations': 3920, 'max_depth': 10, 'learning_rate': 0.07661100707771368, 'l2_leaf_reg': 5.102549893051878}. Best is trial 0 with value: -1.7217622848878316.\n",
      "2025-06-04 18:48:26,022 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:48:26,046 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:48:27,190 - __main__ - INFO - \t MAE: 1.9504, MSE: 7.0712, RMSE: 2.6592, R^2: 0.9800\n",
      "2025-06-04 18:48:27,202 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:48:27,226 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:48:28,658 - __main__ - INFO - \t MAE: 1.8326, MSE: 7.1924, RMSE: 2.6819, R^2: 0.9790\n",
      "[I 2025-06-04 18:48:28,741] Trial 2 finished with value: -1.89152857936037 and parameters: {'iterations': 1071, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'l2_leaf_reg': 18.232892846424658}. Best is trial 0 with value: -1.7217622848878316.\n",
      "2025-06-04 18:48:28,804 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:48:28,827 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:48:29,945 - __main__ - INFO - \t MAE: 1.9620, MSE: 7.1688, RMSE: 2.6775, R^2: 0.9797\n",
      "2025-06-04 18:48:29,957 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:48:29,981 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:48:31,421 - __main__ - INFO - \t MAE: 1.8252, MSE: 7.4641, RMSE: 2.7321, R^2: 0.9782\n",
      "[I 2025-06-04 18:48:31,497] Trial 3 finished with value: -1.8935526492361774 and parameters: {'iterations': 3050, 'max_depth': 2, 'learning_rate': 0.2708160864249968, 'l2_leaf_reg': 25.057057903612442}. Best is trial 0 with value: -1.7217622848878316.\n",
      "2025-06-04 18:48:31,554 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:48:31,577 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:48:33,667 - __main__ - INFO - \t MAE: 1.8437, MSE: 6.6288, RMSE: 2.5746, R^2: 0.9813\n",
      "2025-06-04 18:48:33,678 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:48:33,701 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:48:35,950 - __main__ - INFO - \t MAE: 1.7613, MSE: 7.0413, RMSE: 2.6535, R^2: 0.9794\n",
      "[I 2025-06-04 18:48:36,024] Trial 4 finished with value: -1.8024955214247176 and parameters: {'iterations': 1273, 'max_depth': 4, 'learning_rate': 0.018659959624904916, 'l2_leaf_reg': 9.475146167306363}. Best is trial 0 with value: -1.7217622848878316.\n",
      "2025-06-04 18:48:36,083 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:48:36,106 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:48:38,839 - __main__ - INFO - \t MAE: 1.7950, MSE: 6.8702, RMSE: 2.6211, R^2: 0.9806\n",
      "2025-06-04 18:48:38,850 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:48:38,874 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:48:41,725 - __main__ - INFO - \t MAE: 1.7776, MSE: 7.8061, RMSE: 2.7939, R^2: 0.9772\n",
      "[I 2025-06-04 18:48:41,800] Trial 5 finished with value: -1.7863068971594624 and parameters: {'iterations': 2393, 'max_depth': 6, 'learning_rate': 0.02692655251486473, 'l2_leaf_reg': 18.549660394310195}. Best is trial 0 with value: -1.7217622848878316.\n",
      "2025-06-04 18:48:41,858 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:48:41,882 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:48:43,853 - __main__ - INFO - \t MAE: 1.8013, MSE: 6.6374, RMSE: 2.5763, R^2: 0.9812\n",
      "2025-06-04 18:48:43,863 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:48:43,887 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:48:46,045 - __main__ - INFO - \t MAE: 1.7510, MSE: 7.0786, RMSE: 2.6606, R^2: 0.9793\n",
      "[I 2025-06-04 18:48:46,120] Trial 6 finished with value: -1.7761628942287357 and parameters: {'iterations': 1012, 'max_depth': 5, 'learning_rate': 0.03476649150592621, 'l2_leaf_reg': 13.95406453440256}. Best is trial 0 with value: -1.7217622848878316.\n",
      "2025-06-04 18:48:46,182 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:48:46,205 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:48:47,860 - __main__ - INFO - \t MAE: 1.8072, MSE: 6.6300, RMSE: 2.5749, R^2: 0.9813\n",
      "2025-06-04 18:48:47,870 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:48:47,894 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:48:49,624 - __main__ - INFO - \t MAE: 1.7626, MSE: 7.1295, RMSE: 2.6701, R^2: 0.9792\n",
      "[I 2025-06-04 18:48:49,696] Trial 7 finished with value: -1.7849398987162646 and parameters: {'iterations': 3326, 'max_depth': 4, 'learning_rate': 0.05748924681991978, 'l2_leaf_reg': 17.976229781430252}. Best is trial 0 with value: -1.7217622848878316.\n",
      "2025-06-04 18:48:49,755 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:48:49,778 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:48:52,024 - __main__ - INFO - \t MAE: 1.8514, MSE: 7.7807, RMSE: 2.7894, R^2: 0.9780\n",
      "2025-06-04 18:48:52,037 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:48:52,060 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:48:54,434 - __main__ - INFO - \t MAE: 1.8342, MSE: 9.1871, RMSE: 3.0310, R^2: 0.9732\n",
      "[I 2025-06-04 18:48:54,508] Trial 8 finished with value: -1.8428177551180966 and parameters: {'iterations': 678, 'max_depth': 8, 'learning_rate': 0.0178601378893971, 'l2_leaf_reg': 2.419021993065746}. Best is trial 0 with value: -1.7217622848878316.\n",
      "2025-06-04 18:48:54,564 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:48:54,587 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:49:01,505 - __main__ - INFO - \t MAE: 1.9863, MSE: 10.6204, RMSE: 3.2589, R^2: 0.9700\n",
      "2025-06-04 18:49:01,515 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:01,539 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:49:04,980 - __main__ - INFO - \t MAE: 2.0002, MSE: 12.9536, RMSE: 3.5991, R^2: 0.9622\n",
      "[I 2025-06-04 18:49:05,060] Trial 9 finished with value: -1.993259504083458 and parameters: {'iterations': 3913, 'max_depth': 12, 'learning_rate': 0.1563510870813346, 'l2_leaf_reg': 9.486106190614436}. Best is trial 0 with value: -1.7217622848878316.\n",
      "2025-06-04 18:49:05,133 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:05,156 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:49:09,654 - __main__ - INFO - \t MAE: 2.0059, MSE: 9.3664, RMSE: 3.0605, R^2: 0.9735\n",
      "2025-06-04 18:49:09,664 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:09,688 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:49:14,164 - __main__ - INFO - \t MAE: 1.9648, MSE: 11.3046, RMSE: 3.3622, R^2: 0.9670\n",
      "[I 2025-06-04 18:49:14,247] Trial 10 finished with value: -1.985375984297136 and parameters: {'iterations': 1805, 'max_depth': 8, 'learning_rate': 0.010260022325256576, 'l2_leaf_reg': 28.002565086397723}. Best is trial 0 with value: -1.7217622848878316.\n",
      "2025-06-04 18:49:14,325 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:14,349 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:49:15,976 - __main__ - INFO - \t MAE: 1.8465, MSE: 7.1336, RMSE: 2.6709, R^2: 0.9798\n",
      "2025-06-04 18:49:15,987 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:16,011 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:49:17,811 - __main__ - INFO - \t MAE: 1.8005, MSE: 8.2919, RMSE: 2.8796, R^2: 0.9758\n",
      "[I 2025-06-04 18:49:17,897] Trial 11 finished with value: -1.823467324053499 and parameters: {'iterations': 556, 'max_depth': 6, 'learning_rate': 0.040673163081829, 'l2_leaf_reg': 11.905813367915673}. Best is trial 0 with value: -1.7217622848878316.\n",
      "2025-06-04 18:49:17,973 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:17,997 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:49:19,221 - __main__ - INFO - \t MAE: 1.7383, MSE: 5.9136, RMSE: 2.4318, R^2: 0.9833\n",
      "2025-06-04 18:49:19,233 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:19,257 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:49:20,764 - __main__ - INFO - \t MAE: 1.7020, MSE: 6.0270, RMSE: 2.4550, R^2: 0.9824\n",
      "[I 2025-06-04 18:49:20,837] Trial 12 finished with value: -1.7201706195138742 and parameters: {'iterations': 1712, 'max_depth': 5, 'learning_rate': 0.07840773911146973, 'l2_leaf_reg': 0.7385660590132961}. Best is trial 12 with value: -1.7201706195138742.\n",
      "2025-06-04 18:49:20,907 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:20,930 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:49:22,383 - __main__ - INFO - \t MAE: 1.8245, MSE: 8.1357, RMSE: 2.8523, R^2: 0.9770\n",
      "2025-06-04 18:49:22,394 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:22,418 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:49:23,949 - __main__ - INFO - \t MAE: 1.8589, MSE: 9.8653, RMSE: 3.1409, R^2: 0.9712\n",
      "[I 2025-06-04 18:49:24,025] Trial 13 finished with value: -1.8416899165101448 and parameters: {'iterations': 1816, 'max_depth': 8, 'learning_rate': 0.08747159492446976, 'l2_leaf_reg': 0.8179656659630594}. Best is trial 12 with value: -1.7201706195138742.\n",
      "2025-06-04 18:49:24,095 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:24,118 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:49:25,417 - __main__ - INFO - \t MAE: 1.8009, MSE: 6.3111, RMSE: 2.5122, R^2: 0.9822\n",
      "2025-06-04 18:49:25,428 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:25,452 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:49:26,979 - __main__ - INFO - \t MAE: 1.6998, MSE: 6.3378, RMSE: 2.5175, R^2: 0.9815\n",
      "[I 2025-06-04 18:49:27,059] Trial 14 finished with value: -1.7503404048816567 and parameters: {'iterations': 1731, 'max_depth': 4, 'learning_rate': 0.1045301347176434, 'l2_leaf_reg': 5.302961122533722}. Best is trial 12 with value: -1.7201706195138742.\n",
      "2025-06-04 18:49:27,133 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:27,157 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:49:28,890 - __main__ - INFO - \t MAE: 1.7534, MSE: 6.5505, RMSE: 2.5594, R^2: 0.9815\n",
      "2025-06-04 18:49:28,901 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:28,924 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:49:30,878 - __main__ - INFO - \t MAE: 1.7351, MSE: 7.0971, RMSE: 2.6640, R^2: 0.9793\n",
      "[I 2025-06-04 18:49:30,951] Trial 15 finished with value: -1.7442134248389083 and parameters: {'iterations': 2326, 'max_depth': 6, 'learning_rate': 0.05458612521895595, 'l2_leaf_reg': 5.506939037710027}. Best is trial 12 with value: -1.7201706195138742.\n",
      "2025-06-04 18:49:31,019 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:31,042 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:49:33,347 - __main__ - INFO - \t MAE: 2.0318, MSE: 12.5208, RMSE: 3.5385, R^2: 0.9646\n",
      "2025-06-04 18:49:33,357 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:33,381 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:49:34,844 - __main__ - INFO - \t MAE: 2.0752, MSE: 14.9786, RMSE: 3.8702, R^2: 0.9563\n",
      "[I 2025-06-04 18:49:34,924] Trial 16 finished with value: -2.0534683991942986 and parameters: {'iterations': 1554, 'max_depth': 10, 'learning_rate': 0.13811269358793538, 'l2_leaf_reg': 0.7906597945250486}. Best is trial 12 with value: -1.7201706195138742.\n",
      "2025-06-04 18:49:34,993 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:35,016 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:49:36,410 - __main__ - INFO - \t MAE: 1.8578, MSE: 6.6741, RMSE: 2.5834, R^2: 0.9811\n",
      "2025-06-04 18:49:36,421 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:36,445 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:49:38,452 - __main__ - INFO - \t MAE: 1.7094, MSE: 6.1989, RMSE: 2.4898, R^2: 0.9819\n",
      "[I 2025-06-04 18:49:38,533] Trial 17 finished with value: -1.7836236095380889 and parameters: {'iterations': 2761, 'max_depth': 3, 'learning_rate': 0.05654982256520401, 'l2_leaf_reg': 8.522403445388267}. Best is trial 12 with value: -1.7201706195138742.\n",
      "2025-06-04 18:49:38,612 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:38,636 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:49:40,784 - __main__ - INFO - \t MAE: 1.7842, MSE: 6.8707, RMSE: 2.6212, R^2: 0.9806\n",
      "2025-06-04 18:49:40,796 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:40,820 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:49:43,214 - __main__ - INFO - \t MAE: 1.7716, MSE: 8.2764, RMSE: 2.8769, R^2: 0.9758\n",
      "[I 2025-06-04 18:49:43,306] Trial 18 finished with value: -1.777896165931344 and parameters: {'iterations': 1389, 'max_depth': 7, 'learning_rate': 0.02572567691270217, 'l2_leaf_reg': 3.803295451445255}. Best is trial 12 with value: -1.7201706195138742.\n",
      "2025-06-04 18:49:43,394 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:43,417 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:49:45,650 - __main__ - INFO - \t MAE: 1.7618, MSE: 6.4307, RMSE: 2.5359, R^2: 0.9818\n",
      "2025-06-04 18:49:45,661 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:45,684 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:49:47,944 - __main__ - INFO - \t MAE: 1.7673, MSE: 7.2315, RMSE: 2.6891, R^2: 0.9789\n",
      "[I 2025-06-04 18:49:48,033] Trial 19 finished with value: -1.7645605662436832 and parameters: {'iterations': 2098, 'max_depth': 5, 'learning_rate': 0.04172614088392384, 'l2_leaf_reg': 22.273901092723857}. Best is trial 12 with value: -1.7201706195138742.\n",
      "2025-06-04 18:49:48,107 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:48,130 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:49:52,287 - __main__ - INFO - \t MAE: 2.0403, MSE: 10.0439, RMSE: 3.1692, R^2: 0.9716\n",
      "2025-06-04 18:49:52,299 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:52,323 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:49:56,627 - __main__ - INFO - \t MAE: 1.9828, MSE: 12.6377, RMSE: 3.5550, R^2: 0.9631\n",
      "[I 2025-06-04 18:49:56,701] Trial 20 finished with value: -2.0115622564412634 and parameters: {'iterations': 858, 'max_depth': 10, 'learning_rate': 0.012789553101793399, 'l2_leaf_reg': 12.299430631819584}. Best is trial 12 with value: -1.7201706195138742.\n",
      "2025-06-04 18:49:56,781 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:56,804 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:49:58,463 - __main__ - INFO - \t MAE: 1.7544, MSE: 6.5709, RMSE: 2.5634, R^2: 0.9814\n",
      "2025-06-04 18:49:58,473 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:49:58,497 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:50:00,156 - __main__ - INFO - \t MAE: 1.7520, MSE: 7.3072, RMSE: 2.7032, R^2: 0.9787\n",
      "[I 2025-06-04 18:50:00,243] Trial 21 finished with value: -1.7531973124638651 and parameters: {'iterations': 2284, 'max_depth': 6, 'learning_rate': 0.0694964608617458, 'l2_leaf_reg': 6.426584758649346}. Best is trial 12 with value: -1.7201706195138742.\n",
      "2025-06-04 18:50:00,326 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:00,350 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:50:02,163 - __main__ - INFO - \t MAE: 1.8079, MSE: 7.2996, RMSE: 2.7018, R^2: 0.9794\n",
      "2025-06-04 18:50:02,174 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:02,198 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:50:04,336 - __main__ - INFO - \t MAE: 1.7899, MSE: 8.7975, RMSE: 2.9661, R^2: 0.9743\n",
      "[I 2025-06-04 18:50:04,420] Trial 22 finished with value: -1.7988535362936682 and parameters: {'iterations': 2627, 'max_depth': 7, 'learning_rate': 0.04324927600284404, 'l2_leaf_reg': 7.2381864707208745}. Best is trial 12 with value: -1.7201706195138742.\n",
      "2025-06-04 18:50:04,490 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:04,514 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:50:05,760 - __main__ - INFO - \t MAE: 1.7575, MSE: 6.0356, RMSE: 2.4567, R^2: 0.9829\n",
      "2025-06-04 18:50:05,770 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:05,794 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:50:07,216 - __main__ - INFO - \t MAE: 1.7636, MSE: 7.5623, RMSE: 2.7500, R^2: 0.9779\n",
      "[I 2025-06-04 18:50:07,287] Trial 23 finished with value: -1.7605400688989272 and parameters: {'iterations': 2201, 'max_depth': 5, 'learning_rate': 0.1092585460737071, 'l2_leaf_reg': 3.1397077770459476}. Best is trial 12 with value: -1.7201706195138742.\n",
      "2025-06-04 18:50:07,353 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:07,377 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:50:09,151 - __main__ - INFO - \t MAE: 1.7320, MSE: 6.1539, RMSE: 2.4807, R^2: 0.9826\n",
      "2025-06-04 18:50:09,163 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:09,187 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:50:11,151 - __main__ - INFO - \t MAE: 1.7057, MSE: 6.6564, RMSE: 2.5800, R^2: 0.9806\n",
      "[I 2025-06-04 18:50:11,230] Trial 24 finished with value: -1.7188241162529485 and parameters: {'iterations': 1954, 'max_depth': 6, 'learning_rate': 0.030512239847784707, 'l2_leaf_reg': 0.774998638199013}. Best is trial 24 with value: -1.7188241162529485.\n",
      "2025-06-04 18:50:11,303 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:11,327 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:50:13,847 - __main__ - INFO - \t MAE: 1.8822, MSE: 9.2919, RMSE: 3.0483, R^2: 0.9737\n",
      "2025-06-04 18:50:13,857 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:13,881 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:50:16,319 - __main__ - INFO - \t MAE: 1.8861, MSE: 10.8601, RMSE: 3.2955, R^2: 0.9683\n",
      "[I 2025-06-04 18:50:16,398] Trial 25 finished with value: -1.8841691182562599 and parameters: {'iterations': 1252, 'max_depth': 9, 'learning_rate': 0.025381352089322587, 'l2_leaf_reg': 0.768086633057985}. Best is trial 24 with value: -1.7188241162529485.\n",
      "2025-06-04 18:50:16,477 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:16,501 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:50:18,052 - __main__ - INFO - \t MAE: 1.8288, MSE: 6.3542, RMSE: 2.5208, R^2: 0.9820\n",
      "2025-06-04 18:50:18,062 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:18,085 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:50:20,084 - __main__ - INFO - \t MAE: 1.7194, MSE: 6.2381, RMSE: 2.4976, R^2: 0.9818\n",
      "[I 2025-06-04 18:50:20,164] Trial 26 finished with value: -1.7741209163161824 and parameters: {'iterations': 1997, 'max_depth': 3, 'learning_rate': 0.030671323674580055, 'l2_leaf_reg': 3.148806362999192}. Best is trial 24 with value: -1.7188241162529485.\n",
      "2025-06-04 18:50:20,235 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:20,259 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:50:22,250 - __main__ - INFO - \t MAE: 1.7847, MSE: 6.3071, RMSE: 2.5114, R^2: 0.9822\n",
      "2025-06-04 18:50:22,260 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:22,284 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:50:24,695 - __main__ - INFO - \t MAE: 1.7054, MSE: 6.5808, RMSE: 2.5653, R^2: 0.9808\n",
      "[I 2025-06-04 18:50:24,776] Trial 27 finished with value: -1.745082930181013 and parameters: {'iterations': 1529, 'max_depth': 5, 'learning_rate': 0.020588233575217555, 'l2_leaf_reg': 3.177567090227273}. Best is trial 24 with value: -1.7188241162529485.\n",
      "2025-06-04 18:50:24,853 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:24,877 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:50:26,626 - __main__ - INFO - \t MAE: 1.7701, MSE: 6.8653, RMSE: 2.6202, R^2: 0.9806\n",
      "2025-06-04 18:50:26,638 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:26,662 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:50:28,553 - __main__ - INFO - \t MAE: 1.7475, MSE: 7.6057, RMSE: 2.7579, R^2: 0.9778\n",
      "[I 2025-06-04 18:50:28,637] Trial 28 finished with value: -1.7587740250897586 and parameters: {'iterations': 896, 'max_depth': 7, 'learning_rate': 0.035736384282974806, 'l2_leaf_reg': 0.511616025392903}. Best is trial 24 with value: -1.7188241162529485.\n",
      "2025-06-04 18:50:28,717 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:28,740 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 18:50:34,903 - __main__ - INFO - \t MAE: 1.9629, MSE: 10.2437, RMSE: 3.2006, R^2: 0.9710\n",
      "2025-06-04 18:50:34,915 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:34,939 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 18:50:39,104 - __main__ - INFO - \t MAE: 1.9725, MSE: 12.9711, RMSE: 3.6015, R^2: 0.9621\n",
      "[I 2025-06-04 18:50:39,181] Trial 29 finished with value: -1.967679512749096 and parameters: {'iterations': 1687, 'max_depth': 12, 'learning_rate': 0.07240338333766817, 'l2_leaf_reg': 7.301395007460365}. Best is trial 24 with value: -1.7188241162529485.\n",
      "2025-06-04 18:50:39,189 - __main__ - INFO - Best hyperparameters for fold 1: {'iterations': 1954, 'max_depth': 6, 'learning_rate': 0.030512239847784707, 'l2_leaf_reg': 0.774998638199013}\n",
      "2025-06-04 18:50:39,197 - __main__ - INFO - Best score: -1.7188\n",
      "2025-06-04 18:50:39,197 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:39,321 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-04 18:50:39,322 - __main__ - INFO - Retraining Model on Outer Fold 1\n",
      "2025-06-04 18:50:41,299 - __main__ - INFO - \t Inference Time: 0.00964 seconds\n",
      "2025-06-04 18:50:41,300 - __main__ - INFO - \t MAE: 2.1139, MSE: 8.4618, RMSE: 2.9089, R^2: 0.9718\n",
      "2025-06-04 18:50:41,301 - __main__ - INFO - -------------------- Outer Fold 2 --------------------\n",
      "2025-06-04 18:50:41,302 - __main__ - INFO - Starting hyperparameter optimization for outer fold 2...\n",
      "[I 2025-06-04 18:50:41,342] A new study created in RDB with name: 197.2.catboost\n",
      "2025-06-04 18:50:41,420 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:41,444 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:50:43,523 - __main__ - INFO - \t MAE: 1.7469, MSE: 6.7589, RMSE: 2.5998, R^2: 0.9802\n",
      "2025-06-04 18:50:43,534 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:43,558 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:50:45,852 - __main__ - INFO - \t MAE: 1.7794, MSE: 9.3200, RMSE: 3.0529, R^2: 0.9729\n",
      "[I 2025-06-04 18:50:45,944] Trial 0 finished with value: -1.7631463190047778 and parameters: {'iterations': 1000, 'max_depth': 6, 'learning_rate': 0.03574712922600244, 'l2_leaf_reg': 3.0}. Best is trial 0 with value: -1.7631463190047778.\n",
      "2025-06-04 18:50:46,005 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:46,029 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:50:49,244 - __main__ - INFO - \t MAE: 1.8782, MSE: 9.8666, RMSE: 3.1411, R^2: 0.9710\n",
      "2025-06-04 18:50:49,498 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:49,524 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:50:52,260 - __main__ - INFO - \t MAE: 1.9216, MSE: 13.6146, RMSE: 3.6898, R^2: 0.9603\n",
      "[I 2025-06-04 18:50:52,442] Trial 1 finished with value: -1.8998925532498465 and parameters: {'iterations': 3920, 'max_depth': 10, 'learning_rate': 0.07661100707771368, 'l2_leaf_reg': 5.102549893051878}. Best is trial 0 with value: -1.7631463190047778.\n",
      "2025-06-04 18:50:52,573 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:52,597 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:50:53,722 - __main__ - INFO - \t MAE: 1.8986, MSE: 7.5926, RMSE: 2.7555, R^2: 0.9777\n",
      "2025-06-04 18:50:53,744 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:53,767 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:50:55,131 - __main__ - INFO - \t MAE: 1.8504, MSE: 7.4590, RMSE: 2.7311, R^2: 0.9783\n",
      "[I 2025-06-04 18:50:55,310] Trial 2 finished with value: -1.874494770239605 and parameters: {'iterations': 1071, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'l2_leaf_reg': 18.232892846424658}. Best is trial 0 with value: -1.7631463190047778.\n",
      "2025-06-04 18:50:55,439 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:55,463 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:50:56,941 - __main__ - INFO - \t MAE: 1.8248, MSE: 7.0422, RMSE: 2.6537, R^2: 0.9793\n",
      "2025-06-04 18:50:56,959 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:56,983 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:50:58,453 - __main__ - INFO - \t MAE: 1.8175, MSE: 7.5735, RMSE: 2.7520, R^2: 0.9779\n",
      "[I 2025-06-04 18:50:58,618] Trial 3 finished with value: -1.8211503749417728 and parameters: {'iterations': 3050, 'max_depth': 2, 'learning_rate': 0.2708160864249968, 'l2_leaf_reg': 25.057057903612442}. Best is trial 0 with value: -1.7631463190047778.\n",
      "2025-06-04 18:50:58,745 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:50:58,769 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:51:00,807 - __main__ - INFO - \t MAE: 1.8399, MSE: 7.1846, RMSE: 2.6804, R^2: 0.9789\n",
      "2025-06-04 18:51:00,829 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:51:00,853 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:51:03,118 - __main__ - INFO - \t MAE: 1.7980, MSE: 8.0069, RMSE: 2.8296, R^2: 0.9767\n",
      "[I 2025-06-04 18:51:03,373] Trial 4 finished with value: -1.818921175259418 and parameters: {'iterations': 1273, 'max_depth': 4, 'learning_rate': 0.018659959624904916, 'l2_leaf_reg': 9.475146167306363}. Best is trial 0 with value: -1.7631463190047778.\n",
      "2025-06-04 18:51:03,562 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:51:03,586 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:51:07,264 - __main__ - INFO - \t MAE: 1.7810, MSE: 8.0811, RMSE: 2.8427, R^2: 0.9763\n",
      "2025-06-04 18:51:07,281 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:51:07,305 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:51:10,794 - __main__ - INFO - \t MAE: 1.7632, MSE: 8.8922, RMSE: 2.9820, R^2: 0.9741\n",
      "[I 2025-06-04 18:51:11,390] Trial 5 finished with value: -1.772079031727634 and parameters: {'iterations': 2393, 'max_depth': 6, 'learning_rate': 0.02692655251486473, 'l2_leaf_reg': 18.549660394310195}. Best is trial 0 with value: -1.7631463190047778.\n",
      "2025-06-04 18:51:11,521 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:51:11,545 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:51:12,984 - __main__ - INFO - \t MAE: 1.8144, MSE: 7.6644, RMSE: 2.7685, R^2: 0.9775\n",
      "2025-06-04 18:51:13,008 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:51:13,032 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:51:15,183 - __main__ - INFO - \t MAE: 1.7740, MSE: 8.1326, RMSE: 2.8518, R^2: 0.9763\n",
      "[I 2025-06-04 18:51:15,413] Trial 6 finished with value: -1.794226905692903 and parameters: {'iterations': 1012, 'max_depth': 5, 'learning_rate': 0.03476649150592621, 'l2_leaf_reg': 13.95406453440256}. Best is trial 0 with value: -1.7631463190047778.\n",
      "2025-06-04 18:51:15,546 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:51:15,570 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:51:17,498 - __main__ - INFO - \t MAE: 1.7503, MSE: 6.8088, RMSE: 2.6094, R^2: 0.9800\n",
      "2025-06-04 18:51:17,516 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:51:17,540 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:51:19,901 - __main__ - INFO - \t MAE: 1.7308, MSE: 7.2829, RMSE: 2.6987, R^2: 0.9788\n",
      "[I 2025-06-04 18:51:20,087] Trial 7 finished with value: -1.7405201240290866 and parameters: {'iterations': 3326, 'max_depth': 4, 'learning_rate': 0.05748924681991978, 'l2_leaf_reg': 17.976229781430252}. Best is trial 7 with value: -1.7405201240290866.\n",
      "2025-06-04 18:51:20,219 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:51:20,243 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:51:22,349 - __main__ - INFO - \t MAE: 1.8547, MSE: 7.9856, RMSE: 2.8259, R^2: 0.9766\n",
      "2025-06-04 18:51:22,600 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:51:22,624 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:51:24,764 - __main__ - INFO - \t MAE: 1.8865, MSE: 11.9721, RMSE: 3.4601, R^2: 0.9651\n",
      "[I 2025-06-04 18:51:24,951] Trial 8 finished with value: -1.8706254724374105 and parameters: {'iterations': 678, 'max_depth': 8, 'learning_rate': 0.0178601378893971, 'l2_leaf_reg': 2.419021993065746}. Best is trial 7 with value: -1.7405201240290866.\n",
      "2025-06-04 18:51:25,087 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:51:25,111 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:51:28,736 - __main__ - INFO - \t MAE: 2.0192, MSE: 12.4469, RMSE: 3.5280, R^2: 0.9635\n",
      "2025-06-04 18:51:28,759 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:51:28,783 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:51:33,430 - __main__ - INFO - \t MAE: 2.0549, MSE: 15.9637, RMSE: 3.9955, R^2: 0.9535\n",
      "[I 2025-06-04 18:51:33,617] Trial 9 finished with value: -2.037058481500086 and parameters: {'iterations': 3913, 'max_depth': 12, 'learning_rate': 0.1563510870813346, 'l2_leaf_reg': 9.486106190614436}. Best is trial 7 with value: -1.7405201240290866.\n",
      "2025-06-04 18:51:33,766 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:51:33,789 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:51:40,150 - __main__ - INFO - \t MAE: 1.9475, MSE: 11.5458, RMSE: 3.3979, R^2: 0.9661\n",
      "2025-06-04 18:51:41,440 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:51:41,466 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:51:47,247 - __main__ - INFO - \t MAE: 1.8944, MSE: 10.7898, RMSE: 3.2848, R^2: 0.9686\n",
      "[I 2025-06-04 18:51:47,429] Trial 10 finished with value: -1.9209433129483435 and parameters: {'iterations': 2991, 'max_depth': 8, 'learning_rate': 0.010260022325256576, 'l2_leaf_reg': 28.002565086397723}. Best is trial 7 with value: -1.7405201240290866.\n",
      "2025-06-04 18:51:47,572 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:51:47,596 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:51:49,362 - __main__ - INFO - \t MAE: 1.7771, MSE: 7.0542, RMSE: 2.6560, R^2: 0.9793\n",
      "2025-06-04 18:51:49,380 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:51:49,403 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:51:51,625 - __main__ - INFO - \t MAE: 1.7200, MSE: 7.0008, RMSE: 2.6459, R^2: 0.9796\n",
      "[I 2025-06-04 18:51:51,790] Trial 11 finished with value: -1.7485520774573313 and parameters: {'iterations': 1848, 'max_depth': 4, 'learning_rate': 0.06688881603939227, 'l2_leaf_reg': 21.089636353095667}. Best is trial 7 with value: -1.7405201240290866.\n",
      "2025-06-04 18:51:51,944 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:51:51,968 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:51:53,876 - __main__ - INFO - \t MAE: 1.7722, MSE: 7.2518, RMSE: 2.6929, R^2: 0.9787\n",
      "2025-06-04 18:51:54,383 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:51:54,407 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:51:56,079 - __main__ - INFO - \t MAE: 1.7297, MSE: 7.1847, RMSE: 2.6804, R^2: 0.9791\n",
      "[I 2025-06-04 18:51:56,247] Trial 12 finished with value: -1.7509751466654386 and parameters: {'iterations': 1810, 'max_depth': 4, 'learning_rate': 0.07877075372990869, 'l2_leaf_reg': 22.548128525503664}. Best is trial 7 with value: -1.7405201240290866.\n",
      "2025-06-04 18:51:56,390 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:51:56,414 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:51:58,736 - __main__ - INFO - \t MAE: 1.7443, MSE: 6.9142, RMSE: 2.6295, R^2: 0.9797\n",
      "2025-06-04 18:51:58,756 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:51:58,780 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:52:00,462 - __main__ - INFO - \t MAE: 1.7810, MSE: 7.2212, RMSE: 2.6872, R^2: 0.9790\n",
      "[I 2025-06-04 18:52:00,713] Trial 13 finished with value: -1.7626410326517767 and parameters: {'iterations': 2191, 'max_depth': 3, 'learning_rate': 0.06948345184361522, 'l2_leaf_reg': 20.611285104431534}. Best is trial 7 with value: -1.7405201240290866.\n",
      "2025-06-04 18:52:00,852 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:52:00,876 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:52:02,223 - __main__ - INFO - \t MAE: 1.7573, MSE: 6.8143, RMSE: 2.6104, R^2: 0.9800\n",
      "2025-06-04 18:52:02,246 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:52:02,270 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:52:03,817 - __main__ - INFO - \t MAE: 1.7651, MSE: 7.4816, RMSE: 2.7352, R^2: 0.9782\n",
      "[I 2025-06-04 18:52:03,919] Trial 14 finished with value: -1.7612440308222814 and parameters: {'iterations': 3138, 'max_depth': 4, 'learning_rate': 0.11752482442830568, 'l2_leaf_reg': 13.68331531021721}. Best is trial 7 with value: -1.7405201240290866.\n",
      "2025-06-04 18:52:03,994 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:52:04,018 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:52:07,600 - __main__ - INFO - \t MAE: 1.8376, MSE: 10.3233, RMSE: 3.2130, R^2: 0.9697\n",
      "2025-06-04 18:52:07,610 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:52:07,634 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:52:10,382 - __main__ - INFO - \t MAE: 1.8016, MSE: 9.2763, RMSE: 3.0457, R^2: 0.9730\n",
      "[I 2025-06-04 18:52:10,456] Trial 15 finished with value: -1.8195549894565426 and parameters: {'iterations': 1782, 'max_depth': 7, 'learning_rate': 0.04924185908787293, 'l2_leaf_reg': 25.890136995963996}. Best is trial 7 with value: -1.7405201240290866.\n",
      "2025-06-04 18:52:10,525 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:52:10,549 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:52:12,636 - __main__ - INFO - \t MAE: 1.7912, MSE: 7.9527, RMSE: 2.8201, R^2: 0.9767\n",
      "2025-06-04 18:52:12,646 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:52:12,670 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:52:14,305 - __main__ - INFO - \t MAE: 1.8106, MSE: 8.5704, RMSE: 2.9275, R^2: 0.9750\n",
      "[I 2025-06-04 18:52:14,376] Trial 16 finished with value: -1.8008675932792864 and parameters: {'iterations': 2588, 'max_depth': 5, 'learning_rate': 0.09931952624038086, 'l2_leaf_reg': 29.882487686549347}. Best is trial 7 with value: -1.7405201240290866.\n",
      "2025-06-04 18:52:14,447 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:52:14,471 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:52:16,746 - __main__ - INFO - \t MAE: 1.7650, MSE: 7.0592, RMSE: 2.6569, R^2: 0.9793\n",
      "2025-06-04 18:52:16,756 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:52:16,780 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:52:18,964 - __main__ - INFO - \t MAE: 1.7273, MSE: 6.8628, RMSE: 2.6197, R^2: 0.9800\n",
      "[I 2025-06-04 18:52:19,050] Trial 17 finished with value: -1.7461180986511 and parameters: {'iterations': 3501, 'max_depth': 3, 'learning_rate': 0.04934885095830346, 'l2_leaf_reg': 16.66415965418323}. Best is trial 7 with value: -1.7405201240290866.\n",
      "2025-06-04 18:52:19,129 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:52:19,153 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:52:21,236 - __main__ - INFO - \t MAE: 1.8127, MSE: 6.8236, RMSE: 2.6122, R^2: 0.9800\n",
      "2025-06-04 18:52:21,246 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:52:21,270 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:52:22,966 - __main__ - INFO - \t MAE: 1.8427, MSE: 7.1859, RMSE: 2.6806, R^2: 0.9791\n",
      "[I 2025-06-04 18:52:23,050] Trial 18 finished with value: -1.8277045948707518 and parameters: {'iterations': 3510, 'max_depth': 2, 'learning_rate': 0.04463769752916825, 'l2_leaf_reg': 10.093092323385164}. Best is trial 7 with value: -1.7405201240290866.\n",
      "2025-06-04 18:52:23,131 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:52:23,155 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:52:26,967 - __main__ - INFO - \t MAE: 1.7514, MSE: 6.8812, RMSE: 2.6232, R^2: 0.9798\n",
      "2025-06-04 18:52:26,977 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:52:27,001 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:52:30,020 - __main__ - INFO - \t MAE: 1.7434, MSE: 7.1810, RMSE: 2.6797, R^2: 0.9791\n",
      "[I 2025-06-04 18:52:30,092] Trial 19 finished with value: -1.747391885536323 and parameters: {'iterations': 3608, 'max_depth': 3, 'learning_rate': 0.02319375172421768, 'l2_leaf_reg': 16.6831533401584}. Best is trial 7 with value: -1.7405201240290866.\n",
      "2025-06-04 18:52:30,158 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:52:30,182 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:52:35,503 - __main__ - INFO - \t MAE: 1.7659, MSE: 7.4678, RMSE: 2.7327, R^2: 0.9781\n",
      "2025-06-04 18:52:35,514 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:52:35,538 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:52:40,406 - __main__ - INFO - \t MAE: 1.7678, MSE: 8.8070, RMSE: 2.9677, R^2: 0.9743\n",
      "[I 2025-06-04 18:52:40,485] Trial 20 finished with value: -1.766851285702661 and parameters: {'iterations': 3456, 'max_depth': 6, 'learning_rate': 0.012789553101793399, 'l2_leaf_reg': 12.299430631819584}. Best is trial 7 with value: -1.7405201240290866.\n",
      "2025-06-04 18:52:40,555 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:52:40,579 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:52:43,798 - __main__ - INFO - \t MAE: 1.7523, MSE: 6.9098, RMSE: 2.6286, R^2: 0.9797\n",
      "2025-06-04 18:52:43,809 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:52:43,833 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:52:46,706 - __main__ - INFO - \t MAE: 1.7312, MSE: 7.0525, RMSE: 2.6557, R^2: 0.9795\n",
      "[I 2025-06-04 18:52:46,780] Trial 21 finished with value: -1.7417104287116891 and parameters: {'iterations': 3457, 'max_depth': 3, 'learning_rate': 0.02726034945762637, 'l2_leaf_reg': 16.85835829467325}. Best is trial 7 with value: -1.7405201240290866.\n",
      "2025-06-04 18:52:46,848 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:52:46,871 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:52:50,038 - __main__ - INFO - \t MAE: 1.7434, MSE: 6.9763, RMSE: 2.6413, R^2: 0.9795\n",
      "2025-06-04 18:52:50,051 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:52:50,075 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:52:52,393 - __main__ - INFO - \t MAE: 1.7501, MSE: 7.0590, RMSE: 2.6569, R^2: 0.9794\n",
      "[I 2025-06-04 18:52:52,476] Trial 22 finished with value: -1.7467676960464782 and parameters: {'iterations': 2774, 'max_depth': 3, 'learning_rate': 0.03419321686180797, 'l2_leaf_reg': 15.9946131078077}. Best is trial 7 with value: -1.7405201240290866.\n",
      "2025-06-04 18:52:52,547 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:52:52,571 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:52:54,428 - __main__ - INFO - \t MAE: 1.8176, MSE: 7.3774, RMSE: 2.7161, R^2: 0.9783\n",
      "2025-06-04 18:52:54,438 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:52:54,462 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:52:56,684 - __main__ - INFO - \t MAE: 1.7426, MSE: 7.1572, RMSE: 2.6753, R^2: 0.9792\n",
      "[I 2025-06-04 18:52:56,765] Trial 23 finished with value: -1.7801185778369222 and parameters: {'iterations': 4091, 'max_depth': 3, 'learning_rate': 0.04584818470814286, 'l2_leaf_reg': 19.03904819235181}. Best is trial 7 with value: -1.7405201240290866.\n",
      "2025-06-04 18:52:56,835 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:52:56,859 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:53:00,814 - __main__ - INFO - \t MAE: 1.7806, MSE: 7.9433, RMSE: 2.8184, R^2: 0.9767\n",
      "2025-06-04 18:53:00,826 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:53:00,851 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:53:04,748 - __main__ - INFO - \t MAE: 1.7366, MSE: 8.2405, RMSE: 2.8706, R^2: 0.9760\n",
      "[I 2025-06-04 18:53:04,820] Trial 24 finished with value: -1.758636666942158 and parameters: {'iterations': 3330, 'max_depth': 5, 'learning_rate': 0.025843081622894882, 'l2_leaf_reg': 23.82894532667591}. Best is trial 7 with value: -1.7405201240290866.\n",
      "2025-06-04 18:53:04,888 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:53:04,912 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:53:06,999 - __main__ - INFO - \t MAE: 1.8116, MSE: 6.8052, RMSE: 2.6087, R^2: 0.9800\n",
      "2025-06-04 18:53:07,009 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:53:07,033 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:53:08,752 - __main__ - INFO - \t MAE: 1.8262, MSE: 7.1712, RMSE: 2.6779, R^2: 0.9791\n",
      "[I 2025-06-04 18:53:08,832] Trial 25 finished with value: -1.8188706253117255 and parameters: {'iterations': 3723, 'max_depth': 2, 'learning_rate': 0.055980129260976036, 'l2_leaf_reg': 16.33241313119079}. Best is trial 7 with value: -1.7405201240290866.\n",
      "2025-06-04 18:53:08,906 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:53:08,930 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:53:10,336 - __main__ - INFO - \t MAE: 1.8094, MSE: 7.6058, RMSE: 2.7579, R^2: 0.9777\n",
      "2025-06-04 18:53:10,348 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:53:10,372 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:53:12,048 - __main__ - INFO - \t MAE: 1.7278, MSE: 6.7701, RMSE: 2.6019, R^2: 0.9803\n",
      "[I 2025-06-04 18:53:12,126] Trial 26 finished with value: -1.768624598737123 and parameters: {'iterations': 2785, 'max_depth': 3, 'learning_rate': 0.10355075137742904, 'l2_leaf_reg': 12.332460442936878}. Best is trial 7 with value: -1.7405201240290866.\n",
      "2025-06-04 18:53:12,201 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:53:12,225 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:53:15,597 - __main__ - INFO - \t MAE: 1.7406, MSE: 6.6815, RMSE: 2.5849, R^2: 0.9804\n",
      "2025-06-04 18:53:15,607 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:53:15,631 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:53:19,039 - __main__ - INFO - \t MAE: 1.7373, MSE: 8.1963, RMSE: 2.8629, R^2: 0.9761\n",
      "[I 2025-06-04 18:53:19,121] Trial 27 finished with value: -1.73898606037482 and parameters: {'iterations': 3350, 'max_depth': 5, 'learning_rate': 0.01894950294760495, 'l2_leaf_reg': 7.444301609237941}. Best is trial 27 with value: -1.73898606037482.\n",
      "2025-06-04 18:53:19,199 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:53:19,222 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:53:24,078 - __main__ - INFO - \t MAE: 1.7603, MSE: 7.4211, RMSE: 2.7242, R^2: 0.9782\n",
      "2025-06-04 18:53:24,091 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:53:24,115 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:53:27,769 - __main__ - INFO - \t MAE: 1.8019, MSE: 10.0782, RMSE: 3.1746, R^2: 0.9706\n",
      "[I 2025-06-04 18:53:27,878] Trial 28 finished with value: -1.7810985406610675 and parameters: {'iterations': 3224, 'max_depth': 7, 'learning_rate': 0.01484237858891211, 'l2_leaf_reg': 5.474546291766801}. Best is trial 27 with value: -1.73898606037482.\n",
      "2025-06-04 18:53:27,967 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:53:27,990 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 18:53:31,005 - __main__ - INFO - \t MAE: 1.7287, MSE: 6.7853, RMSE: 2.6049, R^2: 0.9801\n",
      "2025-06-04 18:53:31,018 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:53:31,042 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 18:53:33,662 - __main__ - INFO - \t MAE: 1.7782, MSE: 9.1561, RMSE: 3.0259, R^2: 0.9733\n",
      "[I 2025-06-04 18:53:33,741] Trial 29 finished with value: -1.7534635029460326 and parameters: {'iterations': 2830, 'max_depth': 6, 'learning_rate': 0.029829550604230662, 'l2_leaf_reg': 6.88478181355905}. Best is trial 27 with value: -1.73898606037482.\n",
      "2025-06-04 18:53:33,750 - __main__ - INFO - Best hyperparameters for fold 2: {'iterations': 3350, 'max_depth': 5, 'learning_rate': 0.01894950294760495, 'l2_leaf_reg': 7.444301609237941}\n",
      "2025-06-04 18:53:33,758 - __main__ - INFO - Best score: -1.7390\n",
      "2025-06-04 18:53:33,758 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:53:33,883 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-04 18:53:33,883 - __main__ - INFO - Retraining Model on Outer Fold 2\n",
      "2025-06-04 18:53:46,235 - __main__ - INFO - \t Inference Time: 0.01894 seconds\n",
      "2025-06-04 18:53:46,237 - __main__ - INFO - \t MAE: 1.7629, MSE: 6.1190, RMSE: 2.4737, R^2: 0.9812\n",
      "2025-06-04 18:53:46,238 - __main__ - INFO - -------------------- Outer Fold 3 --------------------\n",
      "2025-06-04 18:53:46,239 - __main__ - INFO - Starting hyperparameter optimization for outer fold 3...\n",
      "[I 2025-06-04 18:53:46,281] A new study created in RDB with name: 197.3.catboost\n",
      "2025-06-04 18:53:46,380 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:53:46,403 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:53:48,589 - __main__ - INFO - \t MAE: 1.7181, MSE: 6.6164, RMSE: 2.5722, R^2: 0.9812\n",
      "2025-06-04 18:53:48,602 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:53:48,626 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:53:50,751 - __main__ - INFO - \t MAE: 1.7525, MSE: 7.0070, RMSE: 2.6471, R^2: 0.9796\n",
      "[I 2025-06-04 18:53:50,839] Trial 0 finished with value: -1.7353298902942087 and parameters: {'iterations': 1000, 'max_depth': 6, 'learning_rate': 0.03574712922600244, 'l2_leaf_reg': 3.0}. Best is trial 0 with value: -1.7353298902942087.\n",
      "2025-06-04 18:53:50,901 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:53:50,924 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:53:52,944 - __main__ - INFO - \t MAE: 1.9138, MSE: 10.3271, RMSE: 3.2136, R^2: 0.9706\n",
      "2025-06-04 18:53:52,956 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:53:52,980 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:53:55,533 - __main__ - INFO - \t MAE: 1.9639, MSE: 12.0244, RMSE: 3.4676, R^2: 0.9650\n",
      "[I 2025-06-04 18:53:55,613] Trial 1 finished with value: -1.9388664938233118 and parameters: {'iterations': 3920, 'max_depth': 10, 'learning_rate': 0.07661100707771368, 'l2_leaf_reg': 5.102549893051878}. Best is trial 0 with value: -1.7353298902942087.\n",
      "2025-06-04 18:53:55,675 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:53:55,698 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:53:56,855 - __main__ - INFO - \t MAE: 1.8773, MSE: 6.5918, RMSE: 2.5674, R^2: 0.9812\n",
      "2025-06-04 18:53:56,865 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:53:56,889 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:53:58,297 - __main__ - INFO - \t MAE: 1.8599, MSE: 6.6654, RMSE: 2.5818, R^2: 0.9806\n",
      "[I 2025-06-04 18:53:58,369] Trial 2 finished with value: -1.8685847423976008 and parameters: {'iterations': 1071, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'l2_leaf_reg': 18.232892846424658}. Best is trial 0 with value: -1.7353298902942087.\n",
      "2025-06-04 18:53:58,427 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:53:58,451 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:53:59,623 - __main__ - INFO - \t MAE: 1.8479, MSE: 6.3890, RMSE: 2.5277, R^2: 0.9818\n",
      "2025-06-04 18:53:59,634 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:53:59,657 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:54:01,012 - __main__ - INFO - \t MAE: 1.8738, MSE: 6.9307, RMSE: 2.6326, R^2: 0.9798\n",
      "[I 2025-06-04 18:54:01,089] Trial 3 finished with value: -1.8608848732454084 and parameters: {'iterations': 3050, 'max_depth': 2, 'learning_rate': 0.2708160864249968, 'l2_leaf_reg': 25.057057903612442}. Best is trial 0 with value: -1.7353298902942087.\n",
      "2025-06-04 18:54:01,149 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:01,173 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:54:03,334 - __main__ - INFO - \t MAE: 1.7673, MSE: 6.1094, RMSE: 2.4717, R^2: 0.9826\n",
      "2025-06-04 18:54:03,346 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:03,370 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:54:05,637 - __main__ - INFO - \t MAE: 1.8171, MSE: 6.9850, RMSE: 2.6429, R^2: 0.9797\n",
      "[I 2025-06-04 18:54:05,716] Trial 4 finished with value: -1.792194297720814 and parameters: {'iterations': 1273, 'max_depth': 4, 'learning_rate': 0.018659959624904916, 'l2_leaf_reg': 9.475146167306363}. Best is trial 0 with value: -1.7353298902942087.\n",
      "2025-06-04 18:54:05,778 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:05,802 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:54:08,644 - __main__ - INFO - \t MAE: 1.7641, MSE: 6.9351, RMSE: 2.6335, R^2: 0.9803\n",
      "2025-06-04 18:54:08,655 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:08,679 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:54:11,681 - __main__ - INFO - \t MAE: 1.8104, MSE: 8.0946, RMSE: 2.8451, R^2: 0.9764\n",
      "[I 2025-06-04 18:54:11,759] Trial 5 finished with value: -1.7872405396008757 and parameters: {'iterations': 2393, 'max_depth': 6, 'learning_rate': 0.02692655251486473, 'l2_leaf_reg': 18.549660394310195}. Best is trial 0 with value: -1.7353298902942087.\n",
      "2025-06-04 18:54:11,817 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:11,840 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:54:13,881 - __main__ - INFO - \t MAE: 1.7356, MSE: 6.2197, RMSE: 2.4939, R^2: 0.9823\n",
      "2025-06-04 18:54:13,891 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:13,915 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:54:16,102 - __main__ - INFO - \t MAE: 1.7878, MSE: 7.3921, RMSE: 2.7189, R^2: 0.9785\n",
      "[I 2025-06-04 18:54:16,181] Trial 6 finished with value: -1.761695951055053 and parameters: {'iterations': 1012, 'max_depth': 5, 'learning_rate': 0.03476649150592621, 'l2_leaf_reg': 13.95406453440256}. Best is trial 0 with value: -1.7353298902942087.\n",
      "2025-06-04 18:54:16,241 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:16,264 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:54:18,202 - __main__ - INFO - \t MAE: 1.6979, MSE: 5.6615, RMSE: 2.3794, R^2: 0.9839\n",
      "2025-06-04 18:54:18,213 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:18,237 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:54:20,211 - __main__ - INFO - \t MAE: 1.7800, MSE: 6.7382, RMSE: 2.5958, R^2: 0.9804\n",
      "[I 2025-06-04 18:54:20,287] Trial 7 finished with value: -1.7389600886434549 and parameters: {'iterations': 3326, 'max_depth': 4, 'learning_rate': 0.05748924681991978, 'l2_leaf_reg': 17.976229781430252}. Best is trial 0 with value: -1.7353298902942087.\n",
      "2025-06-04 18:54:20,348 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:20,372 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:54:22,647 - __main__ - INFO - \t MAE: 1.8487, MSE: 8.4436, RMSE: 2.9058, R^2: 0.9760\n",
      "2025-06-04 18:54:22,658 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:22,682 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:54:25,083 - __main__ - INFO - \t MAE: 1.8807, MSE: 9.3257, RMSE: 3.0538, R^2: 0.9728\n",
      "[I 2025-06-04 18:54:25,158] Trial 8 finished with value: -1.8646757282466035 and parameters: {'iterations': 678, 'max_depth': 8, 'learning_rate': 0.0178601378893971, 'l2_leaf_reg': 2.419021993065746}. Best is trial 0 with value: -1.7353298902942087.\n",
      "2025-06-04 18:54:25,215 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:25,239 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:54:29,257 - __main__ - INFO - \t MAE: 2.0428, MSE: 13.5184, RMSE: 3.6767, R^2: 0.9615\n",
      "2025-06-04 18:54:29,268 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:29,291 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:54:33,888 - __main__ - INFO - \t MAE: 2.0677, MSE: 15.2210, RMSE: 3.9014, R^2: 0.9557\n",
      "[I 2025-06-04 18:54:33,971] Trial 9 finished with value: -2.055239220903823 and parameters: {'iterations': 3913, 'max_depth': 12, 'learning_rate': 0.1563510870813346, 'l2_leaf_reg': 9.486106190614436}. Best is trial 0 with value: -1.7353298902942087.\n",
      "2025-06-04 18:54:34,049 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:34,073 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:54:38,479 - __main__ - INFO - \t MAE: 1.9770, MSE: 9.3936, RMSE: 3.0649, R^2: 0.9733\n",
      "2025-06-04 18:54:38,493 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:38,517 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:54:43,004 - __main__ - INFO - \t MAE: 2.0517, MSE: 12.2323, RMSE: 3.4975, R^2: 0.9644\n",
      "[I 2025-06-04 18:54:43,101] Trial 10 finished with value: -2.0143303537003123 and parameters: {'iterations': 1805, 'max_depth': 8, 'learning_rate': 0.010260022325256576, 'l2_leaf_reg': 28.002565086397723}. Best is trial 0 with value: -1.7353298902942087.\n",
      "2025-06-04 18:54:43,177 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:43,200 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:54:45,087 - __main__ - INFO - \t MAE: 1.7005, MSE: 5.7528, RMSE: 2.3985, R^2: 0.9836\n",
      "2025-06-04 18:54:45,098 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:45,122 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:54:47,117 - __main__ - INFO - \t MAE: 1.7752, MSE: 6.7987, RMSE: 2.6074, R^2: 0.9802\n",
      "[I 2025-06-04 18:54:47,195] Trial 11 finished with value: -1.737851105656164 and parameters: {'iterations': 3043, 'max_depth': 4, 'learning_rate': 0.06688881603939227, 'l2_leaf_reg': 21.089636353095667}. Best is trial 0 with value: -1.7353298902942087.\n",
      "2025-06-04 18:54:47,267 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:47,291 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:54:49,002 - __main__ - INFO - \t MAE: 1.8133, MSE: 8.3904, RMSE: 2.8966, R^2: 0.9761\n",
      "2025-06-04 18:54:49,012 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:49,036 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:54:51,158 - __main__ - INFO - \t MAE: 1.8540, MSE: 9.7878, RMSE: 3.1285, R^2: 0.9715\n",
      "[I 2025-06-04 18:54:51,241] Trial 12 finished with value: -1.833670873659266 and parameters: {'iterations': 2466, 'max_depth': 7, 'learning_rate': 0.09547029523095643, 'l2_leaf_reg': 23.383691953017657}. Best is trial 0 with value: -1.7353298902942087.\n",
      "2025-06-04 18:54:51,311 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:51,334 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:54:53,304 - __main__ - INFO - \t MAE: 1.7138, MSE: 5.8071, RMSE: 2.4098, R^2: 0.9835\n",
      "2025-06-04 18:54:53,315 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:53,339 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:54:55,687 - __main__ - INFO - \t MAE: 1.7625, MSE: 6.6749, RMSE: 2.5836, R^2: 0.9806\n",
      "[I 2025-06-04 18:54:55,767] Trial 13 finished with value: -1.7381350903465436 and parameters: {'iterations': 1896, 'max_depth': 4, 'learning_rate': 0.040699385444240246, 'l2_leaf_reg': 12.25855165503099}. Best is trial 0 with value: -1.7353298902942087.\n",
      "2025-06-04 18:54:55,835 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:55,859 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:54:57,522 - __main__ - INFO - \t MAE: 1.7453, MSE: 6.7305, RMSE: 2.5943, R^2: 0.9808\n",
      "2025-06-04 18:54:57,532 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:57,555 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:54:59,415 - __main__ - INFO - \t MAE: 1.8162, MSE: 7.9934, RMSE: 2.8273, R^2: 0.9767\n",
      "[I 2025-06-04 18:54:59,485] Trial 14 finished with value: -1.7807265700799777 and parameters: {'iterations': 2985, 'max_depth': 6, 'learning_rate': 0.09205638234681292, 'l2_leaf_reg': 21.81332912960127}. Best is trial 0 with value: -1.7353298902942087.\n",
      "2025-06-04 18:54:59,552 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:54:59,575 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:55:01,086 - __main__ - INFO - \t MAE: 1.7019, MSE: 5.6388, RMSE: 2.3746, R^2: 0.9840\n",
      "2025-06-04 18:55:01,096 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:01,120 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:55:03,036 - __main__ - INFO - \t MAE: 1.7150, MSE: 5.8344, RMSE: 2.4155, R^2: 0.9830\n",
      "[I 2025-06-04 18:55:03,106] Trial 15 finished with value: -1.7084738473933712 and parameters: {'iterations': 1681, 'max_depth': 3, 'learning_rate': 0.05497588351395492, 'l2_leaf_reg': 1.8412139329176223}. Best is trial 15 with value: -1.7084738473933712.\n",
      "2025-06-04 18:55:03,175 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:03,198 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:55:04,650 - __main__ - INFO - \t MAE: 1.8314, MSE: 6.3287, RMSE: 2.5157, R^2: 0.9820\n",
      "2025-06-04 18:55:04,661 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:04,685 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:55:06,586 - __main__ - INFO - \t MAE: 1.7943, MSE: 6.2285, RMSE: 2.4957, R^2: 0.9819\n",
      "[I 2025-06-04 18:55:06,662] Trial 16 finished with value: -1.812869147327423 and parameters: {'iterations': 1554, 'max_depth': 2, 'learning_rate': 0.042973802402720244, 'l2_leaf_reg': 0.8283563775626521}. Best is trial 15 with value: -1.7084738473933712.\n",
      "2025-06-04 18:55:06,737 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:06,761 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:55:08,343 - __main__ - INFO - \t MAE: 1.8883, MSE: 9.2061, RMSE: 3.0342, R^2: 0.9738\n",
      "2025-06-04 18:55:08,354 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:08,377 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:55:10,083 - __main__ - INFO - \t MAE: 1.8996, MSE: 9.7175, RMSE: 3.1173, R^2: 0.9717\n",
      "[I 2025-06-04 18:55:10,164] Trial 17 finished with value: -1.8939818073671977 and parameters: {'iterations': 793, 'max_depth': 9, 'learning_rate': 0.12012810326827857, 'l2_leaf_reg': 5.494982148127825}. Best is trial 15 with value: -1.7084738473933712.\n",
      "2025-06-04 18:55:10,233 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:10,257 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:55:12,183 - __main__ - INFO - \t MAE: 1.7516, MSE: 5.9351, RMSE: 2.4362, R^2: 0.9831\n",
      "2025-06-04 18:55:12,193 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:12,217 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:55:14,570 - __main__ - INFO - \t MAE: 1.7525, MSE: 6.0943, RMSE: 2.4687, R^2: 0.9823\n",
      "[I 2025-06-04 18:55:14,642] Trial 18 finished with value: -1.7520315879706496 and parameters: {'iterations': 1528, 'max_depth': 3, 'learning_rate': 0.02572567691270217, 'l2_leaf_reg': 5.27575600097158}. Best is trial 15 with value: -1.7084738473933712.\n",
      "2025-06-04 18:55:14,714 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:14,737 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:55:18,302 - __main__ - INFO - \t MAE: 1.7314, MSE: 6.5246, RMSE: 2.5543, R^2: 0.9814\n",
      "2025-06-04 18:55:18,313 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:18,337 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:55:22,077 - __main__ - INFO - \t MAE: 1.7684, MSE: 7.5087, RMSE: 2.7402, R^2: 0.9781\n",
      "[I 2025-06-04 18:55:22,163] Trial 19 finished with value: -1.749893919604781 and parameters: {'iterations': 2098, 'max_depth': 6, 'learning_rate': 0.015041859097034111, 'l2_leaf_reg': 8.509282420974527}. Best is trial 15 with value: -1.7084738473933712.\n",
      "2025-06-04 18:55:22,236 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:22,259 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:55:25,623 - __main__ - INFO - \t MAE: 1.9440, MSE: 11.1621, RMSE: 3.3410, R^2: 0.9682\n",
      "2025-06-04 18:55:25,635 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:25,659 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:55:30,055 - __main__ - INFO - \t MAE: 1.9996, MSE: 13.1284, RMSE: 3.6233, R^2: 0.9618\n",
      "[I 2025-06-04 18:55:30,133] Trial 20 finished with value: -1.9718459805972262 and parameters: {'iterations': 1366, 'max_depth': 11, 'learning_rate': 0.050885602748261496, 'l2_leaf_reg': 2.967505942864833}. Best is trial 15 with value: -1.7084738473933712.\n",
      "2025-06-04 18:55:30,203 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:30,227 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:55:31,728 - __main__ - INFO - \t MAE: 1.6637, MSE: 5.4876, RMSE: 2.3426, R^2: 0.9844\n",
      "2025-06-04 18:55:31,740 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:31,764 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:55:33,325 - __main__ - INFO - \t MAE: 1.7170, MSE: 6.4630, RMSE: 2.5423, R^2: 0.9812\n",
      "[I 2025-06-04 18:55:33,402] Trial 21 finished with value: -1.6903116403388112 and parameters: {'iterations': 2719, 'max_depth': 5, 'learning_rate': 0.0694964608617458, 'l2_leaf_reg': 0.5978132309341246}. Best is trial 21 with value: -1.6903116403388112.\n",
      "2025-06-04 18:55:33,476 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:33,500 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:55:35,293 - __main__ - INFO - \t MAE: 1.6859, MSE: 5.6585, RMSE: 2.3788, R^2: 0.9839\n",
      "2025-06-04 18:55:35,307 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:35,331 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:55:37,861 - __main__ - INFO - \t MAE: 1.6835, MSE: 6.1829, RMSE: 2.4865, R^2: 0.9820\n",
      "[I 2025-06-04 18:55:37,941] Trial 22 finished with value: -1.6846943496999762 and parameters: {'iterations': 2627, 'max_depth': 5, 'learning_rate': 0.03050284213742654, 'l2_leaf_reg': 1.438038113191645}. Best is trial 22 with value: -1.6846943496999762.\n",
      "2025-06-04 18:55:38,026 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:38,050 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:55:39,675 - __main__ - INFO - \t MAE: 1.7029, MSE: 5.6749, RMSE: 2.3822, R^2: 0.9838\n",
      "2025-06-04 18:55:39,686 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:39,709 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:55:42,068 - __main__ - INFO - \t MAE: 1.6861, MSE: 6.1835, RMSE: 2.4867, R^2: 0.9820\n",
      "[I 2025-06-04 18:55:42,145] Trial 23 finished with value: -1.6945049901455507 and parameters: {'iterations': 2812, 'max_depth': 5, 'learning_rate': 0.02651730307800464, 'l2_leaf_reg': 0.7588396463869549}. Best is trial 22 with value: -1.6846943496999762.\n",
      "2025-06-04 18:55:42,223 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:42,246 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:55:44,670 - __main__ - INFO - \t MAE: 1.7146, MSE: 6.0333, RMSE: 2.4563, R^2: 0.9828\n",
      "2025-06-04 18:55:44,680 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:44,705 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:55:47,665 - __main__ - INFO - \t MAE: 1.7270, MSE: 6.8382, RMSE: 2.6150, R^2: 0.9801\n",
      "[I 2025-06-04 18:55:47,747] Trial 24 finished with value: -1.7207851569866246 and parameters: {'iterations': 2600, 'max_depth': 5, 'learning_rate': 0.02527792320951116, 'l2_leaf_reg': 6.841744031769992}. Best is trial 22 with value: -1.6846943496999762.\n",
      "2025-06-04 18:55:47,820 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:47,844 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:55:51,085 - __main__ - INFO - \t MAE: 1.6587, MSE: 5.4237, RMSE: 2.3289, R^2: 0.9846\n",
      "2025-06-04 18:55:51,097 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:51,121 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:55:54,354 - __main__ - INFO - \t MAE: 1.7056, MSE: 6.2126, RMSE: 2.4925, R^2: 0.9819\n",
      "[I 2025-06-04 18:55:54,437] Trial 25 finished with value: -1.6821422762578258 and parameters: {'iterations': 3461, 'max_depth': 5, 'learning_rate': 0.01208283987497878, 'l2_leaf_reg': 0.6132191467649001}. Best is trial 25 with value: -1.6821422762578258.\n",
      "2025-06-04 18:55:54,517 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:54,541 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:55:57,746 - __main__ - INFO - \t MAE: 1.7700, MSE: 7.3452, RMSE: 2.7102, R^2: 0.9791\n",
      "2025-06-04 18:55:57,756 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:55:57,780 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:56:01,845 - __main__ - INFO - \t MAE: 1.7877, MSE: 7.9046, RMSE: 2.8115, R^2: 0.9770\n",
      "[I 2025-06-04 18:56:01,923] Trial 26 finished with value: -1.7788650792573555 and parameters: {'iterations': 3448, 'max_depth': 7, 'learning_rate': 0.015259146747200452, 'l2_leaf_reg': 3.874774418085221}. Best is trial 25 with value: -1.6821422762578258.\n",
      "2025-06-04 18:56:01,992 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:56:02,015 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:56:06,439 - __main__ - INFO - \t MAE: 1.7309, MSE: 6.2791, RMSE: 2.5058, R^2: 0.9821\n",
      "2025-06-04 18:56:06,450 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:56:06,474 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:56:11,029 - __main__ - INFO - \t MAE: 1.7717, MSE: 7.1729, RMSE: 2.6782, R^2: 0.9791\n",
      "[I 2025-06-04 18:56:11,118] Trial 27 finished with value: -1.7513120706873142 and parameters: {'iterations': 3501, 'max_depth': 5, 'learning_rate': 0.011314271898824845, 'l2_leaf_reg': 11.7447215225016}. Best is trial 25 with value: -1.6821422762578258.\n",
      "2025-06-04 18:56:11,201 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:56:11,224 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:56:14,004 - __main__ - INFO - \t MAE: 1.7681, MSE: 5.9939, RMSE: 2.4483, R^2: 0.9829\n",
      "2025-06-04 18:56:14,015 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:56:14,039 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:56:16,892 - __main__ - INFO - \t MAE: 1.8049, MSE: 6.5289, RMSE: 2.5552, R^2: 0.9810\n",
      "[I 2025-06-04 18:56:16,967] Trial 28 finished with value: -1.7865349164572488 and parameters: {'iterations': 2178, 'max_depth': 3, 'learning_rate': 0.013027210491065509, 'l2_leaf_reg': 7.500271043100355}. Best is trial 25 with value: -1.6821422762578258.\n",
      "2025-06-04 18:56:17,037 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:56:17,060 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 18:56:19,022 - __main__ - INFO - \t MAE: 1.7370, MSE: 6.5665, RMSE: 2.5625, R^2: 0.9813\n",
      "2025-06-04 18:56:19,032 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:56:19,056 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 18:56:21,285 - __main__ - INFO - \t MAE: 1.7568, MSE: 7.0566, RMSE: 2.6564, R^2: 0.9795\n",
      "[I 2025-06-04 18:56:21,358] Trial 29 finished with value: -1.746888864088958 and parameters: {'iterations': 3669, 'max_depth': 6, 'learning_rate': 0.032106753020552666, 'l2_leaf_reg': 3.4938578167429912}. Best is trial 25 with value: -1.6821422762578258.\n",
      "2025-06-04 18:56:21,366 - __main__ - INFO - Best hyperparameters for fold 3: {'iterations': 3461, 'max_depth': 5, 'learning_rate': 0.01208283987497878, 'l2_leaf_reg': 0.6132191467649001}\n",
      "2025-06-04 18:56:21,374 - __main__ - INFO - Best score: -1.6821\n",
      "2025-06-04 18:56:21,374 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:56:21,499 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-04 18:56:21,499 - __main__ - INFO - Retraining Model on Outer Fold 3\n",
      "2025-06-04 18:56:32,318 - __main__ - INFO - \t Inference Time: 0.01413 seconds\n",
      "2025-06-04 18:56:32,320 - __main__ - INFO - \t MAE: 1.7493, MSE: 5.4798, RMSE: 2.3409, R^2: 0.9819\n",
      "2025-06-04 18:56:32,321 - __main__ - INFO - -------------------- Outer Fold 4 --------------------\n",
      "2025-06-04 18:56:32,322 - __main__ - INFO - Starting hyperparameter optimization for outer fold 4...\n",
      "[I 2025-06-04 18:56:32,370] A new study created in RDB with name: 197.4.catboost\n",
      "2025-06-04 18:56:32,458 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:56:32,482 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:56:34,708 - __main__ - INFO - \t MAE: 1.6753, MSE: 6.1761, RMSE: 2.4852, R^2: 0.9797\n",
      "2025-06-04 18:56:34,720 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:56:34,744 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:56:36,925 - __main__ - INFO - \t MAE: 1.7304, MSE: 6.6482, RMSE: 2.5784, R^2: 0.9816\n",
      "[I 2025-06-04 18:56:37,009] Trial 0 finished with value: -1.702852404076392 and parameters: {'iterations': 1000, 'max_depth': 6, 'learning_rate': 0.03574712922600244, 'l2_leaf_reg': 3.0}. Best is trial 0 with value: -1.702852404076392.\n",
      "2025-06-04 18:56:37,075 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:56:37,099 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:56:39,544 - __main__ - INFO - \t MAE: 1.8579, MSE: 8.2591, RMSE: 2.8739, R^2: 0.9729\n",
      "2025-06-04 18:56:39,555 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:56:39,579 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:56:42,344 - __main__ - INFO - \t MAE: 1.9730, MSE: 13.5437, RMSE: 3.6802, R^2: 0.9624\n",
      "[I 2025-06-04 18:56:42,419] Trial 1 finished with value: -1.9154207229441507 and parameters: {'iterations': 3920, 'max_depth': 10, 'learning_rate': 0.07661100707771368, 'l2_leaf_reg': 5.102549893051878}. Best is trial 0 with value: -1.702852404076392.\n",
      "2025-06-04 18:56:42,477 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:56:42,501 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:56:43,889 - __main__ - INFO - \t MAE: 1.7906, MSE: 5.9636, RMSE: 2.4421, R^2: 0.9804\n",
      "2025-06-04 18:56:43,899 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:56:43,923 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:56:45,522 - __main__ - INFO - \t MAE: 1.8022, MSE: 6.2400, RMSE: 2.4980, R^2: 0.9827\n",
      "[I 2025-06-04 18:56:45,599] Trial 2 finished with value: -1.7963674937406509 and parameters: {'iterations': 1071, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'l2_leaf_reg': 18.232892846424658}. Best is trial 0 with value: -1.702852404076392.\n",
      "2025-06-04 18:56:45,659 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:56:45,682 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:56:47,097 - __main__ - INFO - \t MAE: 1.7603, MSE: 5.8009, RMSE: 2.4085, R^2: 0.9809\n",
      "2025-06-04 18:56:47,108 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:56:47,131 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:56:48,751 - __main__ - INFO - \t MAE: 1.8487, MSE: 6.6986, RMSE: 2.5882, R^2: 0.9814\n",
      "[I 2025-06-04 18:56:48,833] Trial 3 finished with value: -1.8044808551152505 and parameters: {'iterations': 3050, 'max_depth': 2, 'learning_rate': 0.2708160864249968, 'l2_leaf_reg': 25.057057903612442}. Best is trial 0 with value: -1.702852404076392.\n",
      "2025-06-04 18:56:48,893 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:56:48,917 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:56:51,053 - __main__ - INFO - \t MAE: 1.7522, MSE: 6.0026, RMSE: 2.4500, R^2: 0.9803\n",
      "2025-06-04 18:56:51,065 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:56:51,088 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:56:53,402 - __main__ - INFO - \t MAE: 1.8278, MSE: 6.6582, RMSE: 2.5804, R^2: 0.9815\n",
      "[I 2025-06-04 18:56:53,490] Trial 4 finished with value: -1.790032847958941 and parameters: {'iterations': 1273, 'max_depth': 4, 'learning_rate': 0.018659959624904916, 'l2_leaf_reg': 9.475146167306363}. Best is trial 0 with value: -1.702852404076392.\n",
      "2025-06-04 18:56:53,562 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:56:53,586 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:56:56,785 - __main__ - INFO - \t MAE: 1.7243, MSE: 6.2290, RMSE: 2.4958, R^2: 0.9795\n",
      "2025-06-04 18:56:56,796 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:56:56,820 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:57:00,441 - __main__ - INFO - \t MAE: 1.8084, MSE: 8.3924, RMSE: 2.8970, R^2: 0.9767\n",
      "[I 2025-06-04 18:57:00,521] Trial 5 finished with value: -1.7663242778117825 and parameters: {'iterations': 2393, 'max_depth': 6, 'learning_rate': 0.02692655251486473, 'l2_leaf_reg': 18.549660394310195}. Best is trial 0 with value: -1.702852404076392.\n",
      "2025-06-04 18:57:00,580 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:00,604 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:57:02,687 - __main__ - INFO - \t MAE: 1.7271, MSE: 6.1329, RMSE: 2.4765, R^2: 0.9799\n",
      "2025-06-04 18:57:02,698 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:02,721 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:57:04,944 - __main__ - INFO - \t MAE: 1.8245, MSE: 7.5161, RMSE: 2.7416, R^2: 0.9791\n",
      "[I 2025-06-04 18:57:05,028] Trial 6 finished with value: -1.7758007885055462 and parameters: {'iterations': 1012, 'max_depth': 5, 'learning_rate': 0.03476649150592621, 'l2_leaf_reg': 13.95406453440256}. Best is trial 0 with value: -1.702852404076392.\n",
      "2025-06-04 18:57:05,094 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:05,118 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:57:06,793 - __main__ - INFO - \t MAE: 1.7428, MSE: 5.8933, RMSE: 2.4276, R^2: 0.9806\n",
      "2025-06-04 18:57:06,804 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:06,828 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:57:09,396 - __main__ - INFO - \t MAE: 1.7504, MSE: 6.6162, RMSE: 2.5722, R^2: 0.9816\n",
      "[I 2025-06-04 18:57:09,476] Trial 7 finished with value: -1.7466191412337828 and parameters: {'iterations': 3326, 'max_depth': 4, 'learning_rate': 0.05748924681991978, 'l2_leaf_reg': 17.976229781430252}. Best is trial 0 with value: -1.702852404076392.\n",
      "2025-06-04 18:57:09,535 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:09,559 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:57:11,839 - __main__ - INFO - \t MAE: 1.8076, MSE: 7.6982, RMSE: 2.7746, R^2: 0.9747\n",
      "2025-06-04 18:57:11,849 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:11,873 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:57:14,281 - __main__ - INFO - \t MAE: 1.9138, MSE: 10.0911, RMSE: 3.1767, R^2: 0.9720\n",
      "[I 2025-06-04 18:57:14,358] Trial 8 finished with value: -1.8606662354399806 and parameters: {'iterations': 678, 'max_depth': 8, 'learning_rate': 0.0178601378893971, 'l2_leaf_reg': 2.419021993065746}. Best is trial 0 with value: -1.702852404076392.\n",
      "2025-06-04 18:57:14,421 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:14,445 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:57:17,716 - __main__ - INFO - \t MAE: 1.9358, MSE: 9.6515, RMSE: 3.1067, R^2: 0.9683\n",
      "2025-06-04 18:57:17,728 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:17,752 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:57:21,650 - __main__ - INFO - \t MAE: 2.1090, MSE: 18.1465, RMSE: 4.2599, R^2: 0.9497\n",
      "[I 2025-06-04 18:57:21,722] Trial 9 finished with value: -2.022384612982304 and parameters: {'iterations': 3913, 'max_depth': 12, 'learning_rate': 0.1563510870813346, 'l2_leaf_reg': 9.486106190614436}. Best is trial 0 with value: -1.702852404076392.\n",
      "2025-06-04 18:57:21,796 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:21,819 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:57:26,137 - __main__ - INFO - \t MAE: 1.9406, MSE: 8.8992, RMSE: 2.9831, R^2: 0.9708\n",
      "2025-06-04 18:57:26,149 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:26,173 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:57:30,640 - __main__ - INFO - \t MAE: 2.1057, MSE: 14.0187, RMSE: 3.7442, R^2: 0.9611\n",
      "[I 2025-06-04 18:57:30,747] Trial 10 finished with value: -2.023154635961903 and parameters: {'iterations': 1805, 'max_depth': 8, 'learning_rate': 0.010260022325256576, 'l2_leaf_reg': 28.002565086397723}. Best is trial 0 with value: -1.702852404076392.\n",
      "2025-06-04 18:57:30,841 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:30,865 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:57:32,497 - __main__ - INFO - \t MAE: 1.7507, MSE: 5.9657, RMSE: 2.4425, R^2: 0.9804\n",
      "2025-06-04 18:57:32,509 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:32,533 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:57:34,602 - __main__ - INFO - \t MAE: 1.7871, MSE: 7.0767, RMSE: 2.6602, R^2: 0.9804\n",
      "[I 2025-06-04 18:57:34,680] Trial 11 finished with value: -1.7688982110152054 and parameters: {'iterations': 3043, 'max_depth': 4, 'learning_rate': 0.06688881603939227, 'l2_leaf_reg': 21.089636353095667}. Best is trial 0 with value: -1.702852404076392.\n",
      "2025-06-04 18:57:34,749 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:34,773 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:57:37,529 - __main__ - INFO - \t MAE: 1.7254, MSE: 6.6174, RMSE: 2.5724, R^2: 0.9783\n",
      "2025-06-04 18:57:37,539 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:37,563 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:57:40,054 - __main__ - INFO - \t MAE: 1.8386, MSE: 9.7209, RMSE: 3.1178, R^2: 0.9730\n",
      "[I 2025-06-04 18:57:40,127] Trial 12 finished with value: -1.7820114796497792 and parameters: {'iterations': 2929, 'max_depth': 7, 'learning_rate': 0.047998151931359735, 'l2_leaf_reg': 12.51313956442466}. Best is trial 0 with value: -1.702852404076392.\n",
      "2025-06-04 18:57:40,195 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:40,219 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:57:41,638 - __main__ - INFO - \t MAE: 1.7096, MSE: 5.7732, RMSE: 2.4028, R^2: 0.9810\n",
      "2025-06-04 18:57:41,648 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:41,672 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:57:43,328 - __main__ - INFO - \t MAE: 1.7434, MSE: 6.4745, RMSE: 2.5445, R^2: 0.9820\n",
      "[I 2025-06-04 18:57:43,399] Trial 13 finished with value: -1.7265394909222704 and parameters: {'iterations': 1896, 'max_depth': 4, 'learning_rate': 0.10072383250581336, 'l2_leaf_reg': 6.3919255270898905}. Best is trial 0 with value: -1.702852404076392.\n",
      "2025-06-04 18:57:43,467 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:43,490 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:57:44,755 - __main__ - INFO - \t MAE: 1.6781, MSE: 6.0182, RMSE: 2.4532, R^2: 0.9802\n",
      "2025-06-04 18:57:44,767 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:44,790 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:57:46,153 - __main__ - INFO - \t MAE: 1.7668, MSE: 6.6164, RMSE: 2.5722, R^2: 0.9816\n",
      "[I 2025-06-04 18:57:46,241] Trial 14 finished with value: -1.7224423705131262 and parameters: {'iterations': 1731, 'max_depth': 6, 'learning_rate': 0.10803136175669444, 'l2_leaf_reg': 0.7244912750975265}. Best is trial 0 with value: -1.702852404076392.\n",
      "2025-06-04 18:57:46,322 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:46,346 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:57:47,767 - __main__ - INFO - \t MAE: 1.8742, MSE: 8.9890, RMSE: 2.9982, R^2: 0.9705\n",
      "2025-06-04 18:57:47,778 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:47,802 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:57:49,438 - __main__ - INFO - \t MAE: 1.9637, MSE: 11.6816, RMSE: 3.4178, R^2: 0.9676\n",
      "[I 2025-06-04 18:57:49,514] Trial 15 finished with value: -1.9189796048531964 and parameters: {'iterations': 1690, 'max_depth': 9, 'learning_rate': 0.11739806935026562, 'l2_leaf_reg': 1.8412139329176223}. Best is trial 0 with value: -1.702852404076392.\n",
      "2025-06-04 18:57:49,586 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:49,610 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:57:51,227 - __main__ - INFO - \t MAE: 1.6945, MSE: 6.1518, RMSE: 2.4803, R^2: 0.9798\n",
      "2025-06-04 18:57:51,239 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:51,262 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:57:53,116 - __main__ - INFO - \t MAE: 1.7149, MSE: 6.2233, RMSE: 2.4946, R^2: 0.9827\n",
      "[I 2025-06-04 18:57:53,197] Trial 16 finished with value: -1.7046929302222362 and parameters: {'iterations': 2388, 'max_depth': 6, 'learning_rate': 0.03972771466278648, 'l2_leaf_reg': 0.7902222446015803}. Best is trial 0 with value: -1.702852404076392.\n",
      "2025-06-04 18:57:53,266 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:53,290 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:57:58,929 - __main__ - INFO - \t MAE: 1.8792, MSE: 8.7036, RMSE: 2.9502, R^2: 0.9714\n",
      "2025-06-04 18:57:58,939 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:57:58,963 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:58:03,265 - __main__ - INFO - \t MAE: 2.0321, MSE: 15.1538, RMSE: 3.8928, R^2: 0.9580\n",
      "[I 2025-06-04 18:58:03,348] Trial 17 finished with value: -1.9556428272231934 and parameters: {'iterations': 2355, 'max_depth': 11, 'learning_rate': 0.03452351502922888, 'l2_leaf_reg': 6.054567966275993}. Best is trial 0 with value: -1.702852404076392.\n",
      "2025-06-04 18:58:03,421 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:03,445 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:58:05,172 - __main__ - INFO - \t MAE: 1.8331, MSE: 7.0372, RMSE: 2.6528, R^2: 0.9769\n",
      "2025-06-04 18:58:05,182 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:05,206 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:58:07,067 - __main__ - INFO - \t MAE: 1.9276, MSE: 8.7097, RMSE: 2.9512, R^2: 0.9758\n",
      "[I 2025-06-04 18:58:07,147] Trial 18 finished with value: -1.8803321080918007 and parameters: {'iterations': 603, 'max_depth': 6, 'learning_rate': 0.022276867878459825, 'l2_leaf_reg': 9.715897235124418}. Best is trial 0 with value: -1.702852404076392.\n",
      "2025-06-04 18:58:07,218 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:07,241 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:58:11,927 - __main__ - INFO - \t MAE: 1.7569, MSE: 7.1766, RMSE: 2.6789, R^2: 0.9764\n",
      "2025-06-04 18:58:11,939 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:11,963 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:58:17,046 - __main__ - INFO - \t MAE: 1.8564, MSE: 10.0405, RMSE: 3.1687, R^2: 0.9721\n",
      "[I 2025-06-04 18:58:17,121] Trial 19 finished with value: -1.806697552341434 and parameters: {'iterations': 2614, 'max_depth': 8, 'learning_rate': 0.012897454741395966, 'l2_leaf_reg': 4.367851399791581}. Best is trial 0 with value: -1.702852404076392.\n",
      "2025-06-04 18:58:17,192 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:17,215 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:58:19,845 - __main__ - INFO - \t MAE: 1.7186, MSE: 6.5461, RMSE: 2.5585, R^2: 0.9785\n",
      "2025-06-04 18:58:19,856 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:19,881 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:58:22,458 - __main__ - INFO - \t MAE: 1.8405, MSE: 9.4776, RMSE: 3.0786, R^2: 0.9737\n",
      "[I 2025-06-04 18:58:22,532] Trial 20 finished with value: -1.779565778298437 and parameters: {'iterations': 1435, 'max_depth': 7, 'learning_rate': 0.04104975015072629, 'l2_leaf_reg': 11.475793696534959}. Best is trial 0 with value: -1.702852404076392.\n",
      "2025-06-04 18:58:22,601 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:22,625 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:58:23,976 - __main__ - INFO - \t MAE: 1.6670, MSE: 5.9071, RMSE: 2.4304, R^2: 0.9806\n",
      "2025-06-04 18:58:23,986 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:24,010 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:58:25,529 - __main__ - INFO - \t MAE: 1.7348, MSE: 6.4833, RMSE: 2.5462, R^2: 0.9820\n",
      "[I 2025-06-04 18:58:25,606] Trial 21 finished with value: -1.7008883991789778 and parameters: {'iterations': 2083, 'max_depth': 6, 'learning_rate': 0.08842801558310893, 'l2_leaf_reg': 0.5584243837613563}. Best is trial 21 with value: -1.7008883991789778.\n",
      "2025-06-04 18:58:25,682 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:25,706 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:58:27,250 - __main__ - INFO - \t MAE: 1.6862, MSE: 5.8830, RMSE: 2.4255, R^2: 0.9807\n",
      "2025-06-04 18:58:27,260 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:27,283 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:58:29,034 - __main__ - INFO - \t MAE: 1.7293, MSE: 6.3012, RMSE: 2.5102, R^2: 0.9825\n",
      "[I 2025-06-04 18:58:29,121] Trial 22 finished with value: -1.707777698848108 and parameters: {'iterations': 2121, 'max_depth': 5, 'learning_rate': 0.07727027918791007, 'l2_leaf_reg': 3.4981098702846873}. Best is trial 21 with value: -1.7008883991789778.\n",
      "2025-06-04 18:58:29,200 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:29,223 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:58:31,555 - __main__ - INFO - \t MAE: 1.7030, MSE: 6.7576, RMSE: 2.5995, R^2: 0.9778\n",
      "2025-06-04 18:58:31,566 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:31,591 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:58:33,555 - __main__ - INFO - \t MAE: 1.8108, MSE: 8.0845, RMSE: 2.8433, R^2: 0.9776\n",
      "[I 2025-06-04 18:58:33,629] Trial 23 finished with value: -1.7568892202757551 and parameters: {'iterations': 2630, 'max_depth': 7, 'learning_rate': 0.029681823431321242, 'l2_leaf_reg': 0.7606980951561546}. Best is trial 21 with value: -1.7008883991789778.\n",
      "2025-06-04 18:58:33,701 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:33,724 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:58:35,859 - __main__ - INFO - \t MAE: 1.6860, MSE: 5.9247, RMSE: 2.4341, R^2: 0.9805\n",
      "2025-06-04 18:58:35,870 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:35,894 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:58:38,083 - __main__ - INFO - \t MAE: 1.7308, MSE: 6.5757, RMSE: 2.5643, R^2: 0.9818\n",
      "[I 2025-06-04 18:58:38,162] Trial 24 finished with value: -1.7084009791172226 and parameters: {'iterations': 3521, 'max_depth': 5, 'learning_rate': 0.04841386825674229, 'l2_leaf_reg': 7.773490593488541}. Best is trial 21 with value: -1.7008883991789778.\n",
      "2025-06-04 18:58:38,233 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:38,256 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:58:40,467 - __main__ - INFO - \t MAE: 1.7911, MSE: 7.6577, RMSE: 2.7673, R^2: 0.9748\n",
      "2025-06-04 18:58:40,477 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:40,501 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:58:42,489 - __main__ - INFO - \t MAE: 1.9313, MSE: 11.7661, RMSE: 3.4302, R^2: 0.9674\n",
      "[I 2025-06-04 18:58:42,561] Trial 25 finished with value: -1.8611544388163836 and parameters: {'iterations': 2210, 'max_depth': 9, 'learning_rate': 0.05920643664094753, 'l2_leaf_reg': 3.3870130397991787}. Best is trial 21 with value: -1.7008883991789778.\n",
      "2025-06-04 18:58:42,632 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:42,655 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:58:44,137 - __main__ - INFO - \t MAE: 1.7101, MSE: 5.5223, RMSE: 2.3500, R^2: 0.9819\n",
      "2025-06-04 18:58:44,147 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:44,171 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:58:45,804 - __main__ - INFO - \t MAE: 1.6905, MSE: 5.6104, RMSE: 2.3686, R^2: 0.9844\n",
      "[I 2025-06-04 18:58:45,876] Trial 26 finished with value: -1.7002564827105626 and parameters: {'iterations': 1447, 'max_depth': 3, 'learning_rate': 0.08296809747197205, 'l2_leaf_reg': 0.6350810887565392}. Best is trial 26 with value: -1.7002564827105626.\n",
      "2025-06-04 18:58:45,945 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:45,969 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:58:47,324 - __main__ - INFO - \t MAE: 1.7363, MSE: 5.7575, RMSE: 2.3995, R^2: 0.9811\n",
      "2025-06-04 18:58:47,335 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:47,359 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:58:48,982 - __main__ - INFO - \t MAE: 1.7501, MSE: 6.0370, RMSE: 2.4570, R^2: 0.9832\n",
      "[I 2025-06-04 18:58:49,062] Trial 27 finished with value: -1.7432256563934383 and parameters: {'iterations': 1353, 'max_depth': 3, 'learning_rate': 0.14868714272900804, 'l2_leaf_reg': 7.387054892694969}. Best is trial 26 with value: -1.7002564827105626.\n",
      "2025-06-04 18:58:49,132 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:49,155 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:58:50,500 - __main__ - INFO - \t MAE: 1.7504, MSE: 5.9010, RMSE: 2.4292, R^2: 0.9806\n",
      "2025-06-04 18:58:50,511 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:50,535 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:58:52,146 - __main__ - INFO - \t MAE: 1.7432, MSE: 5.9223, RMSE: 2.4336, R^2: 0.9836\n",
      "[I 2025-06-04 18:58:52,228] Trial 28 finished with value: -1.746784925864103 and parameters: {'iterations': 929, 'max_depth': 3, 'learning_rate': 0.08433319925944287, 'l2_leaf_reg': 4.132842888172751}. Best is trial 26 with value: -1.7002564827105626.\n",
      "2025-06-04 18:58:52,304 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:52,328 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 18:58:53,664 - __main__ - INFO - \t MAE: 1.7549, MSE: 5.9092, RMSE: 2.4309, R^2: 0.9806\n",
      "2025-06-04 18:58:53,675 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:53,698 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 18:58:55,439 - __main__ - INFO - \t MAE: 1.7200, MSE: 5.7970, RMSE: 2.4077, R^2: 0.9839\n",
      "[I 2025-06-04 18:58:55,519] Trial 29 finished with value: -1.7374698267726343 and parameters: {'iterations': 1587, 'max_depth': 3, 'learning_rate': 0.08748229514668844, 'l2_leaf_reg': 4.3205533127736215}. Best is trial 26 with value: -1.7002564827105626.\n",
      "2025-06-04 18:58:55,528 - __main__ - INFO - Best hyperparameters for fold 4: {'iterations': 1447, 'max_depth': 3, 'learning_rate': 0.08296809747197205, 'l2_leaf_reg': 0.6350810887565392}\n",
      "2025-06-04 18:58:55,536 - __main__ - INFO - Best score: -1.7003\n",
      "2025-06-04 18:58:55,537 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:55,661 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-04 18:58:55,661 - __main__ - INFO - Retraining Model on Outer Fold 4\n",
      "2025-06-04 18:58:58,234 - __main__ - INFO - \t Inference Time: 0.00678 seconds\n",
      "2025-06-04 18:58:58,236 - __main__ - INFO - \t MAE: 1.8379, MSE: 7.3293, RMSE: 2.7073, R^2: 0.9798\n",
      "2025-06-04 18:58:58,237 - __main__ - INFO - -------------------- Outer Fold 5 --------------------\n",
      "2025-06-04 18:58:58,238 - __main__ - INFO - Starting hyperparameter optimization for outer fold 5...\n",
      "[I 2025-06-04 18:58:58,276] A new study created in RDB with name: 197.5.catboost\n",
      "2025-06-04 18:58:58,360 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:58:58,383 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 18:59:00,087 - __main__ - INFO - \t MAE: 1.7530, MSE: 7.3180, RMSE: 2.7052, R^2: 0.9784\n",
      "2025-06-04 18:59:00,099 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:00,123 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 18:59:02,355 - __main__ - INFO - \t MAE: 1.8416, MSE: 7.9957, RMSE: 2.8277, R^2: 0.9740\n",
      "[I 2025-06-04 18:59:02,440] Trial 0 finished with value: -1.7973125615756784 and parameters: {'iterations': 1000, 'max_depth': 6, 'learning_rate': 0.03574712922600244, 'l2_leaf_reg': 3.0}. Best is trial 0 with value: -1.7973125615756784.\n",
      "2025-06-04 18:59:02,500 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:02,523 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 18:59:05,153 - __main__ - INFO - \t MAE: 1.9179, MSE: 14.4991, RMSE: 3.8078, R^2: 0.9571\n",
      "2025-06-04 18:59:05,164 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:05,188 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 18:59:07,984 - __main__ - INFO - \t MAE: 2.0104, MSE: 12.8672, RMSE: 3.5871, R^2: 0.9581\n",
      "[I 2025-06-04 18:59:08,056] Trial 1 finished with value: -1.9641192686732558 and parameters: {'iterations': 3920, 'max_depth': 10, 'learning_rate': 0.07661100707771368, 'l2_leaf_reg': 5.102549893051878}. Best is trial 0 with value: -1.7973125615756784.\n",
      "2025-06-04 18:59:08,120 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:08,144 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 18:59:09,352 - __main__ - INFO - \t MAE: 1.8937, MSE: 7.4850, RMSE: 2.7359, R^2: 0.9779\n",
      "2025-06-04 18:59:09,364 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:09,388 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 18:59:10,914 - __main__ - INFO - \t MAE: 1.8856, MSE: 7.2120, RMSE: 2.6855, R^2: 0.9765\n",
      "[I 2025-06-04 18:59:11,002] Trial 2 finished with value: -1.8896251616712492 and parameters: {'iterations': 1071, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'l2_leaf_reg': 18.232892846424658}. Best is trial 0 with value: -1.7973125615756784.\n",
      "2025-06-04 18:59:11,072 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:11,095 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 18:59:12,233 - __main__ - INFO - \t MAE: 1.9252, MSE: 7.7216, RMSE: 2.7788, R^2: 0.9772\n",
      "2025-06-04 18:59:12,244 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:12,268 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 18:59:13,699 - __main__ - INFO - \t MAE: 1.9134, MSE: 7.2492, RMSE: 2.6924, R^2: 0.9764\n",
      "[I 2025-06-04 18:59:13,772] Trial 3 finished with value: -1.919272742737184 and parameters: {'iterations': 3050, 'max_depth': 2, 'learning_rate': 0.2708160864249968, 'l2_leaf_reg': 25.057057903612442}. Best is trial 0 with value: -1.7973125615756784.\n",
      "2025-06-04 18:59:13,829 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:13,852 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 18:59:15,989 - __main__ - INFO - \t MAE: 1.8141, MSE: 7.1462, RMSE: 2.6732, R^2: 0.9789\n",
      "2025-06-04 18:59:15,999 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:16,023 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 18:59:18,305 - __main__ - INFO - \t MAE: 1.8882, MSE: 7.3864, RMSE: 2.7178, R^2: 0.9760\n",
      "[I 2025-06-04 18:59:18,382] Trial 4 finished with value: -1.8511212937816421 and parameters: {'iterations': 1273, 'max_depth': 4, 'learning_rate': 0.018659959624904916, 'l2_leaf_reg': 9.475146167306363}. Best is trial 0 with value: -1.7973125615756784.\n",
      "2025-06-04 18:59:18,440 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:18,464 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 18:59:20,920 - __main__ - INFO - \t MAE: 1.8312, MSE: 8.7969, RMSE: 2.9660, R^2: 0.9740\n",
      "2025-06-04 18:59:20,931 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:20,955 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 18:59:24,833 - __main__ - INFO - \t MAE: 1.8259, MSE: 7.9720, RMSE: 2.8235, R^2: 0.9741\n",
      "[I 2025-06-04 18:59:24,910] Trial 5 finished with value: -1.8285494662971649 and parameters: {'iterations': 2393, 'max_depth': 6, 'learning_rate': 0.02692655251486473, 'l2_leaf_reg': 18.549660394310195}. Best is trial 0 with value: -1.7973125615756784.\n",
      "2025-06-04 18:59:24,966 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:24,990 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 18:59:27,032 - __main__ - INFO - \t MAE: 1.7948, MSE: 7.7093, RMSE: 2.7766, R^2: 0.9772\n",
      "2025-06-04 18:59:27,041 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:27,065 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 18:59:29,222 - __main__ - INFO - \t MAE: 1.8651, MSE: 7.9936, RMSE: 2.8273, R^2: 0.9740\n",
      "[I 2025-06-04 18:59:29,298] Trial 6 finished with value: -1.8299084276895352 and parameters: {'iterations': 1012, 'max_depth': 5, 'learning_rate': 0.03476649150592621, 'l2_leaf_reg': 13.95406453440256}. Best is trial 0 with value: -1.7973125615756784.\n",
      "2025-06-04 18:59:29,355 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:29,379 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 18:59:31,102 - __main__ - INFO - \t MAE: 1.7980, MSE: 7.4717, RMSE: 2.7334, R^2: 0.9779\n",
      "2025-06-04 18:59:31,113 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:31,137 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 18:59:33,112 - __main__ - INFO - \t MAE: 1.8265, MSE: 7.0633, RMSE: 2.6577, R^2: 0.9770\n",
      "[I 2025-06-04 18:59:33,181] Trial 7 finished with value: -1.812211580965899 and parameters: {'iterations': 3326, 'max_depth': 4, 'learning_rate': 0.05748924681991978, 'l2_leaf_reg': 17.976229781430252}. Best is trial 0 with value: -1.7973125615756784.\n",
      "2025-06-04 18:59:33,235 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:33,259 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 18:59:35,529 - __main__ - INFO - \t MAE: 1.8511, MSE: 10.0589, RMSE: 3.1716, R^2: 0.9702\n",
      "2025-06-04 18:59:35,539 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:35,563 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 18:59:37,945 - __main__ - INFO - \t MAE: 1.9759, MSE: 10.7357, RMSE: 3.2765, R^2: 0.9651\n",
      "[I 2025-06-04 18:59:38,021] Trial 8 finished with value: -1.9134813971529239 and parameters: {'iterations': 678, 'max_depth': 8, 'learning_rate': 0.0178601378893971, 'l2_leaf_reg': 2.419021993065746}. Best is trial 0 with value: -1.7973125615756784.\n",
      "2025-06-04 18:59:38,077 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:38,101 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 18:59:41,249 - __main__ - INFO - \t MAE: 2.0365, MSE: 15.2130, RMSE: 3.9004, R^2: 0.9550\n",
      "2025-06-04 18:59:41,260 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:41,284 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 18:59:46,126 - __main__ - INFO - \t MAE: 2.1112, MSE: 14.5103, RMSE: 3.8092, R^2: 0.9528\n",
      "[I 2025-06-04 18:59:46,210] Trial 9 finished with value: -2.073834571490803 and parameters: {'iterations': 3913, 'max_depth': 12, 'learning_rate': 0.1563510870813346, 'l2_leaf_reg': 9.486106190614436}. Best is trial 0 with value: -1.7973125615756784.\n",
      "2025-06-04 18:59:46,280 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:46,303 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 18:59:50,673 - __main__ - INFO - \t MAE: 2.0317, MSE: 12.9584, RMSE: 3.5998, R^2: 0.9617\n",
      "2025-06-04 18:59:50,685 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:50,709 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 18:59:55,213 - __main__ - INFO - \t MAE: 2.0981, MSE: 11.1990, RMSE: 3.3465, R^2: 0.9636\n",
      "[I 2025-06-04 18:59:55,294] Trial 10 finished with value: -2.0648993291857605 and parameters: {'iterations': 1805, 'max_depth': 8, 'learning_rate': 0.010260022325256576, 'l2_leaf_reg': 28.002565086397723}. Best is trial 0 with value: -1.7973125615756784.\n",
      "2025-06-04 18:59:55,362 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:55,385 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 18:59:56,934 - __main__ - INFO - \t MAE: 1.8249, MSE: 7.8223, RMSE: 2.7968, R^2: 0.9769\n",
      "2025-06-04 18:59:56,944 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:56,968 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 18:59:58,764 - __main__ - INFO - \t MAE: 1.8493, MSE: 7.1706, RMSE: 2.6778, R^2: 0.9767\n",
      "[I 2025-06-04 18:59:58,845] Trial 11 finished with value: -1.8370881505262462 and parameters: {'iterations': 3043, 'max_depth': 4, 'learning_rate': 0.06688881603939227, 'l2_leaf_reg': 21.089636353095667}. Best is trial 0 with value: -1.7973125615756784.\n",
      "2025-06-04 18:59:58,925 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 18:59:58,949 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 19:00:01,178 - __main__ - INFO - \t MAE: 1.8252, MSE: 10.1348, RMSE: 3.1835, R^2: 0.9700\n",
      "2025-06-04 19:00:01,189 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:00:01,213 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 19:00:04,131 - __main__ - INFO - \t MAE: 1.8491, MSE: 8.8425, RMSE: 2.9736, R^2: 0.9712\n",
      "[I 2025-06-04 19:00:04,209] Trial 12 finished with value: -1.837127418660176 and parameters: {'iterations': 2929, 'max_depth': 7, 'learning_rate': 0.047998151931359735, 'l2_leaf_reg': 12.51313956442466}. Best is trial 0 with value: -1.7973125615756784.\n",
      "2025-06-04 19:00:04,280 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:00:04,303 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 19:00:05,562 - __main__ - INFO - \t MAE: 1.7955, MSE: 7.2201, RMSE: 2.6870, R^2: 0.9786\n",
      "2025-06-04 19:00:05,572 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:00:05,596 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 19:00:07,491 - __main__ - INFO - \t MAE: 1.7943, MSE: 6.7597, RMSE: 2.5999, R^2: 0.9780\n",
      "[I 2025-06-04 19:00:07,563] Trial 13 finished with value: -1.794862868753111 and parameters: {'iterations': 1896, 'max_depth': 4, 'learning_rate': 0.10072383250581336, 'l2_leaf_reg': 6.3919255270898905}. Best is trial 13 with value: -1.794862868753111.\n",
      "2025-06-04 19:00:07,636 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:00:07,660 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 19:00:08,940 - __main__ - INFO - \t MAE: 1.7712, MSE: 7.2078, RMSE: 2.6847, R^2: 0.9787\n",
      "2025-06-04 19:00:08,950 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:00:08,974 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 19:00:10,439 - __main__ - INFO - \t MAE: 1.8369, MSE: 8.0157, RMSE: 2.8312, R^2: 0.9739\n",
      "[I 2025-06-04 19:00:10,517] Trial 14 finished with value: -1.8040529305755317 and parameters: {'iterations': 1731, 'max_depth': 6, 'learning_rate': 0.10803136175669444, 'l2_leaf_reg': 0.7244912750975265}. Best is trial 13 with value: -1.794862868753111.\n",
      "2025-06-04 19:00:10,590 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:00:10,613 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 19:00:11,873 - __main__ - INFO - \t MAE: 1.8242, MSE: 7.0545, RMSE: 2.6560, R^2: 0.9791\n",
      "2025-06-04 19:00:11,884 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:00:11,908 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 19:00:13,497 - __main__ - INFO - \t MAE: 1.8261, MSE: 6.7633, RMSE: 2.6006, R^2: 0.9780\n",
      "[I 2025-06-04 19:00:13,581] Trial 15 finished with value: -1.8251762536405955 and parameters: {'iterations': 1820, 'max_depth': 3, 'learning_rate': 0.1117144490080229, 'l2_leaf_reg': 7.412292845076138}. Best is trial 13 with value: -1.794862868753111.\n",
      "2025-06-04 19:00:13,652 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:00:13,675 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 19:00:16,264 - __main__ - INFO - \t MAE: 1.8717, MSE: 11.5241, RMSE: 3.3947, R^2: 0.9659\n",
      "2025-06-04 19:00:16,275 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:00:16,299 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 19:00:19,428 - __main__ - INFO - \t MAE: 1.9413, MSE: 11.0824, RMSE: 3.3290, R^2: 0.9640\n",
      "[I 2025-06-04 19:00:19,508] Trial 16 finished with value: -1.90651952119079 and parameters: {'iterations': 2372, 'max_depth': 9, 'learning_rate': 0.03745804102009777, 'l2_leaf_reg': 4.666033193519914}. Best is trial 13 with value: -1.794862868753111.\n",
      "2025-06-04 19:00:19,583 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:00:19,607 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 19:00:20,801 - __main__ - INFO - \t MAE: 1.8284, MSE: 8.1183, RMSE: 2.8493, R^2: 0.9760\n",
      "2025-06-04 19:00:20,814 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:00:20,837 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 19:00:22,617 - __main__ - INFO - \t MAE: 1.8075, MSE: 7.2626, RMSE: 2.6949, R^2: 0.9764\n",
      "[I 2025-06-04 19:00:22,700] Trial 17 finished with value: -1.8179613815543478 and parameters: {'iterations': 1430, 'max_depth': 6, 'learning_rate': 0.12012810326827857, 'l2_leaf_reg': 4.91809425349141}. Best is trial 13 with value: -1.794862868753111.\n",
      "2025-06-04 19:00:22,774 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:00:22,798 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 19:00:24,412 - __main__ - INFO - \t MAE: 1.7937, MSE: 7.8134, RMSE: 2.7952, R^2: 0.9769\n",
      "2025-06-04 19:00:24,422 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:00:24,445 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 19:00:26,221 - __main__ - INFO - \t MAE: 1.8379, MSE: 7.8049, RMSE: 2.7937, R^2: 0.9746\n",
      "[I 2025-06-04 19:00:26,302] Trial 18 finished with value: -1.81580296314373 and parameters: {'iterations': 603, 'max_depth': 5, 'learning_rate': 0.07507247027716867, 'l2_leaf_reg': 11.7994853555663}. Best is trial 13 with value: -1.794862868753111.\n",
      "2025-06-04 19:00:26,372 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:00:26,396 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 19:00:34,128 - __main__ - INFO - \t MAE: 2.1654, MSE: 19.0279, RMSE: 4.3621, R^2: 0.9437\n",
      "2025-06-04 19:00:34,140 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:00:34,164 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 19:00:44,221 - __main__ - INFO - \t MAE: 2.3081, MSE: 22.1854, RMSE: 4.7101, R^2: 0.9278\n",
      "[I 2025-06-04 19:00:44,307] Trial 19 finished with value: -2.23674808426835 and parameters: {'iterations': 2098, 'max_depth': 12, 'learning_rate': 0.02318231026142995, 'l2_leaf_reg': 0.6072311571937838}. Best is trial 13 with value: -1.794862868753111.\n",
      "2025-06-04 19:00:44,386 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:00:44,409 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 19:00:50,757 - __main__ - INFO - \t MAE: 1.9103, MSE: 12.9511, RMSE: 3.5988, R^2: 0.9617\n",
      "2025-06-04 19:00:50,768 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:00:50,792 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 19:00:57,564 - __main__ - INFO - \t MAE: 2.0013, MSE: 11.5784, RMSE: 3.4027, R^2: 0.9623\n",
      "[I 2025-06-04 19:00:57,644] Trial 20 finished with value: -1.955759870279758 and parameters: {'iterations': 1463, 'max_depth': 10, 'learning_rate': 0.012789553101793399, 'l2_leaf_reg': 7.579322090442729}. Best is trial 13 with value: -1.794862868753111.\n",
      "2025-06-04 19:00:57,712 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:00:57,735 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 19:00:58,952 - __main__ - INFO - \t MAE: 1.7318, MSE: 6.8291, RMSE: 2.6133, R^2: 0.9798\n",
      "2025-06-04 19:00:58,963 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:00:58,986 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 19:01:00,437 - __main__ - INFO - \t MAE: 1.7991, MSE: 7.7024, RMSE: 2.7753, R^2: 0.9749\n",
      "[I 2025-06-04 19:01:00,519] Trial 21 finished with value: -1.7654384852316882 and parameters: {'iterations': 1846, 'max_depth': 6, 'learning_rate': 0.10563030041245185, 'l2_leaf_reg': 0.559031037529238}. Best is trial 21 with value: -1.7654384852316882.\n",
      "2025-06-04 19:01:00,596 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:01:00,620 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 19:01:01,807 - __main__ - INFO - \t MAE: 1.8558, MSE: 9.7607, RMSE: 3.1242, R^2: 0.9711\n",
      "2025-06-04 19:01:01,820 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:01:01,844 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 19:01:03,587 - __main__ - INFO - \t MAE: 1.8636, MSE: 8.9189, RMSE: 2.9865, R^2: 0.9710\n",
      "[I 2025-06-04 19:01:03,667] Trial 22 finished with value: -1.8596880228864916 and parameters: {'iterations': 2627, 'max_depth': 7, 'learning_rate': 0.08626199119641416, 'l2_leaf_reg': 3.3008059604067346}. Best is trial 21 with value: -1.7654384852316882.\n",
      "2025-06-04 19:01:03,744 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:01:03,767 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 19:01:05,439 - __main__ - INFO - \t MAE: 1.7701, MSE: 7.2797, RMSE: 2.6981, R^2: 0.9785\n",
      "2025-06-04 19:01:05,451 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:01:05,476 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 19:01:07,186 - __main__ - INFO - \t MAE: 1.8654, MSE: 7.8121, RMSE: 2.7950, R^2: 0.9746\n",
      "[I 2025-06-04 19:01:07,261] Trial 23 finished with value: -1.817713853690131 and parameters: {'iterations': 2134, 'max_depth': 5, 'learning_rate': 0.04589960925141962, 'l2_leaf_reg': 6.976470729831255}. Best is trial 21 with value: -1.7654384852316882.\n",
      "2025-06-04 19:01:07,328 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:01:07,351 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 19:01:08,516 - __main__ - INFO - \t MAE: 1.8129, MSE: 7.0334, RMSE: 2.6520, R^2: 0.9792\n",
      "2025-06-04 19:01:08,526 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:01:08,550 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 19:01:09,857 - __main__ - INFO - \t MAE: 1.8629, MSE: 6.8019, RMSE: 2.6080, R^2: 0.9779\n",
      "[I 2025-06-04 19:01:09,935] Trial 24 finished with value: -1.837915000440388 and parameters: {'iterations': 885, 'max_depth': 3, 'learning_rate': 0.17441159401168885, 'l2_leaf_reg': 2.6484735128086414}. Best is trial 21 with value: -1.7654384852316882.\n",
      "2025-06-04 19:01:10,004 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:01:10,027 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 19:01:11,185 - __main__ - INFO - \t MAE: 1.7928, MSE: 7.5368, RMSE: 2.7453, R^2: 0.9777\n",
      "2025-06-04 19:01:11,196 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:01:11,220 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 19:01:12,711 - __main__ - INFO - \t MAE: 1.8147, MSE: 7.8930, RMSE: 2.8094, R^2: 0.9743\n",
      "[I 2025-06-04 19:01:12,785] Trial 25 finished with value: -1.8037644030059388 and parameters: {'iterations': 1622, 'max_depth': 6, 'learning_rate': 0.13995468113888979, 'l2_leaf_reg': 0.6614329518586488}. Best is trial 21 with value: -1.7654384852316882.\n",
      "2025-06-04 19:01:12,860 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:01:12,883 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 19:01:14,115 - __main__ - INFO - \t MAE: 2.0141, MSE: 16.2120, RMSE: 4.0264, R^2: 0.9520\n",
      "2025-06-04 19:01:14,128 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:01:14,152 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 19:01:15,726 - __main__ - INFO - \t MAE: 1.9762, MSE: 10.9445, RMSE: 3.3082, R^2: 0.9644\n",
      "[I 2025-06-04 19:01:15,822] Trial 26 finished with value: -1.9951518844031675 and parameters: {'iterations': 2075, 'max_depth': 8, 'learning_rate': 0.25248670510317445, 'l2_leaf_reg': 5.917595705029827}. Best is trial 21 with value: -1.7654384852316882.\n",
      "2025-06-04 19:01:15,899 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:01:15,923 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 19:01:17,226 - __main__ - INFO - \t MAE: 1.8833, MSE: 10.4856, RMSE: 3.2381, R^2: 0.9690\n",
      "2025-06-04 19:01:17,237 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:01:17,261 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 19:01:18,958 - __main__ - INFO - \t MAE: 1.9261, MSE: 9.8880, RMSE: 3.1445, R^2: 0.9678\n",
      "[I 2025-06-04 19:01:19,039] Trial 27 finished with value: -1.9046992485114025 and parameters: {'iterations': 1248, 'max_depth': 7, 'learning_rate': 0.08655262247468272, 'l2_leaf_reg': 8.829552011370001}. Best is trial 21 with value: -1.7654384852316882.\n",
      "2025-06-04 19:01:19,111 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:01:19,135 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 19:01:20,874 - __main__ - INFO - \t MAE: 1.7691, MSE: 6.5010, RMSE: 2.5497, R^2: 0.9808\n",
      "2025-06-04 19:01:20,885 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:01:20,909 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 19:01:23,063 - __main__ - INFO - \t MAE: 1.8106, MSE: 6.6194, RMSE: 2.5728, R^2: 0.9785\n",
      "[I 2025-06-04 19:01:23,153] Trial 28 finished with value: -1.7898293798653055 and parameters: {'iterations': 2591, 'max_depth': 3, 'learning_rate': 0.03604696968977692, 'l2_leaf_reg': 3.3577841363258556}. Best is trial 21 with value: -1.7654384852316882.\n",
      "2025-06-04 19:01:23,236 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:01:23,260 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 19:01:24,410 - __main__ - INFO - \t MAE: 1.8786, MSE: 7.5297, RMSE: 2.7440, R^2: 0.9777\n",
      "2025-06-04 19:01:24,422 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:01:24,446 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 19:01:25,844 - __main__ - INFO - \t MAE: 1.8596, MSE: 7.0057, RMSE: 2.6468, R^2: 0.9772\n",
      "[I 2025-06-04 19:01:25,917] Trial 29 finished with value: -1.8690965830681838 and parameters: {'iterations': 2677, 'max_depth': 3, 'learning_rate': 0.21137515383113398, 'l2_leaf_reg': 11.374626117123036}. Best is trial 21 with value: -1.7654384852316882.\n",
      "2025-06-04 19:01:25,928 - __main__ - INFO - Best hyperparameters for fold 5: {'iterations': 1846, 'max_depth': 6, 'learning_rate': 0.10563030041245185, 'l2_leaf_reg': 0.559031037529238}\n",
      "2025-06-04 19:01:25,937 - __main__ - INFO - Best score: -1.7654\n",
      "2025-06-04 19:01:25,937 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n",
      "2025-06-04 19:01:26,061 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-04 19:01:26,062 - __main__ - INFO - Retraining Model on Outer Fold 5\n",
      "2025-06-04 19:01:28,050 - __main__ - INFO - \t Inference Time: 0.00948 seconds\n",
      "2025-06-04 19:01:28,052 - __main__ - INFO - \t MAE: 1.8322, MSE: 6.8643, RMSE: 2.6200, R^2: 0.9829\n",
      "2025-06-04 19:01:28,055 - __main__ - INFO - Outer fold metrics saved to: data/outer_fold/teacher/hpo/197_catboost.csv\n",
      "2025-06-04 19:01:28,056 - __main__ - INFO - === FINAL RESULTS FOR DATASET 197 ===\n",
      "2025-06-04 19:01:28,056 - __main__ - INFO - MAE: 1.8592  0.1478\n",
      "2025-06-04 19:01:28,057 - __main__ - INFO - R2: 0.9795  0.0045\n",
      "2025-06-04 19:01:28,060 - __main__ - INFO - Summary statistics saved to: results/197_results.json\n",
      "2025-06-04 19:01:28,060 - __main__ - INFO - Fold indices file already exists: data/fold_indices/dataset_197.json\n",
      "2025-06-04 19:01:28,071 - __main__ - INFO - catboost outputs saved to: data/output/hpo/teacher/197_catboost.csv\n"
     ]
    }
   ],
   "source": [
    "# Get list of datasets to process from configuration\n",
    "datasets = config[\"data\"][\"datasets\"]\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINING LOOP - Process each dataset independently\n",
    "# =============================================================================\n",
    "for dataset_id in datasets:\n",
    "# -------------------------------------------------------------------------\n",
    "    # SETUP AND INITIALIZATION\n",
    "    # -------------------------------------------------------------------------\n",
    "    logger = setup_logging()\n",
    "    logger.info(\"Loading configuration...\")\n",
    "\n",
    "    # Extract model configuration for this run\n",
    "    model_type = config[\"model\"][\"teacher_model\"]\n",
    "    preprocessing_type = config[\"teacher_models\"][model_type][\"preprocessing\"]\n",
    "    use_hpo = config[\"training\"][\"use_hpo\"]\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 0: DATA LOADING\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Load dataset from OpenML with caching for efficiency\n",
    "    X, y, cat_cols, _, task_type = load_dataset(\n",
    "        dataset_id=dataset_id,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 1: INFRASTRUCTURE SETUP\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Note: Checking for existing results is placeholder for future implementation\n",
    "    summary_file = os.path.join(config[\"data\"][\"results_dir_path\"], f\"{dataset_id}_results.json\")\n",
    "    if os.path.exists(summary_file):\n",
    "        with open(summary_file, 'r') as f:\n",
    "            existing_results = json.load(f)\n",
    "        \n",
    "        # Check if we already have results with the same configuration\n",
    "        config_exists = False\n",
    "        for key, result in existing_results.items():\n",
    "            if (result.get('model_type') == model_type and \n",
    "                result.get('use_hpo') == use_hpo and \n",
    "                result.get('seed') == config[\"training\"][\"random_state\"]):\n",
    "                config_exists = True\n",
    "                logger.info(f\"Results already exist for dataset {dataset_id} with model {model_type}, HPO: {use_hpo}, seed: {config['training']['random_state']}\")\n",
    "                break\n",
    "        \n",
    "        if config_exists:\n",
    "            logger.info(f\"Skipping dataset {dataset_id} - results already computed\")\n",
    "            continue\n",
    "\n",
    "    # Configure GPU/CPU usage for training\n",
    "    device = check_GPU_availability()\n",
    "\n",
    "    # Set random seed for reproducibility across all libraries\n",
    "    set_seed(config[\"training\"][\"random_state\"])\n",
    "    logger.info(f\"Random seed set to {config['training']['random_state']}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 2: INITIALIZE DATA STRUCTURES\n",
    "    # -------------------------------------------------------------------------\n",
    "    # List to store predictions from each outer fold\n",
    "    output_dfs = []\n",
    "\n",
    "    outer_fold_scores = []\n",
    "    \n",
    "    # Dictionary to store all fold indices for reproducibility and student training\n",
    "    # Structure: {\"outer_folds\": {fold_id: {train_idx, test_idx}},\n",
    "    #            \"inner_folds\": {outer_fold_id: {inner_fold_id: {train_idx, val_idx}}}}\n",
    "    fold_indices = {\n",
    "        \"outer_folds\": {},\n",
    "        \"inner_folds\": {}\n",
    "    }\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 3: OUTER CROSS-VALIDATION SETUP\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Choose appropriate CV strategy based on task type to maintain class balance\n",
    "    if task_type == \"binary\":\n",
    "        outer_cv = StratifiedKFold(\n",
    "            n_splits=config[\"training\"][\"outer_folds\"],\n",
    "            shuffle=True,\n",
    "            random_state=config[\"training\"][\"random_state\"],\n",
    "        )\n",
    "    else:\n",
    "        outer_cv = KFold(\n",
    "            n_splits=config[\"training\"][\"outer_folds\"],\n",
    "            shuffle=True,\n",
    "            random_state=config[\"training\"][\"random_state\"],\n",
    "        )\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 4: OUTER CROSS-VALIDATION LOOP\n",
    "    # =========================================================================\n",
    "    # Each iteration provides one unbiased performance estimate\n",
    "    for outer_fold, (train_idx, test_idx) in enumerate(outer_cv.split(X, y), start=1):\n",
    "\n",
    "        logger.info(f\"-------------------- Outer Fold {outer_fold} --------------------\")\n",
    "        \n",
    "        # ---------------------------------------------------------------------\n",
    "        # FOLD INDEX MANAGEMENT\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Store outer fold indices for later use in student training and validation\n",
    "        fold_indices[\"outer_folds\"][f\"fold_{outer_fold}\"] = {\n",
    "            \"train_idx\": train_idx.tolist(),\n",
    "            \"test_idx\": test_idx.tolist()\n",
    "        }\n",
    "\n",
    "        # Split data according to current outer fold\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # INNER CROSS-VALIDATION SETUP (for model validation)\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Choose appropriate CV strategy for inner folds\n",
    "        if task_type == \"binary\":\n",
    "            inner_cv = StratifiedKFold(\n",
    "                n_splits=config[\"training\"][\"inner_folds\"],\n",
    "                shuffle=True,\n",
    "                random_state=config[\"training\"][\"random_state\"],\n",
    "            )\n",
    "        else:\n",
    "            inner_cv = KFold(\n",
    "                n_splits=config[\"training\"][\"inner_folds\"],\n",
    "                shuffle=True,\n",
    "                random_state=config[\"training\"][\"random_state\"],\n",
    "            )\n",
    "        \n",
    "        # Initialize storage for inner fold indices within this outer fold\n",
    "        fold_indices[\"inner_folds\"][f\"outer_fold_{outer_fold}\"] = {}\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 5: INNER CROSS-VALIDATION LOOP (hyperparameter validation) \n",
    "        # =====================================================================\n",
    "        def objective(trial):\n",
    "\n",
    "            hyperparams = suggest_hyperparameters(\n",
    "                trial=trial,\n",
    "                model_type=model_type,\n",
    "                task_type=task_type,\n",
    "                use_hpo=use_hpo,\n",
    "            )\n",
    "\n",
    "            inner_fold_scores = []\n",
    "            val_metrics_list = []\n",
    "\n",
    "            # This loop would typically be used for hyperparameter optimization\n",
    "            for inner_fold, (inner_train_index, inner_val_index) in enumerate(inner_cv.split(X_train, y_train), start=1):\n",
    "                \n",
    "                # -----------------------------------------------------------------\n",
    "                # INDEX MANAGEMENT (Critical for avoiding data leakage)\n",
    "                # -----------------------------------------------------------------\n",
    "                # Convert relative indices (within outer training set) to absolute indices\n",
    "                absolute_inner_train_idx = train_idx[inner_train_index]\n",
    "                absolute_inner_val_idx = train_idx[inner_val_index]\n",
    "                \n",
    "                # Store inner fold indices using absolute indices for consistency\n",
    "                # Only store indices for the first trial\n",
    "                if trial.number == 0:  \n",
    "                    fold_indices[\"inner_folds\"][f\"outer_fold_{outer_fold}\"][f\"inner_fold_{inner_fold}\"] = {\n",
    "                        \"train_idx\": absolute_inner_train_idx.tolist(),\n",
    "                        \"val_idx\": absolute_inner_val_idx.tolist()\n",
    "                    }\n",
    "\n",
    "                # Split inner training data using relative indices\n",
    "                X_inner_train, X_inner_val = X_train.iloc[inner_train_index], X_train.iloc[inner_val_index]\n",
    "                y_inner_train, y_inner_val = y_train[inner_train_index], y_train[inner_val_index]\n",
    "\n",
    "                # -----------------------------------------------------------------\n",
    "                # PREPROCESSING: Apply model-specific data transformations\n",
    "                # -----------------------------------------------------------------\n",
    "                X_inner_train, X_inner_val = preprocess(\n",
    "                    X_inner_train,\n",
    "                    y_inner_train,\n",
    "                    X_inner_val, \n",
    "                    cat_cols,\n",
    "                    config,\n",
    "                    preprocessing_type=preprocessing_type,\n",
    "                )\n",
    "\n",
    "                # -----------------------------------------------------------------\n",
    "                # MODEL TRAINING: Train teacher model on inner training data\n",
    "                # -----------------------------------------------------------------\n",
    "                model = get_teacher_model(config=config, task_type=task_type, device=device, hyperparams=hyperparams)\n",
    "                logger.info(f\"Training Model on Outer Fold {outer_fold}, Inner Fold {inner_fold}...\")\n",
    "                model.train(X_inner_train, y_inner_train)\n",
    "\n",
    "                # -----------------------------------------------------------------\n",
    "                # VALIDATION: Evaluate model performance on inner validation set\n",
    "                # -----------------------------------------------------------------\n",
    "                val_preds = model.predict(X_inner_val)\n",
    "                val_metrics = model.evaluate(val_preds, y_inner_val)\n",
    "\n",
    "                # Store metrics from this fold for later mean calculation\n",
    "                val_metrics_list.append(val_metrics)\n",
    "\n",
    "                if task_type == \"binary\":\n",
    "                    inner_fold_scores.append(val_metrics[\"f1\"])\n",
    "                    trial.report(np.mean(inner_fold_scores), step=inner_fold)\n",
    "                else:\n",
    "                    inner_fold_scores.append(-val_metrics[\"mae\"])\n",
    "                    trial.report(np.mean(inner_fold_scores), step=inner_fold)\n",
    "\n",
    "                # Check if the trial should be pruned (With 2 inner folds, pruning not recommended)\n",
    "                # if trial.should_prune():\n",
    "                #     logger.info(\"Trial pruned.\")\n",
    "                #     raise optuna.TrialPruned()\n",
    "\n",
    "            # Calculate and set mean metrics as user attributes after all inner folds\n",
    "            if val_metrics_list:\n",
    "                # Get all metric keys from the first fold\n",
    "                metric_keys = val_metrics_list[0].keys()\n",
    "                \n",
    "                for metric_key in metric_keys:\n",
    "                    # Calculate mean across all inner folds for this metric\n",
    "                    metric_values = [fold_metrics[metric_key] for fold_metrics in val_metrics_list]\n",
    "                    mean_metric = np.mean(metric_values)\n",
    "                    trial.set_user_attr(f\"mean_{metric_key}\", mean_metric)\n",
    "\n",
    "            return np.mean(inner_fold_scores)\n",
    "            \n",
    "        logger.info(f\"Starting hyperparameter optimization for outer fold {outer_fold}...\")\n",
    "\n",
    "        study_kwargs = dict(\n",
    "            direction=\"maximize\",\n",
    "            study_name=f\"{dataset_id}.{outer_fold}.{model_type}\",\n",
    "            load_if_exists=True,\n",
    "            sampler=optuna.samplers.TPESampler(seed=config[\"training\"][\"random_state\"]),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps=1),\n",
    "        )\n",
    "\n",
    "        if use_hpo:\n",
    "            os.makedirs(config[\"data\"][\"optuna_db_path\"], exist_ok=True)\n",
    "            study_kwargs[\"storage\"] = f\"sqlite:///{config['data']['optuna_db_path']}/optuna.db\"\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            **study_kwargs,\n",
    "        )\n",
    "\n",
    "        # Optimize\n",
    "        completed_trials = len(study.trials)\n",
    "        remaining_trials = config[\"training\"][\"trials\"] - completed_trials\n",
    "\n",
    "        if remaining_trials > 0:\n",
    "            default_hyperparams = suggest_hyperparameters(None, model_type, task_type, False)\n",
    "            study.enqueue_trial(default_hyperparams)\n",
    "            study.optimize(\n",
    "                objective, \n",
    "                n_trials=remaining_trials if use_hpo else 1,\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\"The study has already reached the maximum number of trials.\")\n",
    "\n",
    "        # Get best hyperparameters\n",
    "        best_hyperparams = study.best_params\n",
    "        logger.info(f\"Best hyperparameters for fold {outer_fold}: {best_hyperparams}\")\n",
    "        logger.info(f\"Best score: {study.best_value:.4f}\")\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 6: FINAL MODEL TRAINING (on complete outer training set)\n",
    "        # =====================================================================        \n",
    "        # Apply same preprocessing to outer training and test sets\n",
    "        X_train, X_test = preprocess(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            cat_cols,\n",
    "            config,\n",
    "            preprocessing_type=preprocessing_type,  # Use 'nn' for TabPFN preprocessing\n",
    "        )\n",
    "        \n",
    "        # Train final model on complete outer training set\n",
    "        logger.info(\"------------------------------------------------------\")\n",
    "        logger.info(f\"Retraining Model on Outer Fold {outer_fold}\")\n",
    "\n",
    "        model = get_teacher_model(\n",
    "            config=config,\n",
    "            task_type=task_type,\n",
    "            device=device,\n",
    "            hyperparams=best_hyperparams,  # Use best hyperparameters from HPO\n",
    "        )\n",
    "        model.train(X_train, y_train)\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 7: FINAL EVALUATION (unbiased performance on outer test set)\n",
    "        # =====================================================================\n",
    "        # This provides the unbiased performance estimate for this fold\n",
    "        start_time = time.time()\n",
    "        test_preds = model.predict(X_test)\n",
    "        end_time = time.time() - start_time\n",
    "        logger.info(f\"\\t Inference Time: {end_time:.5f} seconds\")\n",
    "        test_metrics = model.evaluate(test_preds, y_test)\n",
    "\n",
    "        # Store outer fold score for later analysis\n",
    "        outer_fold_results = {\n",
    "            \"fold\": outer_fold,\n",
    "            \"seed\": config[\"training\"][\"random_state\"],\n",
    "            \"inference_time\": end_time,\n",
    "            **test_metrics,\n",
    "        }\n",
    "        outer_fold_scores.append(outer_fold_results)\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 8: STORE PREDICTIONS FOR STUDENT TRAINING\n",
    "        # =====================================================================\n",
    "        # Save predictions with their corresponding dataset indices\n",
    "        # These will be used as targets for training student models\n",
    "        output_dfs.append(pd.DataFrame({\n",
    "            \"index\": test_idx,\n",
    "            \"output\": test_preds[:, 1] if task_type == \"binary\" else test_preds\n",
    "        }))\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 9: SAVE RESULTS AND METADATA\n",
    "    # =========================================================================\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # SAVE OUTER FOLD METRICS\n",
    "    # -------------------------------------------------------------------------\n",
    "    outer_fold_df = pd.DataFrame(outer_fold_scores)\n",
    "    sub_folder = \"hpo\" if use_hpo else \"default\"\n",
    "    output_dir = os.path.join(config[\"data\"][\"outer_folds_path\"], \"teacher\", sub_folder)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    metrics_file = os.path.join(output_dir, f\"{dataset_id}_{model_type}.csv\")\n",
    "    outer_fold_df.to_csv(metrics_file, index=False)\n",
    "    logger.info(f\"Outer fold metrics saved to: {metrics_file}\")\n",
    "\n",
    "    # Calculate and log overall performance across all folds\n",
    "    if task_type == \"binary\":\n",
    "        mean_acc = outer_fold_df['acc'].mean()\n",
    "        std_acc = outer_fold_df['acc'].std()\n",
    "        mean_f1 = outer_fold_df['f1'].mean()\n",
    "        std_f1 = outer_fold_df['f1'].std()\n",
    "        mean_roc = outer_fold_df['roc'].mean()\n",
    "        std_roc = outer_fold_df['roc'].std()\n",
    "        \n",
    "        logger.info(f\"=== FINAL RESULTS FOR DATASET {dataset_id} ===\")\n",
    "        logger.info(f\"Balanced Accuracy: {mean_acc:.4f}  {std_acc:.4f}\")\n",
    "        logger.info(f\"F1 Score: {mean_f1:.4f}  {std_f1:.4f}\")\n",
    "        logger.info(f\"ROC AUC: {mean_roc:.4f}  {std_roc:.4f}\")\n",
    "    else:\n",
    "        mean_mae = outer_fold_df['mae'].mean()\n",
    "        std_mae = outer_fold_df['mae'].std()\n",
    "        mean_mse = outer_fold_df['mse'].mean()\n",
    "        std_mse = outer_fold_df['mse'].std()\n",
    "        mean_rmse = outer_fold_df['rmse'].mean()\n",
    "        std_rmse = outer_fold_df['rmse'].std()\n",
    "        mean_r2 = outer_fold_df['r2'].mean()\n",
    "        std_r2 = outer_fold_df['r2'].std()\n",
    "        \n",
    "        logger.info(f\"=== FINAL RESULTS FOR DATASET {dataset_id} ===\")\n",
    "        logger.info(f\"MAE: {mean_mae:.4f}  {std_mae:.4f}\")\n",
    "        logger.info(f\"MSE: {mean_mse:.4f}  {std_mse:.4f}\")\n",
    "        logger.info(f\"RMSE: {mean_rmse:.4f}  {std_rmse:.4f}\")\n",
    "        logger.info(f\"R2: {mean_r2:.4f}  {std_r2:.4f}\")\n",
    "\n",
    "    # Save summary statistics as well\n",
    "    summary_stats = {\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"model_type\": model_type,\n",
    "        \"task_type\": task_type,\n",
    "        \"seed\": config[\"training\"][\"random_state\"],\n",
    "        \"use_hpo\": config[\"training\"][\"use_hpo\"],\n",
    "    }\n",
    "\n",
    "    if task_type == \"binary\":\n",
    "        summary_stats.update({\n",
    "            \"mean_acc\": mean_acc,\n",
    "            \"std_acc\": std_acc,\n",
    "            \"mean_f1\": mean_f1,\n",
    "            \"std_f1\": std_f1,\n",
    "            \"mean_roc\": mean_roc,\n",
    "            \"std_roc\": std_roc,\n",
    "        })\n",
    "    else:\n",
    "        summary_stats.update({\n",
    "            \"mean_mae\": mean_mae,\n",
    "            \"std_mae\": std_mae,\n",
    "            \"mean_mse\": mean_mse,\n",
    "            \"std_mse\": std_mse,\n",
    "            \"mean_rmse\": mean_rmse,\n",
    "            \"std_rmse\": std_rmse,\n",
    "            \"mean_r2\": mean_r2,\n",
    "            \"std_r2\": std_r2,\n",
    "        })\n",
    "\n",
    "    # Load existing summary file if it exists, otherwise create new\n",
    "    summary_file = os.path.join(config[\"data\"][\"results_dir_path\"], f\"{dataset_id}_results.json\")\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(config[\"data\"][\"results_dir_path\"], exist_ok=True)\n",
    "    if os.path.exists(summary_file):\n",
    "        with open(summary_file, 'r') as f:\n",
    "            all_results = json.load(f)\n",
    "    else:\n",
    "        all_results = {}\n",
    "\n",
    "    # Add current model results to the dataset summary\n",
    "    # Use simple incremental numbering\n",
    "    next_num = len(all_results) + 1\n",
    "    model_key = str(next_num)\n",
    "\n",
    "    # Add current model results to the dataset summary\n",
    "    all_results[model_key] = summary_stats\n",
    "\n",
    "    # Save updated summary\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    logger.info(f\"Summary statistics saved to: {summary_file}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # SAVE FOLD INDICES (for reproducibility and student training)\n",
    "    # -------------------------------------------------------------------------\n",
    "    fold_indices_file = os.path.join(config[\"data\"][\"fold_indices_path\"], f\"dataset_{dataset_id}.json\")\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(config[\"data\"][\"fold_indices_path\"], exist_ok=True)\n",
    "    if not os.path.exists(fold_indices_file):\n",
    "        with open(fold_indices_file, 'w') as f:\n",
    "            json.dump(fold_indices, f, indent=2)\n",
    "        logger.info(f\"Fold indices saved to: {fold_indices_file}\")\n",
    "    else:\n",
    "        logger.info(f\"Fold indices file already exists: {fold_indices_file}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # SAVE TEACHER PREDICTIONS (targets for student training)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Combine predictions from all outer folds\n",
    "    if output_dfs:  # Check if we have any DataFrames to concatenate\n",
    "        output_df = pd.concat(output_dfs, ignore_index=True)\n",
    "    else:\n",
    "        output_df = pd.DataFrame(columns=[\"index\", \"output\"])\n",
    "    output_df = output_df.sort_values(by=\"index\")\n",
    "    \n",
    "    # Create the full directory structure\n",
    "    sub_folder = \"hpo\" if use_hpo else \"default\"\n",
    "    output_dir = os.path.join(config[\"data\"][\"output_dir_path\"], sub_folder, \"teacher\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    output_file = os.path.join(output_dir, f\"{dataset_id}_{model_type}.csv\")\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "    logger.info(f\"{config['model']['teacher_model']} outputs saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c1e150",
   "metadata": {},
   "source": [
    "# Train Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1289fc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 18:26:50,330 - __main__ - INFO - Loading dataset 23381 from cache at 'data/cache/openml_dataset_23381.pkl'...\n",
      "2025-06-04 18:26:50,331 - __main__ - INFO - Dataset 23381 loaded successfully with task type: binary\n",
      "2025-06-04 18:26:50,332 - __main__ - INFO - Using GPU: NVIDIA RTX A6000\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/cache/dataset_23381_fold_indices.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load the saved fold indices \u001b[39;00m\n\u001b[1;32m     14\u001b[0m fold_indices_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir_path\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fold_indices.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfold_indices_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     16\u001b[0m     fold_indices \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded fold indices from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_indices_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/cache/dataset_23381_fold_indices.json'"
     ]
    }
   ],
   "source": [
    "datasets = config[\"data\"][\"datasets\"]\n",
    "\n",
    "for dataset_id in datasets:\n",
    "    X, y, cat_cols, _, task_type = load_dataset(\n",
    "        dataset_id=dataset_id,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    device = check_GPU_availability()\n",
    "\n",
    "    set_seed(config[\"training\"][\"random_state\"])\n",
    "\n",
    "    # Load the saved fold indices \n",
    "    fold_indices_file = os.path.join(config[\"data\"][\"cache_dir_path\"], f\"dataset_{dataset_id}_fold_indices.json\")\n",
    "    with open(fold_indices_file, 'r') as f:\n",
    "        fold_indices = json.load(f)\n",
    "    print(f\"Loaded fold indices from: {fold_indices_file}\") \n",
    "\n",
    "    # Load the TabPFN outputs (teacher predictions)\n",
    "    tabpfn_outputs_file = os.path.join(config[\"data\"][\"cache_dir_path\"], f\"dataset_{dataset_id}_tabpfn_outputs.csv\")\n",
    "    teacher_outputs_df = pd.read_csv(tabpfn_outputs_file)\n",
    "    print(f\"Loaded TabPFN outputs from: {tabpfn_outputs_file}\")\n",
    "\n",
    "    # Convert probabilities to logits\n",
    "    # Clip probabilities to avoid log(0) or log(1)\n",
    "    eps = 1e-7\n",
    "    teacher_probs = np.clip(teacher_outputs_df['output'].values, eps, 1 - eps)\n",
    "    teacher_logits = np.log(teacher_probs / (1 - teacher_probs))\n",
    "    \n",
    "    # Create a mapping from index to logits for easy lookup\n",
    "    index_to_logits = dict(zip(teacher_outputs_df['index'].values, teacher_logits))\n",
    "\n",
    "    output_df = pd.DataFrame(columns=[\"index\", \"output\"])\n",
    "\n",
    "    for fold_key, fold_data in fold_indices[\"outer_folds\"].items():\n",
    "        outer_fold = int(fold_key.split('_')[1])\n",
    "        train_idx = np.array(fold_data[\"train_idx\"])\n",
    "        test_idx = np.array(fold_data[\"test_idx\"])\n",
    "\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        inner_folds_data = fold_indices[\"inner_folds\"][f\"outer_fold_{outer_fold}\"]\n",
    "\n",
    "        for inner_fold_key, inner_fold_data in inner_folds_data.items():\n",
    "            inner_fold = int(inner_fold_key.split('_')[2])\n",
    "            \n",
    "            # Get absolute indices from saved data\n",
    "            absolute_inner_train_idx = np.array(inner_fold_data[\"train_idx\"])\n",
    "            absolute_inner_val_idx = np.array(inner_fold_data[\"val_idx\"])\n",
    "            \n",
    "            # Convert absolute indices to relative indices for the current outer training set\n",
    "            inner_train_relative = np.where(np.isin(train_idx, absolute_inner_train_idx))[0]\n",
    "            inner_val_relative = np.where(np.isin(train_idx, absolute_inner_val_idx))[0]\n",
    "\n",
    "            X_inner_train, X_inner_val = X_train.iloc[inner_train_relative], X_train.iloc[inner_val_relative]\n",
    "            y_inner_train, y_inner_val = y_train[inner_train_relative], y_train[inner_val_relative]\n",
    "\n",
    "            # Get teacher logits for inner training and validation sets\n",
    "            teacher_logits_inner_train = np.array([index_to_logits[idx] for idx in absolute_inner_train_idx])\n",
    "            teacher_logits_inner_val = np.array([index_to_logits[idx] for idx in absolute_inner_val_idx])\n",
    "\n",
    "            # Preprocess the data for tree-based model\n",
    "            X_inner_train, X_inner_val = preprocess(\n",
    "                X_inner_train,\n",
    "                y_inner_train,\n",
    "                X_inner_val, \n",
    "                cat_cols,\n",
    "                config,\n",
    "                preprocessing_type=\"tree\",\n",
    "            )\n",
    "\n",
    "            # Train CatBoost regressor to predict teacher logits\n",
    "            catboost = CatBoostRegressor(verbose=False)\n",
    "            catboost.fit(X_inner_train, teacher_logits_inner_train)\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            val_pred_logits = catboost.predict(X_inner_val)\n",
    "            # Convert predicted logits back to probabilities for evaluation\n",
    "            val_probs = 1 / (1 + np.exp(-val_pred_logits))\n",
    "            val_preds = (val_probs > 0.5).astype(int)\n",
    "            \n",
    "            val_acc = balanced_accuracy_score(y_inner_val, val_preds)\n",
    "            val_f1 = f1_score(y_inner_val, val_preds, average=\"macro\")\n",
    "            val_roc_auc = roc_auc_score(y_inner_val, val_probs)\n",
    "            print(f\"Outer Fold {outer_fold}, Inner Fold {inner_fold} - Accuracy: {val_acc:.4f}, F1 Score: {val_f1:.4f}, ROC AUC: {val_roc_auc:.4f}\")\n",
    "\n",
    "        # Get teacher logits for outer training set\n",
    "        teacher_logits_train = np.array([index_to_logits[idx] for idx in train_idx])\n",
    "\n",
    "        # Preprocess the outer training and test sets\n",
    "        X_train, X_test = preprocess(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            cat_cols,\n",
    "            config,\n",
    "            preprocessing_type=\"tree\",\n",
    "        )\n",
    "        \n",
    "        # Retrain CatBoost regressor on the full training set for the outer fold\n",
    "        catboost = CatBoostRegressor(verbose=False)\n",
    "        catboost.fit(X_train, teacher_logits_train)\n",
    "\n",
    "        # Evaluate on the outer test set\n",
    "        test_pred_logits = catboost.predict(X_test)\n",
    "        # Convert predicted logits back to probabilities for evaluation\n",
    "        test_probs = 1 / (1 + np.exp(-test_pred_logits))\n",
    "        test_preds = (test_probs > 0.5).astype(int)\n",
    "        \n",
    "        test_acc = balanced_accuracy_score(y_test, test_preds)\n",
    "        test_f1 = f1_score(y_test, test_preds, average=\"macro\")\n",
    "        test_roc_auc = roc_auc_score(y_test, test_probs)\n",
    "        print(f\"Outer Fold {outer_fold} - Test Accuracy: {test_acc:.4f}, Test F1 Score: {test_f1:.4f}, Test ROC AUC: {test_roc_auc:.4f}\")\n",
    "        \n",
    "        # Save student probabilities with indices\n",
    "        output_df = pd.concat(\n",
    "            [\n",
    "                output_df,\n",
    "                pd.DataFrame({\n",
    "                    \"index\": test_idx,\n",
    "                    \"output\": test_probs  # Student probabilities (converted from predicted logits)\n",
    "                })\n",
    "            ],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "    # Export the output DataFrame to a CSV file\n",
    "    output_df = output_df.sort_values(by=\"index\")\n",
    "    output_file = os.path.join(config[\"data\"][\"cache_dir_path\"], f\"dataset_{dataset_id}_catboost_outputs_student.csv\")\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "    print(f\"CatBoost student outputs saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23cc6db",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b141aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Validating Fold Indices ===\n",
      "Total samples in dataset: 500\n",
      "Total test samples across all folds: 500\n",
      "Unique test samples: 500\n",
      " All samples appear exactly once in test sets across folds\n",
      " Teacher outputs cover all dataset indices\n",
      " Fold 1: No overlap between train/test\n",
      " Fold 1, Inner fold inner_fold_1: Indices are subset of outer train\n",
      " Fold 1, Inner fold inner_fold_2: Indices are subset of outer train\n",
      " Fold 1: Inner folds cover all outer training data\n",
      " Fold 2: No overlap between train/test\n",
      " Fold 2, Inner fold inner_fold_1: Indices are subset of outer train\n",
      " Fold 2, Inner fold inner_fold_2: Indices are subset of outer train\n",
      " Fold 2: Inner folds cover all outer training data\n",
      " Fold 3: No overlap between train/test\n",
      " Fold 3, Inner fold inner_fold_1: Indices are subset of outer train\n",
      " Fold 3, Inner fold inner_fold_2: Indices are subset of outer train\n",
      " Fold 3: Inner folds cover all outer training data\n",
      " Fold 4: No overlap between train/test\n",
      " Fold 4, Inner fold inner_fold_1: Indices are subset of outer train\n",
      " Fold 4, Inner fold inner_fold_2: Indices are subset of outer train\n",
      " Fold 4: Inner folds cover all outer training data\n",
      " Fold 5: No overlap between train/test\n",
      " Fold 5, Inner fold inner_fold_1: Indices are subset of outer train\n",
      " Fold 5, Inner fold inner_fold_2: Indices are subset of outer train\n",
      " Fold 5: Inner folds cover all outer training data\n",
      "=== End Validation ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add this validation code after loading the fold indices and before the main loop\n",
    "print(\"=== Validating Fold Indices ===\")\n",
    "\n",
    "# 1. Check that all indices are within valid range\n",
    "total_samples = len(X)\n",
    "print(f\"Total samples in dataset: {total_samples}\")\n",
    "\n",
    "# 2. Collect all test indices across outer folds\n",
    "all_test_indices = []\n",
    "for fold_key, fold_data in fold_indices[\"outer_folds\"].items():\n",
    "    test_idx = np.array(fold_data[\"test_idx\"])\n",
    "    all_test_indices.extend(test_idx)\n",
    "\n",
    "all_test_indices = np.array(all_test_indices)\n",
    "print(f\"Total test samples across all folds: {len(all_test_indices)}\")\n",
    "print(f\"Unique test samples: {len(np.unique(all_test_indices))}\")\n",
    "\n",
    "# 3. Check if all samples appear exactly once in test sets\n",
    "if len(all_test_indices) == len(np.unique(all_test_indices)) == total_samples:\n",
    "    print(\" All samples appear exactly once in test sets across folds\")\n",
    "else:\n",
    "    print(\" Issue with test set coverage!\")\n",
    "\n",
    "# 4. Check teacher outputs coverage\n",
    "teacher_indices = set(teacher_outputs_df['index'].values)\n",
    "dataset_indices = set(range(total_samples))\n",
    "if teacher_indices == dataset_indices:\n",
    "    print(\" Teacher outputs cover all dataset indices\")\n",
    "else:\n",
    "    missing = dataset_indices - teacher_indices\n",
    "    extra = teacher_indices - dataset_indices\n",
    "    print(f\" Teacher outputs mismatch - Missing: {missing}, Extra: {extra}\")\n",
    "\n",
    "# 5. Validate fold structure\n",
    "for fold_key, fold_data in fold_indices[\"outer_folds\"].items():\n",
    "    outer_fold = int(fold_key.split('_')[1])\n",
    "    train_idx = np.array(fold_data[\"train_idx\"])\n",
    "    test_idx = np.array(fold_data[\"test_idx\"])\n",
    "    \n",
    "    # Check no overlap between train and test\n",
    "    overlap = np.intersect1d(train_idx, test_idx)\n",
    "    if len(overlap) == 0:\n",
    "        print(f\" Fold {outer_fold}: No overlap between train/test\")\n",
    "    else:\n",
    "        print(f\" Fold {outer_fold}: Found {len(overlap)} overlapping indices!\")\n",
    "    \n",
    "    # Check inner folds\n",
    "    inner_folds_data = fold_indices[\"inner_folds\"][f\"outer_fold_{outer_fold}\"]\n",
    "    inner_train_indices = []\n",
    "    inner_val_indices = []\n",
    "    \n",
    "    for inner_fold_key, inner_fold_data in inner_folds_data.items():\n",
    "        inner_train_idx = np.array(inner_fold_data[\"train_idx\"])\n",
    "        inner_val_idx = np.array(inner_fold_data[\"val_idx\"])\n",
    "        \n",
    "        # Check inner indices are subset of outer train indices\n",
    "        if np.all(np.isin(inner_train_idx, train_idx)) and np.all(np.isin(inner_val_idx, train_idx)):\n",
    "            print(f\" Fold {outer_fold}, Inner fold {inner_fold_key}: Indices are subset of outer train\")\n",
    "        else:\n",
    "            print(f\" Fold {outer_fold}, Inner fold {inner_fold_key}: Indices not subset of outer train!\")\n",
    "        \n",
    "        inner_train_indices.extend(inner_train_idx)\n",
    "        inner_val_indices.extend(inner_val_idx)\n",
    "    \n",
    "    # Check inner folds cover all outer training data\n",
    "    inner_all = np.unique(np.concatenate([inner_train_indices, inner_val_indices]))\n",
    "    if len(np.setdiff1d(train_idx, inner_all)) == 0:\n",
    "        print(f\" Fold {outer_fold}: Inner folds cover all outer training data\")\n",
    "    else:\n",
    "        missing_in_inner = np.setdiff1d(train_idx, inner_all)\n",
    "        print(f\" Fold {outer_fold}: {len(missing_in_inner)} samples missing from inner folds\")\n",
    "\n",
    "print(\"=== End Validation ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc9aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validating Teacher Logits for Fold 5 ===\n",
      " All 400 training indices have teacher outputs\n",
      " WARNING: 100 test indices found in teacher outputs - possible data leakage!\n",
      "Sample teacher logits: [-0.5883296581435397, -0.79494873035179, -0.49716914708297155, -0.6096161248627843, -0.4098342731792005]\n",
      "Converted to probabilities: [0.3570182  0.31110707 0.37820616 0.35214677 0.39895186]\n",
      "=== End Teacher Logits Validation ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add this inside the main loop, after creating index_to_logits mapping\n",
    "print(f\"\\n=== Validating Teacher Logits for Fold {outer_fold} ===\")\n",
    "\n",
    "# Check if all training indices have corresponding teacher outputs\n",
    "missing_teacher_outputs = []\n",
    "for idx in train_idx:\n",
    "    if idx not in index_to_logits:\n",
    "        missing_teacher_outputs.append(idx)\n",
    "\n",
    "if len(missing_teacher_outputs) == 0:\n",
    "    print(f\" All {len(train_idx)} training indices have teacher outputs\")\n",
    "else:\n",
    "    print(f\" Missing teacher outputs for {len(missing_teacher_outputs)} indices: {missing_teacher_outputs[:10]}...\")\n",
    "\n",
    "# Check if test indices have teacher outputs (they shouldn't for proper validation)\n",
    "test_in_teacher = []\n",
    "for idx in test_idx:\n",
    "    if idx in index_to_logits:\n",
    "        test_in_teacher.append(idx)\n",
    "\n",
    "if len(test_in_teacher) == 0:\n",
    "    print(f\" No test indices found in teacher outputs (proper data leakage prevention)\")\n",
    "else:\n",
    "    print(f\" WARNING: {len(test_in_teacher)} test indices found in teacher outputs - possible data leakage!\")\n",
    "\n",
    "# Sample a few logits to check they're reasonable\n",
    "sample_indices = train_idx[:5]\n",
    "sample_logits = [index_to_logits[idx] for idx in sample_indices]\n",
    "sample_probs = 1 / (1 + np.exp(-np.array(sample_logits)))\n",
    "print(f\"Sample teacher logits: {sample_logits}\")\n",
    "print(f\"Converted to probabilities: {sample_probs}\")\n",
    "print(\"=== End Teacher Logits Validation ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b55ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Validation ===\n",
      " Student output file created with 500 predictions\n",
      " Student predictions cover exactly the expected test indices\n",
      " All student outputs are valid probabilities [0,1]\n",
      "  Range: [0.2589, 0.7412]\n",
      "  Mean: 0.4127\n",
      "=== End Final Validation ===\n"
     ]
    }
   ],
   "source": [
    "# Add this at the very end to verify your student outputs match expectations\n",
    "print(\"=== Final Validation ===\")\n",
    "\n",
    "# Check output file was created and has expected structure\n",
    "if os.path.exists(output_file):\n",
    "    student_df = pd.read_csv(output_file)\n",
    "    print(f\" Student output file created with {len(student_df)} predictions\")\n",
    "    \n",
    "    # Check all test indices are covered\n",
    "    predicted_indices = set(student_df['index'].values)\n",
    "    expected_test_indices = set(all_test_indices)\n",
    "    \n",
    "    if predicted_indices == expected_test_indices:\n",
    "        print(\" Student predictions cover exactly the expected test indices\")\n",
    "    else:\n",
    "        missing = expected_test_indices - predicted_indices\n",
    "        extra = predicted_indices - expected_test_indices\n",
    "        print(f\" Prediction coverage mismatch - Missing: {len(missing)}, Extra: {len(extra)}\")\n",
    "    \n",
    "    # Check output values are reasonable probabilities\n",
    "    outputs = student_df['output'].values\n",
    "    if np.all((outputs >= 0) & (outputs <= 1)):\n",
    "        print(f\" All student outputs are valid probabilities [0,1]\")\n",
    "        print(f\"  Range: [{outputs.min():.4f}, {outputs.max():.4f}]\")\n",
    "        print(f\"  Mean: {outputs.mean():.4f}\")\n",
    "    else:\n",
    "        invalid_count = np.sum((outputs < 0) | (outputs > 1))\n",
    "        print(f\" {invalid_count} invalid probability values found!\")\n",
    "        \n",
    "else:\n",
    "    print(\" Student output file was not created!\")\n",
    "\n",
    "print(\"=== End Final Validation ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
