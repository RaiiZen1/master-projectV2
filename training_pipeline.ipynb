{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eaf3d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:33:25.437384: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-06 11:33:26.852015: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/mherre/.local/lib/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "/home/mherre/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, accuracy_score, f1_score, roc_auc_score,\n",
    "    mean_absolute_error, mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, OneHotEncoder, LabelEncoder, TargetEncoder\n",
    ")\n",
    "\n",
    "# Deep learning imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "\n",
    "# Model-specific imports\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from tabpfn import TabPFNClassifier, TabPFNRegressor\n",
    "from GRANDE import GRANDE\n",
    "from packages.tabm_reference import Model, make_parameter_groups\n",
    "\n",
    "# Optimization imports\n",
    "import optuna\n",
    "\n",
    "# Data source imports\n",
    "import openml\n",
    "\n",
    "# Abstract base class imports\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Literal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae3463",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf7565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    Set random seeds for reproducibility across Python, NumPy, PyTorch, and TensorFlow.\n",
    "\n",
    "    Args:\n",
    "        seed: Integer seed for random number generators\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "def setup_logging() -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Set up logging configuration.\n",
    "    This function configures the logging settings for the application, including\n",
    "    the logging level and format. It returns a logger instance that can be used\n",
    "    throughout the application.\n",
    "    Returns:\n",
    "        logging.Logger: Configured logger instance.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def check_GPU_availability():\n",
    "    \"\"\"\n",
    "    Checks if a GPU is available and configures TensorFlow to use it appropriately.\n",
    "\n",
    "    Sets up memory growth for TensorFlow GPU usage to avoid allocating all GPU memory at once.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: A torch device object ('cuda' if GPU is available, 'cpu' otherwise)\n",
    "    \"\"\"\n",
    "    logger = setup_logging()\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"Using GPU: {torch.cuda.get_device_name()}\")\n",
    "        device = torch.device(\"cuda\")\n",
    "        gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        logger.info(\"Using CPU for training.\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14164da",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3635ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target(y):\n",
    "    \"\"\"\n",
    "    Encode the target variable using LabelEncoder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array\n",
    "        The target variable to encode.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_encoded : np.array\n",
    "        The encoded target variable.\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    return y_encoded\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_val: pd.DataFrame,\n",
    "    cat_cols: list,\n",
    "    config: dict,\n",
    "    preprocessing_type: str,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Preprocess the training and validation datasets based on the specified model type.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : pandas.DataFrame\n",
    "        The training dataset features.\n",
    "    y_train : pandas.Series or np.array\n",
    "        The target variable for the training dataset.\n",
    "    X_val : pandas.DataFrame\n",
    "        The validation dataset features.\n",
    "    cat_cols : list or array of bool\n",
    "        Boolean mask indicating which columns in X_train and X_val are categorical (True).\n",
    "    config : dict\n",
    "        Configuration dictionary containing model and preprocessing details.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train : np.array\n",
    "        The preprocessed training dataset features.\n",
    "    X_val : np.array\n",
    "        The preprocessed validation dataset features.\n",
    "    \"\"\"\n",
    "    X_train, preprocessor_inner = _minimal_preprocess_train(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cat_cols,\n",
    "        preprocessing_type,\n",
    "        config,\n",
    "    )\n",
    "    X_val = _minimal_preprocess_test(X_val, preprocessor_inner)\n",
    "\n",
    "    return X_train, X_val\n",
    "\n",
    "\n",
    "def _minimal_preprocess_train(X, y, categorical_features, model_type, config):\n",
    "    \"\"\"\n",
    "    Perform minimal preprocessing on the input features for training.\n",
    "\n",
    "    This function applies preprocessing to both numeric and categorical features\n",
    "    based on the model type specified in the configuration. For Neural Networks,\n",
    "    it standardizes numeric features and one-hot encodes categorical features. For\n",
    "    tree-based models, it does not scale numeric features and applies different\n",
    "    encoding strategies for low- and high-cardinality categorical features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pandas.DataFrame\n",
    "        The input features.\n",
    "\n",
    "    categorical_features : list or array of bool\n",
    "        Boolean mask indicating which columns in X are categorical (True) and\n",
    "        which are numeric (False).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_preprocessed : np.array\n",
    "        The preprocessed features.\n",
    "    preprocessor : ColumnTransformer\n",
    "        The fitted preprocessor, which can be used to transform future datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the boolean mask to a NumPy array (if not already) for fast indexing.\n",
    "    cat_mask = np.asarray(categorical_features)\n",
    "\n",
    "    # Check that the mask length matches the number of columns in X.\n",
    "    if cat_mask.shape[0] != X.shape[1]:\n",
    "        raise ValueError(\n",
    "            \"Length of categorical_features mask must match the number of columns in X.\"\n",
    "        )\n",
    "\n",
    "    # Extract column names using boolean indexing.\n",
    "    categorical_feature_names = X.columns[cat_mask].tolist()\n",
    "    numeric_feature_names = X.columns[~cat_mask].tolist()\n",
    "\n",
    "    # Select pipeline based on config.\n",
    "    logger = setup_logging()\n",
    "\n",
    "    # Neural Networks: standardize numeric features and one-hot encode categoricals.\n",
    "    if model_type == \"nn\":\n",
    "\n",
    "        # Define the preprocessing pipeline for numeric features.\n",
    "        numeric_pipeline = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "                (\"scaler\", StandardScaler()),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Define the preprocessing pipeline for categorical features.\n",
    "        categorical_pipeline = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\n",
    "                    \"onehot\",\n",
    "                    OneHotEncoder(\n",
    "                        drop=\"first\", sparse_output=False, handle_unknown=\"ignore\"\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        transformers = [\n",
    "            (\"num\", numeric_pipeline, numeric_feature_names),\n",
    "            (\"cat\", categorical_pipeline, categorical_feature_names),\n",
    "        ]\n",
    "\n",
    "        logger.info(\"Preprocessing pipeline for Neural Network model.\")\n",
    "\n",
    "    elif model_type == \"tree\":\n",
    "\n",
    "        # Tree-based models: leave numeric features unscaled, and differentiate encoding for categoricals.\n",
    "        numeric_pipeline = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            ]\n",
    "        )\n",
    "        # Split categorical features into low- and high-cardinality groups.\n",
    "        threshold = config[\"preprocessing\"][\"threshold_high_cardinality\"]\n",
    "        low_card_features = []\n",
    "        high_card_features = []\n",
    "        for feature in categorical_feature_names:\n",
    "            if X[feature].nunique() <= threshold:\n",
    "                low_card_features.append(feature)\n",
    "            else:\n",
    "                high_card_features.append(feature)\n",
    "\n",
    "        # For low-cardinality categoricals, use one-hot encoding.\n",
    "        low_card_pipeline = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\n",
    "                    \"onehot\",\n",
    "                    OneHotEncoder(\n",
    "                        drop=\"first\", sparse_output=False, handle_unknown=\"ignore\"\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # For high-cardinality categoricals, use target encoding.\n",
    "        high_card_pipeline = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"target_encode\", TargetEncoder()),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        transformers = [\n",
    "            (\"num\", numeric_pipeline, numeric_feature_names),\n",
    "            (\"low_card\", low_card_pipeline, low_card_features),\n",
    "            (\"high_card\", high_card_pipeline, high_card_features),\n",
    "        ]\n",
    "\n",
    "        logger.info(\"Preprocessing pipeline for Decision Tree model.\")\n",
    "\n",
    "    elif model_type == \"grande\":\n",
    "        logger.info(\"No preprocessing required for Grande model.\")\n",
    "        return X, None\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Unknown teacher type. Must be 'nn' for Neural Network, 'tree' for Decision Tree, or 'grande' for Gradient Based Tree.\"\n",
    "        )\n",
    "\n",
    "    # Create a ColumnTransformer to apply the transformations to the appropriate columns.\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        n_jobs=-2,\n",
    "    )\n",
    "\n",
    "    # Fit the preprocessor on the training data and transform it.\n",
    "    X_preprocessed = preprocessor.fit_transform(X, y)\n",
    "\n",
    "    return X_preprocessed, preprocessor\n",
    "\n",
    "\n",
    "def _minimal_preprocess_test(X, preprocessor):\n",
    "    \"\"\"\n",
    "    Transform the input features according to the preprocessor fitted on the training data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pandas.DataFrame\n",
    "        The input features.\n",
    "    preprocessor : ColumnTransformer\n",
    "        The preprocessor fitted on the training data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_preprocessed : np.array\n",
    "        The preprocessed features.\n",
    "    \"\"\"\n",
    "    if preprocessor is None:\n",
    "        return X\n",
    "    return preprocessor.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d700d2",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f642a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_id: int, config: dict):\n",
    "    \"\"\"\n",
    "    Loads an OpenML dataset based on its ID and processes it according to the task type.\n",
    "\n",
    "    This function fetches the dataset using the _fetch_dataset helper, then processes\n",
    "    the target variable based on whether the task is binary classification or regression.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_id (int): The ID of the dataset on OpenML.\n",
    "        config (dict): Configuration dictionary containing data paths and settings.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - X: The feature data (pandas DataFrame)\n",
    "            - y: The processed target variable (encoded for binary classification or numpy array for regression)\n",
    "            - cat_cols: List of booleans indicating which features are categorical\n",
    "            - attribute_names: List of attribute names\n",
    "            - task_type: String indicating the task type ('binary' or 'regression')\n",
    "    \"\"\"\n",
    "    X, y, cat_cols, attribute_names, task_type = fetch_dataset(\n",
    "        dataset_id=dataset_id,\n",
    "        cache_dir=config[\"data\"][\"cache_dir_path\"],\n",
    "    )\n",
    "    if task_type == \"binary\":\n",
    "        # Encode the target variable if it's binary\n",
    "        y = encode_target(y)\n",
    "    else:\n",
    "        # Transform to np.array for regression\n",
    "        y = np.array(y, dtype=float)\n",
    "\n",
    "    return X, y, cat_cols, attribute_names, task_type\n",
    "\n",
    "\n",
    "def fetch_dataset(dataset_id: int, cache_dir: str = \"data/cache/\"):\n",
    "    \"\"\"\n",
    "    Downloads an OpenML dataset based on its id, caches the data locally, and returns the data\n",
    "    along with its metadata.\n",
    "\n",
    "    The five returned objects are:\n",
    "        - X: The feature data (typically a pandas DataFrame).\n",
    "        - y: The target variable.\n",
    "        - categorical_indicator: A list of booleans indicating which features are categorical.\n",
    "        - attribute_names: A list of attribute names.\n",
    "        - task_type: A string indicating the type of task ('binary' or 'regression').\n",
    "\n",
    "    If the dataset has been previously downloaded and stored in the cache directory,\n",
    "    it will be loaded from the local file instead of re-downloading.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_id (int): The id of the dataset on OpenML.\n",
    "        cache_dir (str): The directory to store the downloaded dataset. Defaults to \"openml_cache\".\n",
    "\n",
    "    Returns:\n",
    "        X, y, categorical_indicator, attribute_names, task_type\n",
    "    \"\"\"\n",
    "    logger = setup_logging()\n",
    "\n",
    "    # Ensure the cache directory exists\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    # Define a cache file name that is unique to the dataset id\n",
    "    cache_file = os.path.join(cache_dir, f\"openml_dataset_{dataset_id}.pkl\")\n",
    "\n",
    "    # If the cache file exists, load the data from the file.\n",
    "    if os.path.exists(cache_file):\n",
    "        logger.info(\n",
    "            f\"Loading dataset {dataset_id} from cache at '{cache_file}'...\"\n",
    "        )\n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        X = data[\"X\"]\n",
    "        y = data[\"y\"]\n",
    "        categorical_indicator = data[\"categorical_indicator\"]\n",
    "        attribute_names = data[\"attribute_names\"]\n",
    "        task_type = data[\"task_type\"]\n",
    "    else:\n",
    "        # Download the dataset from OpenML.\n",
    "        logger.info(f\"Downloading dataset {dataset_id} from OpenML...\")\n",
    "        dataset = openml.datasets.get_dataset(dataset_id)\n",
    "\n",
    "        # Use the default target attribute (if defined) when retrieving the data.\n",
    "        X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "            target=dataset.default_target_attribute, dataset_format=\"dataframe\"\n",
    "        )\n",
    "\n",
    "        # Determine the task type (binary classification or regression)\n",
    "        task_type = _determine_task_type(y)\n",
    "\n",
    "        # Store the data and metadata in a dictionary.\n",
    "        data = {\n",
    "            \"X\": X,\n",
    "            \"y\": y,\n",
    "            \"categorical_indicator\": categorical_indicator,\n",
    "            \"attribute_names\": attribute_names,\n",
    "            \"task_type\": task_type,\n",
    "        }\n",
    "\n",
    "        # Save the dictionary to a local file using pickle.\n",
    "        with open(cache_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        logger.info(f\"Dataset {dataset_id} stored locally at '{cache_file}'.\")\n",
    "\n",
    "    logger.info(f\"Dataset {dataset_id} loaded successfully with task type: {task_type}\")\n",
    "\n",
    "    return X, y, categorical_indicator, attribute_names, task_type\n",
    "\n",
    "\n",
    "def _determine_task_type(y):\n",
    "    \"\"\"\n",
    "    Determines if the target variable is for binary classification, or regression.\n",
    "\n",
    "    Parameters:\n",
    "        y: The target variable (pandas dataframe).\n",
    "\n",
    "    Returns:\n",
    "        str: 'binary', or 'regression'\n",
    "    \"\"\"\n",
    "\n",
    "    # Get unique values\n",
    "    unique_values = pd.unique(y)\n",
    "\n",
    "    # Check if it's binary (2 unique values)\n",
    "    if len(unique_values) == 2:\n",
    "        return \"binary\"\n",
    "    else:\n",
    "        return \"regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4579812",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee7681de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(y_prob, y_true):\n",
    "    \"\"\"\n",
    "    Evaluate classification performance using balanced accuracy, F1 score, and ROC AUC.\n",
    "\n",
    "    Parameters:\n",
    "        y_prob (np.array): Predicted probabilities for the positive class.\n",
    "        y_true (np.array): True labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    logger = setup_logging()\n",
    "    y_pred = (y_prob[:, 1] > 0.5).astype(int)\n",
    "    acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    roc_auc = roc_auc_score(y_true, y_prob[:, 1])\n",
    "\n",
    "    logger.info(f\"\\t Balanced Accuracy: {acc:.4f}, F1 Score: {f1:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"roc\": roc_auc,\n",
    "    }\n",
    "\n",
    "def evaluate_regression(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Evaluate regression performance using R^2 score.\n",
    "\n",
    "    Parameters:\n",
    "        y_pred (np.array): Predicted values.\n",
    "        y_true (np.array): True values.\n",
    "\n",
    "    Returns:\n",
    "        float: R^2 score.\n",
    "    \"\"\"\n",
    "    logger = setup_logging()\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    logger.info(f\"\\t MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R^2: {r2:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"mae\": mae,\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\": r2,\n",
    "    }\n",
    "\n",
    "def evaluate_fidelity_classification(y_prob_student, y_prob_teacher):\n",
    "    \"\"\"\n",
    "    Evaluate the fidelity of a student model compared to a teacher model in classification tasks.\n",
    "\n",
    "    Parameters:\n",
    "        y_prob_teacher (np.array): Predicted probabilities from the teacher model.\n",
    "        y_prob_student (np.array): Predicted probabilities from the student model.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics for fidelity.\n",
    "    \"\"\"\n",
    "    logger = setup_logging()\n",
    "\n",
    "    y_pred_student = (y_prob_student[:, 1] > 0.5).astype(int)\n",
    "\n",
    "    if y_prob_teacher.ndim == 2:\n",
    "        y_pred_teacher = (y_prob_teacher[:, 1] > 0.5).astype(int)\n",
    "        kl_div = kl_divergence_soft(y_prob_student[:, 1], y_prob_teacher[:, 1])\n",
    "\n",
    "    elif y_prob_teacher.ndim == 1:\n",
    "        y_pred_teacher = y_prob_teacher.astype(int)\n",
    "        kl_div = kl_divergence_hard(y_prob_student[:, 1], y_pred_teacher)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"y_prob_teacher has unexpected shape: {y_prob_teacher.shape}\"\n",
    "        )\n",
    "    \n",
    "    acc = balanced_accuracy_score(y_pred_teacher, y_pred_student)\n",
    "    f1 = f1_score(y_pred_teacher, y_pred_student, average=\"macro\")\n",
    "    roc_auc = roc_auc_score(y_pred_teacher, y_prob_student[:, 1])\n",
    "\n",
    "    logger.info(f\"\\t Fidelity - Balanced Accuracy: {acc:.4f}, F1 Score: {f1:.4f}, ROC AUC: {roc_auc:.4f}, KL Divergence: {kl_div:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"fidelity_acc\": acc,\n",
    "        \"fidelity_f1\": f1,\n",
    "        \"fidelity_roc\": roc_auc,\n",
    "        \"fidelity_kl_div\": kl_div,\n",
    "    }\n",
    "\n",
    "def evaluate_fidelity_regression(y_pred_student, y_pred_teacher):\n",
    "    \"\"\"\n",
    "    Evaluate the fidelity of a student model compared to a teacher model in regression tasks.\n",
    "\n",
    "    Parameters:\n",
    "        y_pred_teacher (np.array): Predicted values from the teacher model.\n",
    "        y_pred_student (np.array): Predicted values from the student model.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics for fidelity.\n",
    "    \"\"\"\n",
    "    logger = setup_logging()\n",
    "\n",
    "    mae = mean_absolute_error(y_pred_teacher, y_pred_student)\n",
    "    mse = mean_squared_error(y_pred_teacher, y_pred_student)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_pred_teacher, y_pred_student)\n",
    "\n",
    "    logger.info(f\"\\t Fidelity - MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R^2: {r2:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"fidelity_mae\": mae,\n",
    "        \"fidelity_mse\": mse,\n",
    "        \"fidelity_rmse\": rmse,\n",
    "        \"fidelity_r2\": r2,\n",
    "    }\n",
    "\n",
    "def kl_divergence_hard(probs_pos, y_true):\n",
    "    eps = 1e-7  \n",
    "    probs_pos = np.clip(probs_pos, eps, 1 - eps)\n",
    "    \n",
    "    log_likelihood = y_true * np.log(probs_pos) + (1 - y_true) * np.log(1 - probs_pos)\n",
    "    result = -np.mean(log_likelihood)\n",
    "    \n",
    "    return result if not np.isnan(result) else 0.0\n",
    "\n",
    "def kl_divergence_soft(p, q):\n",
    "    eps = 1e-7\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    q = np.clip(q, eps, 1 - eps)\n",
    "    return np.mean(p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8af1a5",
   "metadata": {},
   "source": [
    "# Model Factory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb36799",
   "metadata": {},
   "source": [
    "## Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9825c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherModelBase(ABC):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.params = kwargs\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model on the given data.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.ndarray): The input features with shape (n_samples, n_features).\n",
    "            y (np.ndarray): The target labels with shape (n_samples,).\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Makes predictions on the provided data.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.ndarray): The input features with shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: An array of predicted probabilities with shape (n_samples, n_classes).\n",
    "                        (If the model returns logits, these should be post-processed to probabilities.)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, y_pred: np.ndarray, y_true: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluates the model's predictions against the true labels.\n",
    "\n",
    "        Parameters:\n",
    "            y_pred (np.ndarray): The predicted probabilities or values.\n",
    "            y_true (np.ndarray): The true labels.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: A dictionary containing evaluation metrics.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            metrics = evaluate_classification(y_pred, y_true)\n",
    "            metrics['parameters'] = self.count_parameters()\n",
    "            return metrics\n",
    "        \n",
    "        elif self.task_type == \"regression\":\n",
    "            metrics = evaluate_regression(y_pred, y_true)\n",
    "            metrics['parameters'] = self.count_parameters()\n",
    "            return metrics\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown task type: {self.task_type}. Must be 'binary' or 'regression'.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def count_parameters(self) -> int:\n",
    "        \"\"\"\n",
    "        Counts the number of trainable parameters in the model.\n",
    "\n",
    "        Returns:\n",
    "            int: The total number of trainable parameters.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class TabPFNTeacherModel(TeacherModelBase):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a TabPFN teacher model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments to pass to the base\n",
    "            class constructor.\n",
    "        \"\"\"\n",
    "        self.device = kwargs.pop(\"device\")\n",
    "        self.config = kwargs.pop(\"config\")\n",
    "        self.task_type = kwargs.pop(\"task_type\")\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def train(self, X, y, **training_params):\n",
    "        if self.task_type == \"binary\":\n",
    "            self.model = TabPFNClassifier()\n",
    "        else:\n",
    "            self.model = TabPFNRegressor()\n",
    "        # If X is bigger than 10000 samples, reduce data size to 10000\n",
    "        if X.shape[0] > 10000:\n",
    "            X = X[:10000]\n",
    "            y = y[:10000]\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.task_type == \"binary\":\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            return self.model.predict(X)\n",
    "\n",
    "    def count_parameters(self) -> int:\n",
    "        \"\"\"\n",
    "        Counts the number of trainable parameters in the underlying TabPFN model.\n",
    "\n",
    "        Returns:\n",
    "            int: Total number of trainable parameters.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the TabPFN model has not been fitted yet.\n",
    "        \"\"\"\n",
    "        if not hasattr(self.model, \"model_\"):\n",
    "            raise ValueError(\n",
    "                \"The TabPFN model is not fitted yet. Please call fit() first.\"\n",
    "            )\n",
    "        return sum(p.numel() for p in self.model.model_.parameters() if p.requires_grad)\n",
    "    \n",
    "\n",
    "class CatBoostTeacherModel(TeacherModelBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a CatBoost teacher model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments to pass to the base class constructor.\n",
    "        \"\"\"\n",
    "        self.device = kwargs.pop(\"device\")\n",
    "        self.config = kwargs.pop(\"config\")\n",
    "        self.task_type = kwargs.pop(\"task_type\")\n",
    "        self.hyperparams = kwargs.pop(\"hyperparams\")\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the CatBoost model using the provided data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target data.\n",
    "        **training_params : dict\n",
    "            Additional training parameters to pass to the CatBoost model's fit method.\n",
    "        \"\"\"\n",
    "        config = self.config\n",
    "        random_state = config[\"training\"][\"random_state\"]\n",
    "\n",
    "        # Initialize the CatBoost model\n",
    "        if self.task_type == \"binary\":\n",
    "            self.model = CatBoostClassifier(\n",
    "                random_seed=random_state,\n",
    "                thread_count=-1,  # Use all available CPU cores\n",
    "                logging_level=\"Silent\",  # Suppress CatBoost output\n",
    "                task_type=(\"GPU\" if str(self.device).lower() == \"cuda\" else \"CPU\"),\n",
    "                **self.hyperparams,  # Include the sampled hyperparameters\n",
    "            )\n",
    "        else:\n",
    "            self.model = CatBoostRegressor(\n",
    "                random_seed=random_state,\n",
    "                thread_count=-1,  # Use all available CPU cores\n",
    "                logging_level=\"Silent\",  # Suppress CatBoost output\n",
    "                task_type=(\"GPU\" if str(self.device).lower() == \"cuda\" else \"CPU\"),\n",
    "                **self.hyperparams,  # Include the sampled hyperparameters\n",
    "            )\n",
    "\n",
    "        # Split data into training and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=random_state\n",
    "        )\n",
    "        # Train the model\n",
    "        self.model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            early_stopping_rounds=20, \n",
    "        )\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on the given data using the trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array-like of shape (n_samples, n_classes)\n",
    "            The predicted probabilities for each class.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            return self.model.predict(X)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"\n",
    "        Count the number of parameters in the trained CatBoost model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The number of parameters in the model.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"model\") or self.model is None:\n",
    "            raise ValueError(\"The model has not been trained yet.\")\n",
    "\n",
    "        # Get the number of trees in the model\n",
    "        tree_count = self.model.tree_count_\n",
    "\n",
    "        # For each tree, count:\n",
    "        # - split values (one per non-leaf node)\n",
    "        # - leaf values (one per leaf node)\n",
    "        # - feature indices (one per non-leaf node)\n",
    "        # For a binary tree with depth d, there are 2^d - 1 non-leaf nodes and 2^d leaf nodes\n",
    "        # This is a simplified approximation\n",
    "        approx_depth = self.hyperparams.get(\"max_depth\", 6)  # Default depth in CatBoost is 6\n",
    "        non_leaf_nodes = 2**approx_depth - 1\n",
    "        leaf_nodes = 2**approx_depth\n",
    "\n",
    "        # Total parameters per tree\n",
    "        params_per_tree = (\n",
    "            non_leaf_nodes * 2 + leaf_nodes\n",
    "        )  # split values + feature indices + leaf values\n",
    "        return tree_count * params_per_tree\n",
    "    \n",
    "\n",
    "class GRANDETeacherModel(TeacherModelBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a GRANDE teacher model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments. The `device` parameter is popped from kwargs and\n",
    "            defaults to 'cpu' if not provided.\n",
    "        \"\"\"\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "        os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "        self.device = kwargs.pop(\"device\")\n",
    "        self.config = kwargs.pop(\"config\")\n",
    "        self.task_type = kwargs.pop(\"task_type\")\n",
    "        self.hyperparams = kwargs.pop(\"hyperparams\")\n",
    "        self.cat_cols = kwargs.pop(\"cat_cols\")\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the GRANDE teacher model using the provided data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target data.\n",
    "        **training_params : dict\n",
    "            Additional training parameters to pass to the GRANDE model's fit method.\n",
    "        \"\"\"\n",
    "        config = self.config\n",
    "        random_state = config[\"training\"][\"random_state\"]\n",
    "\n",
    "        # Determine categorical indices\n",
    "        cat_indices = [i for i, is_cat in enumerate(self.cat_cols) if is_cat]\n",
    "\n",
    "        training_params = {\n",
    "            \"epochs\": 1000,\n",
    "            \"early_stopping_epochs\": 25,\n",
    "            \"batch_size\": 64,\n",
    "            \"cat_idx\": cat_indices,\n",
    "            \"random_seed\": random_state,\n",
    "        }\n",
    "\n",
    "        if self.task_type == \"binary\":\n",
    "            training_params[\"objective\"] = \"binary\"\n",
    "        else:\n",
    "            training_params[\"objective\"] = \"regression\"\n",
    "\n",
    "        # Instantiate the GRANDE teacher model.\n",
    "        self.model = GRANDE(params=self.hyperparams, args=training_params)\n",
    "\n",
    "        # Split the data into training and validation sets.\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=random_state\n",
    "        )\n",
    "        self.model.fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on the given input data using the trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array-like\n",
    "            The predicted target values.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            return self.model.predict(X)\n",
    "        else:\n",
    "            return self.model.predict(X).squeeze()\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"\n",
    "        Count the number of trainable parameters in the GRANDE teacher model based on the\n",
    "        dense representation of a single tree and the number of trees in the ensemble.\n",
    "\n",
    "        The number of parameters for a single tree is computed as:\n",
    "            leaf parameters: 2^d\n",
    "            split thresholds: (2^d - 1) * n\n",
    "            feature selection (one-hot): (2^d - 1) * n\n",
    "        so that:\n",
    "            params_per_tree = 2^d + 2 * n * (2^d - 1)\n",
    "        The total is then:\n",
    "            total_params = n_estimators * params_per_tree\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"model\"):\n",
    "            raise ValueError(\"The model has not been trained/initialized yet.\")\n",
    "\n",
    "        # These attributes must be set when the GRANDE model is instantiated.\n",
    "        # Adjust the attribute names if they are different in your GRANDE implementation.\n",
    "        d = self.model.depth  # Depth of each decision tree.\n",
    "        n = self.model.number_of_variables  # Number of features used in the tree.\n",
    "        E = self.model.n_estimators  # Number of trees in the ensemble.\n",
    "\n",
    "        # Calculate the number of parameters for one tree.\n",
    "        params_per_tree = (2**d) + 2 * n * ((2**d) - 1)\n",
    "        total_params = E * params_per_tree\n",
    "\n",
    "        return total_params\n",
    "    \n",
    "class TabMTeacherModel(TeacherModelBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a TabM teacher model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments to pass to the base class constructor.\n",
    "        \"\"\"\n",
    "        self.device = kwargs.pop(\"device\")\n",
    "        self.config = kwargs.pop(\"config\")\n",
    "        self.task_type = kwargs.pop(\"task_type\")\n",
    "        self.hyperparams = kwargs.pop(\"hyperparams\")\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Set up AMP if CUDA is available.\n",
    "        if torch.cuda.is_available():\n",
    "            self.amp_dtype = (\n",
    "                torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "            )\n",
    "            self.amp_enabled = True\n",
    "        else:\n",
    "            self.amp_dtype = None\n",
    "            self.amp_enabled = False\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the TabM model using the provided data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target data.\n",
    "        **training_params : dict\n",
    "            Additional training parameters to pass to the TabM model's fit method.\n",
    "        \"\"\"\n",
    "        # Get parameters\n",
    "        n_blocks = self.hyperparams.get(\"n_blocks\")\n",
    "        d_block = self.hyperparams.get(\"d_block\")\n",
    "        dropout = self.hyperparams.get(\"dropout\")\n",
    "        lr = self.hyperparams.get(\"learning_rate\")\n",
    "        weight_decay = self.hyperparams.get(\"weight_decay\")\n",
    "\n",
    "        # Set model architecture\n",
    "        arch_type = \"tabm\"\n",
    "        bins = None\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        if self.task_type == \"binary\":\n",
    "            n_classes = len(np.unique(y))\n",
    "            self.n_classes = n_classes\n",
    "            cat_idx = []  # Assumes all features are continuous\n",
    "\n",
    "            # Initialize model\n",
    "            compile_model = False\n",
    "            self.model = Model(\n",
    "                n_num_features=n_features,\n",
    "                cat_cardinalities=cat_idx,\n",
    "                n_classes=n_classes,\n",
    "                backbone={\n",
    "                    \"type\": \"MLP\",\n",
    "                    \"n_blocks\": n_blocks,\n",
    "                    \"d_block\": d_block,\n",
    "                    \"dropout\": dropout,\n",
    "                },\n",
    "                bins=bins,\n",
    "                num_embeddings=(\n",
    "                    None\n",
    "                    if bins is None\n",
    "                    else {\n",
    "                        \"type\": \"PiecewiseLinearEmbeddings\",\n",
    "                        \"d_embedding\": 16,\n",
    "                        \"activation\": False,\n",
    "                        \"version\": \"B\",\n",
    "                    }\n",
    "                ),\n",
    "                arch_type=arch_type,\n",
    "                k=32,\n",
    "                share_training_batches=True,\n",
    "            ).to(self.device)\n",
    "\n",
    "        else:\n",
    "            if len(y.shape) > 1 and y.shape[1] > 1:\n",
    "                # Multiclass logits\n",
    "                n_outputs = y.shape[1]\n",
    "            else:\n",
    "                # Binary or single-output regression\n",
    "                n_outputs = 1\n",
    "\n",
    "            # Initialize model\n",
    "            compile_model = False\n",
    "            self.model = Model(\n",
    "                n_num_features=n_features,\n",
    "                cat_cardinalities=[],  # Assumes all features are continuous\n",
    "                n_classes=n_outputs,\n",
    "                backbone={\n",
    "                    \"type\": \"MLP\",\n",
    "                    \"n_blocks\": n_blocks,\n",
    "                    \"d_block\": d_block,\n",
    "                    \"dropout\": dropout,\n",
    "                },\n",
    "                bins=bins,\n",
    "                num_embeddings=(\n",
    "                    None\n",
    "                    if bins is None\n",
    "                    else {\n",
    "                        \"type\": \"PiecewiseLinearEmbeddings\",\n",
    "                        \"d_embedding\": 16,\n",
    "                        \"activation\": False,\n",
    "                        \"version\": \"B\",\n",
    "                    }\n",
    "                ),\n",
    "                arch_type=arch_type,\n",
    "                k=32,\n",
    "                share_training_batches=True,\n",
    "            ).to(self.device)\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            make_parameter_groups(self.model), lr=lr, weight_decay=weight_decay\n",
    "        )\n",
    "        self.evaluation_mode = torch.no_grad if compile_model else torch.inference_mode\n",
    "\n",
    "        # Prepare data\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        if self.task_type == \"binary\":\n",
    "            y_tensor = torch.tensor(y, dtype=torch.long, device=self.device)\n",
    "        else:\n",
    "            y_tensor = torch.tensor(y, dtype=torch.float32, device=self.device)\n",
    "        n_samples = X_tensor.size(0)\n",
    "        indices = torch.randperm(n_samples)\n",
    "        train_end = int(0.8 * n_samples)\n",
    "        train_idx, val_idx = indices[:train_end], indices[train_end:]\n",
    "        self.data = {\n",
    "            \"train\": {\"x_cont\": X_tensor[train_idx], \"y\": y_tensor[train_idx]},\n",
    "            \"val\": {\"x_cont\": X_tensor[val_idx], \"y\": y_tensor[val_idx]},\n",
    "        }\n",
    "        Y_train = self.data[\"train\"][\"y\"]\n",
    "\n",
    "        # Training parameters\n",
    "        n_epochs = 100000\n",
    "        patience = 16\n",
    "        batch_size = 256\n",
    "        train_size = len(Y_train)\n",
    "        best = {\"val\": -float(\"inf\"), \"epoch\": -1}\n",
    "        remaining_patience = patience\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(n_epochs):\n",
    "            batches = torch.randperm(train_size, device=self.device).split(batch_size)\n",
    "            for batch_idx in batches:\n",
    "                self.model.train()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.loss_fn(\n",
    "                    self.apply_model(\"train\", batch_idx), Y_train[batch_idx]\n",
    "                )\n",
    "                if self.amp_enabled and self.amp_dtype == torch.float16:\n",
    "                    scaler = torch.cuda.amp.GradScaler()\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(self.optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "            val_score = self.evaluate_tabm(\"val\")\n",
    "            if val_score > best[\"val\"]:\n",
    "                best = {\"val\": val_score, \"epoch\": epoch}\n",
    "                remaining_patience = patience\n",
    "            else:\n",
    "                remaining_patience -= 1\n",
    "            if remaining_patience < 0:\n",
    "                break\n",
    "\n",
    "    def apply_model(self, part: str, idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply the model to a batch of data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        part : str\n",
    "            The data partition to use ('train' or 'val').\n",
    "        idx : torch.Tensor\n",
    "            The indices of the samples to use.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The model output.\n",
    "        \"\"\"\n",
    "        with torch.autocast(\n",
    "            self.device.type, enabled=self.amp_enabled, dtype=self.amp_dtype\n",
    "        ):\n",
    "            x_cont = self.data[part][\"x_cont\"][idx]\n",
    "            x_cat = self.data[part].get(\"x_cat\")\n",
    "            if self.task_type == \"binary\":\n",
    "                return self.model(x_cont, x_cat).float()\n",
    "            else:\n",
    "                return (\n",
    "                    self.model(x_cont, x_cat)\n",
    "                    .squeeze(-1)  # Remove the last dimension for regression tasks\n",
    "                    .float()\n",
    "                )\n",
    "\n",
    "    def loss_fn(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : torch.Tensor\n",
    "            The predicted outputs.\n",
    "        y_true : torch.Tensor\n",
    "            The ground truth.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The loss value.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            # Use the second-to-last dimension for averaging over k heads.\n",
    "            k = y_pred.shape[-2]\n",
    "            target = y_true.repeat_interleave(k)\n",
    "            return F.cross_entropy(y_pred.flatten(0, 1), target)\n",
    "        else:\n",
    "            # TabM produces k predictions. Each of them must be trained separately.\n",
    "            # For student model (regression on logits), y_pred.shape == (batch_size, k)\n",
    "            k = y_pred.shape[-1 if len(y_pred.shape) <= 2 else -2]\n",
    "\n",
    "            # Flatten the predictions and repeat the targets\n",
    "            return F.mse_loss(\n",
    "                y_pred.flatten(0, 1),\n",
    "                (\n",
    "                    y_true.repeat_interleave(k)\n",
    "                    if self.model.share_training_batches\n",
    "                    else y_true\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def evaluate_tabm(self, part: str) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the model on a data partition.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        part : str\n",
    "            The data partition to use ('train' or 'val').\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The accuracy score.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        with self.evaluation_mode():\n",
    "            eval_batch_size = 8096\n",
    "\n",
    "            if self.task_type == \"binary\":\n",
    "                indices = torch.arange(\n",
    "                    len(self.data[part][\"y\"]), device=self.device\n",
    "                ).split(eval_batch_size)\n",
    "                y_pred = torch.cat(\n",
    "                    [self.apply_model(part, idx) for idx in indices]\n",
    "                ).cpu()\n",
    "                y_pred = torch.softmax(y_pred, dim=-1).mean(1)\n",
    "                y_true = self.data[part][\"y\"].cpu().numpy()\n",
    "                return float(accuracy_score(y_true, y_pred.argmax(1)))\n",
    "            else:\n",
    "                y_pred = (\n",
    "                    torch.cat(\n",
    "                        [\n",
    "                            self.apply_model(part, idx)\n",
    "                            for idx in torch.arange(\n",
    "                                len(self.data[part][\"y\"]), device=self.device\n",
    "                            ).split(eval_batch_size)\n",
    "                        ]\n",
    "                    )\n",
    "                    .cpu()\n",
    "                    .numpy()\n",
    "                )\n",
    "\n",
    "                # Transform the predictions back to the original label space\n",
    "                if hasattr(self, \"regression_label_stats\"):\n",
    "                    y_pred = (\n",
    "                        y_pred * self.regression_label_stats[\"std\"]\n",
    "                        + self.regression_label_stats[\"mean\"]\n",
    "                    )\n",
    "\n",
    "                # Compute the mean of the k predictions\n",
    "                y_pred = y_pred.mean(1)\n",
    "\n",
    "                y_true = self.data[part][\"y\"].cpu().numpy()\n",
    "\n",
    "                # For student model, we want to minimize MSE, so return negative MSE\n",
    "                # The higher the value, the better (consistent with example.ipynb)\n",
    "                score = -(mean_squared_error(y_true, y_pred) ** 0.5)\n",
    "                return float(score)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on the given data using the trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array-like of shape (n_samples,)\n",
    "            The predicted class indices.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            self.model.eval()\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(X_tensor)\n",
    "                # Since share_training_batches is True, average over the second dimension.\n",
    "                probs = torch.softmax(logits, dim=-1).mean(1)\n",
    "                return probs.cpu().numpy()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "            with torch.no_grad():\n",
    "                # TabM produces output with shape (batch_size, k) or (batch_size, k, n_outputs)\n",
    "                logits = self.model(X_tensor)\n",
    "\n",
    "                # For student model (regression), we need to average over the k predictions\n",
    "                # Check the dimensions of logits and average appropriately\n",
    "                if logits.ndim > 2:  # Case: (batch_size, k, n_outputs)\n",
    "                    logits = logits.mean(1)  # Average over k dimension\n",
    "                elif (\n",
    "                    logits.shape[1] > 1\n",
    "                    and hasattr(self.model, \"k\")\n",
    "                    and self.model.k > 1\n",
    "                ):\n",
    "                    # Case: (batch_size, k) for single output regression\n",
    "                    logits = logits.mean(\n",
    "                        1, keepdim=True\n",
    "                    )  # Average and keep output dimension\n",
    "\n",
    "                return logits.cpu().numpy()\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"\n",
    "        Count the number of trainable parameters in the model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The total number of trainable parameters.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"model\"):\n",
    "            raise ValueError(\"The model has not been trained yet\")\n",
    "        return sum(p.numel() for p in self.model.parameters() if p.requires_grad)    \n",
    "\n",
    "\n",
    "class MLPTeacherModel(TeacherModelBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a TabM teacher model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments to pass to the base class constructor.\n",
    "        \"\"\"\n",
    "        self.device = kwargs.pop(\"device\")\n",
    "        self.config = kwargs.pop(\"config\")\n",
    "        self.task_type = kwargs.pop(\"task_type\")\n",
    "        self.hyperparams = kwargs.pop(\"hyperparams\")\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Set up AMP if CUDA is available.\n",
    "        if torch.cuda.is_available():\n",
    "            self.amp_dtype = (\n",
    "                torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "            )\n",
    "            self.amp_enabled = True\n",
    "        else:\n",
    "            self.amp_dtype = None\n",
    "            self.amp_enabled = False\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the TabM model using the provided data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target data.\n",
    "        **training_params : dict\n",
    "            Additional training parameters to pass to the TabM model's fit method.\n",
    "        \"\"\"\n",
    "        # Get parameters\n",
    "        n_blocks = self.hyperparams.get(\"n_blocks\")\n",
    "        d_block = self.hyperparams.get(\"d_block\")\n",
    "        dropout = self.hyperparams.get(\"dropout\")\n",
    "        lr = self.hyperparams.get(\"learning_rate\")\n",
    "        weight_decay = self.hyperparams.get(\"weight_decay\")\n",
    "\n",
    "        # Set model architecture\n",
    "        arch_type = \"plain\"\n",
    "        bins = None\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        if self.task_type == \"binary\":\n",
    "            n_classes = len(np.unique(y))\n",
    "            self.n_classes = n_classes\n",
    "            cat_idx = []  # Assumes all features are continuous\n",
    "\n",
    "            # Initialize model\n",
    "            compile_model = False\n",
    "            self.model = Model(\n",
    "                n_num_features=n_features,\n",
    "                cat_cardinalities=cat_idx,\n",
    "                n_classes=n_classes,\n",
    "                backbone={\n",
    "                    \"type\": \"MLP\",\n",
    "                    \"n_blocks\": n_blocks,\n",
    "                    \"d_block\": d_block,\n",
    "                    \"dropout\": dropout,\n",
    "                },\n",
    "                bins=bins,\n",
    "                num_embeddings=(\n",
    "                    None\n",
    "                    if bins is None\n",
    "                    else {\n",
    "                        \"type\": \"PiecewiseLinearEmbeddings\",\n",
    "                        \"d_embedding\": 16,\n",
    "                        \"activation\": False,\n",
    "                        \"version\": \"B\",\n",
    "                    }\n",
    "                ),\n",
    "                arch_type=arch_type,\n",
    "                k=None,\n",
    "                share_training_batches=True,\n",
    "            ).to(self.device)\n",
    "\n",
    "        else:\n",
    "            if len(y.shape) > 1 and y.shape[1] > 1:\n",
    "                # Multiclass logits\n",
    "                n_outputs = y.shape[1]\n",
    "            else:\n",
    "                # Binary or single-output regression\n",
    "                n_outputs = 1\n",
    "\n",
    "            # Initialize model\n",
    "            compile_model = False\n",
    "            self.model = Model(\n",
    "                n_num_features=n_features,\n",
    "                cat_cardinalities=[],  # Assumes all features are continuous\n",
    "                n_classes=n_outputs,\n",
    "                backbone={\n",
    "                    \"type\": \"MLP\",\n",
    "                    \"n_blocks\": n_blocks,\n",
    "                    \"d_block\": d_block,\n",
    "                    \"dropout\": dropout,\n",
    "                },\n",
    "                bins=bins,\n",
    "                num_embeddings=(\n",
    "                    None\n",
    "                    if bins is None\n",
    "                    else {\n",
    "                        \"type\": \"PiecewiseLinearEmbeddings\",\n",
    "                        \"d_embedding\": 16,\n",
    "                        \"activation\": False,\n",
    "                        \"version\": \"B\",\n",
    "                    }\n",
    "                ),\n",
    "                arch_type=arch_type,\n",
    "                k=None,\n",
    "                share_training_batches=True,\n",
    "            ).to(self.device)\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            make_parameter_groups(self.model), lr=lr, weight_decay=weight_decay\n",
    "        )\n",
    "        self.evaluation_mode = torch.no_grad if compile_model else torch.inference_mode\n",
    "\n",
    "        # Prepare data\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        if self.task_type == \"binary\":\n",
    "            y_tensor = torch.tensor(y, dtype=torch.long, device=self.device)\n",
    "        else:\n",
    "            y_tensor = torch.tensor(y, dtype=torch.float32, device=self.device)\n",
    "        n_samples = X_tensor.size(0)\n",
    "        indices = torch.randperm(n_samples)\n",
    "        train_end = int(0.8 * n_samples)\n",
    "        train_idx, val_idx = indices[:train_end], indices[train_end:]\n",
    "        self.data = {\n",
    "            \"train\": {\"x_cont\": X_tensor[train_idx], \"y\": y_tensor[train_idx]},\n",
    "            \"val\": {\"x_cont\": X_tensor[val_idx], \"y\": y_tensor[val_idx]},\n",
    "        }\n",
    "        Y_train = self.data[\"train\"][\"y\"]\n",
    "\n",
    "        # Training parameters\n",
    "        n_epochs = 100000\n",
    "        patience = 16\n",
    "        batch_size = 256\n",
    "        train_size = len(Y_train)\n",
    "        best = {\"val\": -float(\"inf\"), \"epoch\": -1}\n",
    "        remaining_patience = patience\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(n_epochs):\n",
    "            batches = torch.randperm(train_size, device=self.device).split(batch_size)\n",
    "            for batch_idx in batches:\n",
    "                self.model.train()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.loss_fn(\n",
    "                    self.apply_model(\"train\", batch_idx), Y_train[batch_idx]\n",
    "                )\n",
    "                if self.amp_enabled and self.amp_dtype == torch.float16:\n",
    "                    scaler = torch.cuda.amp.GradScaler()\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(self.optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "            val_score = self.evaluate_tabm(\"val\")\n",
    "            if val_score > best[\"val\"]:\n",
    "                best = {\"val\": val_score, \"epoch\": epoch}\n",
    "                remaining_patience = patience\n",
    "            else:\n",
    "                remaining_patience -= 1\n",
    "            if remaining_patience < 0:\n",
    "                break\n",
    "\n",
    "    def apply_model(self, part: str, idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply the model to a batch of data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        part : str\n",
    "            The data partition to use ('train' or 'val').\n",
    "        idx : torch.Tensor\n",
    "            The indices of the samples to use.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The model output.\n",
    "        \"\"\"\n",
    "        with torch.autocast(\n",
    "            self.device.type, enabled=self.amp_enabled, dtype=self.amp_dtype\n",
    "        ):\n",
    "            x_cont = self.data[part][\"x_cont\"][idx]\n",
    "            x_cat = self.data[part].get(\"x_cat\")\n",
    "            if self.task_type == \"binary\":\n",
    "                return self.model(x_cont, x_cat).float()\n",
    "            else:\n",
    "                return (\n",
    "                    self.model(x_cont, x_cat)\n",
    "                    .squeeze(-1)  # Remove the last dimension for regression tasks\n",
    "                    .float()\n",
    "                )\n",
    "\n",
    "    def loss_fn(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : torch.Tensor\n",
    "            The predicted outputs.\n",
    "        y_true : torch.Tensor\n",
    "            The ground truth.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The loss value.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            # Use the second-to-last dimension for averaging over k heads.\n",
    "            k = y_pred.shape[-2]\n",
    "            target = y_true.repeat_interleave(k)\n",
    "            return F.cross_entropy(y_pred.flatten(0, 1), target)\n",
    "        else:\n",
    "            # TabM produces k predictions. Each of them must be trained separately.\n",
    "            # For student model (regression on logits), y_pred.shape == (batch_size, k)\n",
    "            k = y_pred.shape[-1 if len(y_pred.shape) <= 2 else -2]\n",
    "\n",
    "            # Flatten the predictions and repeat the targets\n",
    "            return F.mse_loss(\n",
    "                y_pred.flatten(0, 1),\n",
    "                (\n",
    "                    y_true.repeat_interleave(k)\n",
    "                    if self.model.share_training_batches\n",
    "                    else y_true\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def evaluate_tabm(self, part: str) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the model on a data partition.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        part : str\n",
    "            The data partition to use ('train' or 'val').\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The accuracy score.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        with self.evaluation_mode():\n",
    "            eval_batch_size = 8096\n",
    "\n",
    "            if self.task_type == \"binary\":\n",
    "                indices = torch.arange(\n",
    "                    len(self.data[part][\"y\"]), device=self.device\n",
    "                ).split(eval_batch_size)\n",
    "                y_pred = torch.cat(\n",
    "                    [self.apply_model(part, idx) for idx in indices]\n",
    "                ).cpu()\n",
    "                y_pred = torch.softmax(y_pred, dim=-1).mean(1)\n",
    "                y_true = self.data[part][\"y\"].cpu().numpy()\n",
    "                return float(accuracy_score(y_true, y_pred.argmax(1)))\n",
    "            else:\n",
    "                y_pred = (\n",
    "                    torch.cat(\n",
    "                        [\n",
    "                            self.apply_model(part, idx)\n",
    "                            for idx in torch.arange(\n",
    "                                len(self.data[part][\"y\"]), device=self.device\n",
    "                            ).split(eval_batch_size)\n",
    "                        ]\n",
    "                    )\n",
    "                    .cpu()\n",
    "                    .numpy()\n",
    "                )\n",
    "\n",
    "                # Transform the predictions back to the original label space\n",
    "                if hasattr(self, \"regression_label_stats\"):\n",
    "                    y_pred = (\n",
    "                        y_pred * self.regression_label_stats[\"std\"]\n",
    "                        + self.regression_label_stats[\"mean\"]\n",
    "                    )\n",
    "\n",
    "                # Compute the mean of the k predictions\n",
    "                y_pred = y_pred.mean(1)\n",
    "\n",
    "                y_true = self.data[part][\"y\"].cpu().numpy()\n",
    "\n",
    "                # For student model, we want to minimize MSE, so return negative MSE\n",
    "                # The higher the value, the better (consistent with example.ipynb)\n",
    "                score = -(mean_squared_error(y_true, y_pred) ** 0.5)\n",
    "                return float(score)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on the given data using the trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array-like of shape (n_samples,)\n",
    "            The predicted class indices.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            self.model.eval()\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(X_tensor)\n",
    "                # Since share_training_batches is True, average over the second dimension.\n",
    "                probs = torch.softmax(logits, dim=-1).mean(1)\n",
    "                return probs.cpu().numpy()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "            with torch.no_grad():\n",
    "                # TabM produces output with shape (batch_size, k) or (batch_size, k, n_outputs)\n",
    "                logits = self.model(X_tensor)\n",
    "\n",
    "                # For student model (regression), we need to average over the k predictions\n",
    "                # Check the dimensions of logits and average appropriately\n",
    "                if logits.ndim > 2:  # Case: (batch_size, k, n_outputs)\n",
    "                    logits = logits.mean(1)  # Average over k dimension\n",
    "                elif (\n",
    "                    logits.shape[1] > 1\n",
    "                    and hasattr(self.model, \"k\")\n",
    "                    and self.model.k > 1\n",
    "                ):\n",
    "                    # Case: (batch_size, k) for single output regression\n",
    "                    logits = logits.mean(\n",
    "                        1, keepdim=True\n",
    "                    )  # Average and keep output dimension\n",
    "\n",
    "                return logits.cpu().numpy()\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"\n",
    "        Count the number of trainable parameters in the model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The total number of trainable parameters.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"model\"):\n",
    "            raise ValueError(\"The model has not been trained yet\")\n",
    "        return sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "class GRANDETeacherModel(TeacherModelBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a GRANDE teacher model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments. The `device` parameter is popped from kwargs and\n",
    "            defaults to 'cpu' if not provided.\n",
    "        \"\"\"\n",
    "        self.device = kwargs.pop(\"device\", \"cpu\")\n",
    "        self.config = kwargs.pop(\"config\")\n",
    "        self.dataset_id = kwargs.pop(\"dataset_id\")\n",
    "        self.task_type = kwargs.pop(\"task_type\")\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def train(self, X, y, **training_params):\n",
    "        \"\"\"\n",
    "        Train the GRANDE teacher model using the provided data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target data.\n",
    "        **training_params : dict\n",
    "            Additional training parameters to pass to the GRANDE model's fit method.\n",
    "        \"\"\"\n",
    "        config = self.config\n",
    "        random_state = config[\"training\"][\"random_state\"]\n",
    "        cat_idx = config[\"teacher_models\"][\"grande\"][\"hyperparameter_space\"][\n",
    "            f\"dataset_{self.dataset_id}\"\n",
    "        ][\"training_params\"].get(\"cat_idx\", None)\n",
    "\n",
    "        training_params[\"random_seed\"] = random_state\n",
    "        training_params[\"cat_idx\"] = cat_idx[\"choices\"][0]\n",
    "\n",
    "        # Instantiate the GRANDE teacher model.\n",
    "        self.model = GRANDE(params=self.params, args=training_params)\n",
    "\n",
    "        # Split the data into training and validation sets.\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=random_state\n",
    "        )\n",
    "        self.model.fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on the given input data using the trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array-like\n",
    "            The predicted target values.\n",
    "        \"\"\"\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def get_logits(self, X):\n",
    "        \"\"\"\n",
    "        Compute the logits of the GRANDE teacher model on the given input data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            The computed logits as a numpy array.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            probs = self.predict(X)\n",
    "\n",
    "            # Ensure binary classification (2 classes)\n",
    "            if probs.shape[1] != 2:\n",
    "                raise ValueError(\n",
    "                    \"get_logits is implemented for binary classification only.\"\n",
    "                )\n",
    "\n",
    "            # Clip probabilities to avoid log(0)\n",
    "            eps = 1e-15\n",
    "            probs = np.clip(probs, eps, 1 - eps)\n",
    "\n",
    "            # Compute logits by taking the logarithm of probabilities for each class.\n",
    "            logits = np.log(probs)\n",
    "\n",
    "        else:\n",
    "            # For regression, we can directly use the predictions as logits.\n",
    "            logits = self.predict(X)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        if self.task_type == \"binary\":\n",
    "            return evaluate_classification(\n",
    "                y_true=y,\n",
    "                y_pred=np.round(preds[:, 1]),\n",
    "                y_prob=preds[:, 1],\n",
    "                average=\"macro\",\n",
    "            )\n",
    "        else:\n",
    "            return evaluate_regression(y_true=y, y_pred=preds)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"\n",
    "        Count the number of trainable parameters in the GRANDE teacher model based on the\n",
    "        dense representation of a single tree and the number of trees in the ensemble.\n",
    "\n",
    "        The number of parameters for a single tree is computed as:\n",
    "            leaf parameters: 2^d\n",
    "            split thresholds: (2^d - 1) * n\n",
    "            feature selection (one-hot): (2^d - 1) * n\n",
    "        so that:\n",
    "            params_per_tree = 2^d + 2 * n * (2^d - 1)\n",
    "        The total is then:\n",
    "            total_params = n_estimators * params_per_tree\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"model\"):\n",
    "            raise ValueError(\"The model has not been trained/initialized yet.\")\n",
    "\n",
    "        # These attributes must be set when the GRANDE model is instantiated.\n",
    "        # Adjust the attribute names if they are different in your GRANDE implementation.\n",
    "        d = self.model.depth  # Depth of each decision tree.\n",
    "        n = self.model.number_of_variables  # Number of features used in the tree.\n",
    "        E = self.model.n_estimators  # Number of trees in the ensemble.\n",
    "\n",
    "        # Calculate the number of parameters for one tree.\n",
    "        params_per_tree = (2**d) + 2 * n * ((2**d) - 1)\n",
    "        total_params = E * params_per_tree\n",
    "\n",
    "        return total_params\n",
    "\n",
    "class RandomForestTeacherModel(TeacherModelBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a RandomForest teacher model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments to pass to the base class constructor.\n",
    "        \"\"\"\n",
    "        self.device = kwargs.pop(\"device\")\n",
    "        self.config = kwargs.pop(\"config\")\n",
    "        self.task_type = kwargs.pop(\"task_type\")\n",
    "        self.hyperparams = kwargs.pop(\"hyperparams\")\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the RandomForest teacher model using the provided data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target data.\n",
    "        **training_params : dict\n",
    "            Additional training parameters (not used for RandomForest).\n",
    "        \"\"\"\n",
    "        config = self.config\n",
    "        random_state = config[\"training\"][\"random_state\"]\n",
    "\n",
    "        if self.task_type == \"binary\":\n",
    "            # Initialize the RandomForest model with the provided hyperparameters\n",
    "            self.model = RandomForestClassifier(\n",
    "                random_state=random_state,\n",
    "                n_jobs=-1,  # Use all available CPU cores\n",
    "                **self.hyperparams,\n",
    "            )\n",
    "        else:\n",
    "            self.model = RandomForestRegressor(\n",
    "                random_state=random_state,\n",
    "                n_jobs=-1,  # Use all available CPU cores\n",
    "                **self.hyperparams,\n",
    "            )\n",
    "\n",
    "        # Train the model\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make probability predictions on the given data using the trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array-like of shape (n_samples, n_classes)\n",
    "            The predicted probabilities for each class.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            return self.model.predict(X)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"\n",
    "        Count the number of parameters in the trained RandomForest model.\n",
    "\n",
    "        For each tree, we count:\n",
    "        - The feature indices at each node (1 parameter per node except leaf nodes)\n",
    "        - The thresholds at each node (1 parameter per node except leaf nodes)\n",
    "        - The values at each leaf node (1 parameter per leaf node)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The number of parameters in the model.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"model\") or self.model is None:\n",
    "            raise ValueError(\"The model has not been trained yet.\")\n",
    "\n",
    "        n_params = 0\n",
    "        for tree in self.model.estimators_:\n",
    "            tree = tree.tree_\n",
    "            # Number of nodes in the tree\n",
    "            n_nodes = tree.node_count\n",
    "            # Number of leaf nodes\n",
    "            n_leaves = tree.n_leaves\n",
    "            # Number of internal nodes\n",
    "            n_internal = n_nodes - n_leaves\n",
    "            # Each internal node has a feature index and a threshold\n",
    "            n_params += 2 * n_internal\n",
    "\n",
    "            # Each leaf node has value(s) - depends on task type\n",
    "            if self.task_type == \"binary\":\n",
    "                # For classification, each leaf stores class distribution\n",
    "                n_params += n_leaves * self.model.n_classes_\n",
    "            else:\n",
    "                # For regression, each leaf stores a single value\n",
    "                n_params += n_leaves\n",
    "\n",
    "        return n_params\n",
    "    \n",
    "def get_teacher_model(config: dict, task_type: Literal[\"binary\", \"regression\"], device, hyperparams, cat_cols=None) -> TeacherModelBase:\n",
    "    \"\"\"\n",
    "    Returns an instance of the teacher model based on the configuration.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict\n",
    "        Configuration dictionary containing model and dataset details.\n",
    "    device : torch.device\n",
    "        The device to use for training (CPU or GPU).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TeacherModelBase\n",
    "        An instance of the teacher model.\n",
    "    \"\"\"\n",
    "    model_type = config[\"model\"][\"teacher_model\"]\n",
    "    if model_type == \"tabpfn\":\n",
    "        return TabPFNTeacherModel(config=config, task_type=task_type, device=device, hyperparams=hyperparams)\n",
    "    elif model_type == \"catboost\":\n",
    "        return CatBoostTeacherModel(config=config, task_type=task_type, device=device, hyperparams=hyperparams)\n",
    "    elif model_type == \"grande\":\n",
    "        return GRANDETeacherModel(config=config, task_type=task_type, device=device, hyperparams=hyperparams, cat_cols=cat_cols)\n",
    "    elif model_type == \"tabm\":\n",
    "        return TabMTeacherModel(config=config, task_type=task_type, device=device, hyperparams=hyperparams)\n",
    "    elif model_type == \"mlp\":\n",
    "        return MLPTeacherModel(config=config, task_type=task_type, device=device, hyperparams=hyperparams)\n",
    "    elif model_type == \"random_forest\":\n",
    "        return RandomForestTeacherModel(config=config, task_type=task_type, device=device, hyperparams=hyperparams)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown teacher type: {config['model']['teacher_model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26ba41a",
   "metadata": {},
   "source": [
    "## Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fbb13e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentModelBase(ABC):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.params = kwargs\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model on the given data.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.ndarray): The input features with shape (n_samples, n_features).\n",
    "            y (np.ndarray): The target labels with shape (n_samples,).\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Makes predictions on the provided data.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.ndarray): The input features with shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: An array of predicted probabilities with shape (n_samples, n_classes).\n",
    "                        (If the model returns logits, these should be post-processed to probabilities.)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, y_pred: np.ndarray, y_true: np.ndarray, y_teacher_true: np.ndarray, original_task_type: Literal[\"binary\", \"regression\"]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluates the model's predictions against the true labels.\n",
    "\n",
    "        Parameters:\n",
    "            y_pred (np.ndarray): The predicted probabilities or values.\n",
    "            y_true (np.ndarray): The true labels.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: A dictionary containing evaluation metrics.\n",
    "        \"\"\"     \n",
    "        if original_task_type == \"binary\":\n",
    "            if self.task_type == \"binary\":\n",
    "                metrics = evaluate_classification(y_pred, y_true)\n",
    "                fidelity_metrics = evaluate_fidelity_classification(y_pred, y_teacher_true)\n",
    "                metrics.update(fidelity_metrics)\n",
    "                metrics['parameters'] = self.count_parameters()\n",
    "                return metrics\n",
    "            \n",
    "            elif self.task_type == \"regression\":\n",
    "                # Convert logits to probabilities\n",
    "                y_prob = 1 / (1 + np.exp(-y_pred))  # Sigmoid function\n",
    "                y_prob = np.column_stack([1 - y_prob, y_prob])  # Create 2D array with probs for both classes\n",
    "\n",
    "                y_teacher_prob = 1 / (1 + np.exp(-y_teacher_true))\n",
    "                y_teacher_prob = np.column_stack([1 - y_teacher_prob, y_teacher_prob])\n",
    "                \n",
    "                metrics = evaluate_classification(y_prob, y_true)\n",
    "                class_fidelity_metrics = evaluate_fidelity_classification(y_prob, y_teacher_prob)\n",
    "                metrics.update(class_fidelity_metrics)\n",
    "                reg_fidelity_metrics = evaluate_fidelity_regression(y_pred, y_teacher_true)\n",
    "                metrics.update(reg_fidelity_metrics)\n",
    "                metrics['parameters'] = self.count_parameters()\n",
    "                return metrics\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"Unknown task type: {self.task_type}. Must be 'binary' or 'regression'.\")\n",
    "        \n",
    "        elif original_task_type == \"regression\":\n",
    "            metrics = evaluate_regression(y_pred, y_true)\n",
    "            fidelity_metrics = evaluate_fidelity_regression(y_pred, y_teacher_true)\n",
    "            metrics.update(fidelity_metrics)\n",
    "            metrics['parameters'] = self.count_parameters()\n",
    "            return metrics\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown task type: {self.task_type}. Must be 'binary' or 'regression'.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def count_parameters(self) -> int:\n",
    "        \"\"\"\n",
    "        Counts the number of trainable parameters in the model.\n",
    "\n",
    "        Returns:\n",
    "            int: The total number of trainable parameters.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class CatBoostStudentModel(StudentModelBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a CatBoost teacher model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments to pass to the base class constructor.\n",
    "        \"\"\"\n",
    "        self.device = kwargs.pop(\"device\")\n",
    "        self.config = kwargs.pop(\"config\")\n",
    "        self.task_type = kwargs.pop(\"task_type\")\n",
    "        self.hyperparams = kwargs.pop(\"hyperparams\")\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the CatBoost model using the provided data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target data.\n",
    "        **training_params : dict\n",
    "            Additional training parameters to pass to the CatBoost model's fit method.\n",
    "        \"\"\"\n",
    "        config = self.config\n",
    "        random_state = config[\"training\"][\"random_state\"]\n",
    "\n",
    "        # Initialize the CatBoost model\n",
    "        if self.task_type == \"binary\":\n",
    "            self.model = CatBoostClassifier(\n",
    "                random_seed=random_state,\n",
    "                thread_count=-1,  # Use all available CPU cores\n",
    "                logging_level=\"Silent\",  # Suppress CatBoost output\n",
    "                task_type=(\"GPU\" if str(self.device).lower() == \"cuda\" else \"CPU\"),\n",
    "                **self.hyperparams,  # Include the sampled hyperparameters\n",
    "            )\n",
    "        else:\n",
    "            self.model = CatBoostRegressor(\n",
    "                random_seed=random_state,\n",
    "                thread_count=-1,  # Use all available CPU cores\n",
    "                logging_level=\"Silent\",  # Suppress CatBoost output\n",
    "                task_type=(\"GPU\" if str(self.device).lower() == \"cuda\" else \"CPU\"),\n",
    "                **self.hyperparams,  # Include the sampled hyperparameters\n",
    "            )\n",
    "\n",
    "        # Split data into training and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=random_state\n",
    "        )\n",
    "        # Train the model\n",
    "        self.model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            early_stopping_rounds=20, \n",
    "        )\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on the given data using the trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array-like of shape (n_samples, n_classes)\n",
    "            The predicted probabilities for each class.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            return self.model.predict(X)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"\n",
    "        Count the number of parameters in the trained CatBoost model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The number of parameters in the model.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"model\") or self.model is None:\n",
    "            raise ValueError(\"The model has not been trained yet.\")\n",
    "\n",
    "        # Get the number of trees in the model\n",
    "        tree_count = self.model.tree_count_\n",
    "\n",
    "        # For each tree, count:\n",
    "        # - split values (one per non-leaf node)\n",
    "        # - leaf values (one per leaf node)\n",
    "        # - feature indices (one per non-leaf node)\n",
    "        # For a binary tree with depth d, there are 2^d - 1 non-leaf nodes and 2^d leaf nodes\n",
    "        # This is a simplified approximation\n",
    "        approx_depth = self.hyperparams.get(\"max_depth\", 6)  # Default depth in CatBoost is 6\n",
    "        non_leaf_nodes = 2**approx_depth - 1\n",
    "        leaf_nodes = 2**approx_depth\n",
    "\n",
    "        # Total parameters per tree\n",
    "        params_per_tree = (\n",
    "            non_leaf_nodes * 2 + leaf_nodes\n",
    "        )  # split values + feature indices + leaf values\n",
    "        return tree_count * params_per_tree\n",
    "    \n",
    "\n",
    "class GRANDEStudentModel(StudentModelBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a GRANDE teacher model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments. The `device` parameter is popped from kwargs and\n",
    "            defaults to 'cpu' if not provided.\n",
    "        \"\"\"\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "        os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "        self.device = kwargs.pop(\"device\")\n",
    "        self.config = kwargs.pop(\"config\")\n",
    "        self.task_type = kwargs.pop(\"task_type\")\n",
    "        self.hyperparams = kwargs.pop(\"hyperparams\")\n",
    "        self.cat_cols = kwargs.pop(\"cat_cols\")\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the GRANDE teacher model using the provided data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target data.\n",
    "        **training_params : dict\n",
    "            Additional training parameters to pass to the GRANDE model's fit method.\n",
    "        \"\"\"\n",
    "        config = self.config\n",
    "        random_state = config[\"training\"][\"random_state\"]\n",
    "\n",
    "        # Determine categorical indices\n",
    "        cat_indices = [i for i, is_cat in enumerate(self.cat_cols) if is_cat]\n",
    "\n",
    "        training_params = {\n",
    "            \"epochs\": 1000,\n",
    "            \"early_stopping_epochs\": 25,\n",
    "            \"batch_size\": 64,\n",
    "            \"cat_idx\": cat_indices,\n",
    "            \"random_seed\": random_state,\n",
    "        }\n",
    "\n",
    "        if self.task_type == \"binary\":\n",
    "            training_params[\"objective\"] = \"binary\"\n",
    "        else:\n",
    "            training_params[\"objective\"] = \"regression\"\n",
    "\n",
    "        # Instantiate the GRANDE teacher model.\n",
    "        self.model = GRANDE(params=self.hyperparams, args=training_params)\n",
    "\n",
    "        # Split the data into training and validation sets.\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=random_state\n",
    "        )\n",
    "        self.model.fit(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on the given input data using the trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array-like\n",
    "            The predicted target values.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            return self.model.predict(X)\n",
    "        else:\n",
    "            return self.model.predict(X).squeeze()\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"\n",
    "        Count the number of trainable parameters in the GRANDE teacher model based on the\n",
    "        dense representation of a single tree and the number of trees in the ensemble.\n",
    "\n",
    "        The number of parameters for a single tree is computed as:\n",
    "            leaf parameters: 2^d\n",
    "            split thresholds: (2^d - 1) * n\n",
    "            feature selection (one-hot): (2^d - 1) * n\n",
    "        so that:\n",
    "            params_per_tree = 2^d + 2 * n * (2^d - 1)\n",
    "        The total is then:\n",
    "            total_params = n_estimators * params_per_tree\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"model\"):\n",
    "            raise ValueError(\"The model has not been trained/initialized yet.\")\n",
    "\n",
    "        # These attributes must be set when the GRANDE model is instantiated.\n",
    "        # Adjust the attribute names if they are different in your GRANDE implementation.\n",
    "        d = self.model.depth  # Depth of each decision tree.\n",
    "        n = self.model.number_of_variables  # Number of features used in the tree.\n",
    "        E = self.model.n_estimators  # Number of trees in the ensemble.\n",
    "\n",
    "        # Calculate the number of parameters for one tree.\n",
    "        params_per_tree = (2**d) + 2 * n * ((2**d) - 1)\n",
    "        total_params = E * params_per_tree\n",
    "\n",
    "        return total_params\n",
    "    \n",
    "class TabMStudentModel(StudentModelBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a TabM teacher model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments to pass to the base class constructor.\n",
    "        \"\"\"\n",
    "        self.device = kwargs.pop(\"device\")\n",
    "        self.config = kwargs.pop(\"config\")\n",
    "        self.task_type = kwargs.pop(\"task_type\")\n",
    "        self.hyperparams = kwargs.pop(\"hyperparams\")\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Set up AMP if CUDA is available.\n",
    "        if torch.cuda.is_available():\n",
    "            self.amp_dtype = (\n",
    "                torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "            )\n",
    "            self.amp_enabled = True\n",
    "        else:\n",
    "            self.amp_dtype = None\n",
    "            self.amp_enabled = False\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the TabM model using the provided data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target data.\n",
    "        **training_params : dict\n",
    "            Additional training parameters to pass to the TabM model's fit method.\n",
    "        \"\"\"\n",
    "        # Get parameters\n",
    "        n_blocks = self.hyperparams.get(\"n_blocks\")\n",
    "        d_block = self.hyperparams.get(\"d_block\")\n",
    "        dropout = self.hyperparams.get(\"dropout\")\n",
    "        lr = self.hyperparams.get(\"learning_rate\")\n",
    "        weight_decay = self.hyperparams.get(\"weight_decay\")\n",
    "\n",
    "        # Set model architecture\n",
    "        arch_type = \"tabm\"\n",
    "        bins = None\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        if self.task_type == \"binary\":\n",
    "            n_classes = len(np.unique(y))\n",
    "            self.n_classes = n_classes\n",
    "            cat_idx = []  # Assumes all features are continuous\n",
    "\n",
    "            # Initialize model\n",
    "            compile_model = False\n",
    "            self.model = Model(\n",
    "                n_num_features=n_features,\n",
    "                cat_cardinalities=cat_idx,\n",
    "                n_classes=n_classes,\n",
    "                backbone={\n",
    "                    \"type\": \"MLP\",\n",
    "                    \"n_blocks\": n_blocks,\n",
    "                    \"d_block\": d_block,\n",
    "                    \"dropout\": dropout,\n",
    "                },\n",
    "                bins=bins,\n",
    "                num_embeddings=(\n",
    "                    None\n",
    "                    if bins is None\n",
    "                    else {\n",
    "                        \"type\": \"PiecewiseLinearEmbeddings\",\n",
    "                        \"d_embedding\": 16,\n",
    "                        \"activation\": False,\n",
    "                        \"version\": \"B\",\n",
    "                    }\n",
    "                ),\n",
    "                arch_type=arch_type,\n",
    "                k=32,\n",
    "                share_training_batches=True,\n",
    "            ).to(self.device)\n",
    "\n",
    "        else:\n",
    "            if len(y.shape) > 1 and y.shape[1] > 1:\n",
    "                # Multiclass logits\n",
    "                n_outputs = y.shape[1]\n",
    "            else:\n",
    "                # Binary or single-output regression\n",
    "                n_outputs = 1\n",
    "\n",
    "            # Initialize model\n",
    "            compile_model = False\n",
    "            self.model = Model(\n",
    "                n_num_features=n_features,\n",
    "                cat_cardinalities=[],  # Assumes all features are continuous\n",
    "                n_classes=n_outputs,\n",
    "                backbone={\n",
    "                    \"type\": \"MLP\",\n",
    "                    \"n_blocks\": n_blocks,\n",
    "                    \"d_block\": d_block,\n",
    "                    \"dropout\": dropout,\n",
    "                },\n",
    "                bins=bins,\n",
    "                num_embeddings=(\n",
    "                    None\n",
    "                    if bins is None\n",
    "                    else {\n",
    "                        \"type\": \"PiecewiseLinearEmbeddings\",\n",
    "                        \"d_embedding\": 16,\n",
    "                        \"activation\": False,\n",
    "                        \"version\": \"B\",\n",
    "                    }\n",
    "                ),\n",
    "                arch_type=arch_type,\n",
    "                k=32,\n",
    "                share_training_batches=True,\n",
    "            ).to(self.device)\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            make_parameter_groups(self.model), lr=lr, weight_decay=weight_decay\n",
    "        )\n",
    "        self.evaluation_mode = torch.no_grad if compile_model else torch.inference_mode\n",
    "\n",
    "        # Prepare data\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        if self.task_type == \"binary\":\n",
    "            y_tensor = torch.tensor(y, dtype=torch.long, device=self.device)\n",
    "        else:\n",
    "            y_tensor = torch.tensor(y, dtype=torch.float32, device=self.device)\n",
    "        n_samples = X_tensor.size(0)\n",
    "        indices = torch.randperm(n_samples)\n",
    "        train_end = int(0.8 * n_samples)\n",
    "        train_idx, val_idx = indices[:train_end], indices[train_end:]\n",
    "        self.data = {\n",
    "            \"train\": {\"x_cont\": X_tensor[train_idx], \"y\": y_tensor[train_idx]},\n",
    "            \"val\": {\"x_cont\": X_tensor[val_idx], \"y\": y_tensor[val_idx]},\n",
    "        }\n",
    "        Y_train = self.data[\"train\"][\"y\"]\n",
    "\n",
    "        # Training parameters\n",
    "        n_epochs = 100000\n",
    "        patience = 16\n",
    "        batch_size = 256\n",
    "        train_size = len(Y_train)\n",
    "        best = {\"val\": -float(\"inf\"), \"epoch\": -1}\n",
    "        remaining_patience = patience\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(n_epochs):\n",
    "            batches = torch.randperm(train_size, device=self.device).split(batch_size)\n",
    "            for batch_idx in batches:\n",
    "                self.model.train()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.loss_fn(\n",
    "                    self.apply_model(\"train\", batch_idx), Y_train[batch_idx]\n",
    "                )\n",
    "                if self.amp_enabled and self.amp_dtype == torch.float16:\n",
    "                    scaler = torch.cuda.amp.GradScaler()\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(self.optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "            val_score = self.evaluate_tabm(\"val\")\n",
    "            if val_score > best[\"val\"]:\n",
    "                best = {\"val\": val_score, \"epoch\": epoch}\n",
    "                remaining_patience = patience\n",
    "            else:\n",
    "                remaining_patience -= 1\n",
    "            if remaining_patience < 0:\n",
    "                break\n",
    "\n",
    "    def apply_model(self, part: str, idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply the model to a batch of data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        part : str\n",
    "            The data partition to use ('train' or 'val').\n",
    "        idx : torch.Tensor\n",
    "            The indices of the samples to use.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The model output.\n",
    "        \"\"\"\n",
    "        with torch.autocast(\n",
    "            self.device.type, enabled=self.amp_enabled, dtype=self.amp_dtype\n",
    "        ):\n",
    "            x_cont = self.data[part][\"x_cont\"][idx]\n",
    "            x_cat = self.data[part].get(\"x_cat\")\n",
    "            if self.task_type == \"binary\":\n",
    "                return self.model(x_cont, x_cat).float()\n",
    "            else:\n",
    "                return (\n",
    "                    self.model(x_cont, x_cat)\n",
    "                    .squeeze(-1)  # Remove the last dimension for regression tasks\n",
    "                    .float()\n",
    "                )\n",
    "\n",
    "    def loss_fn(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : torch.Tensor\n",
    "            The predicted outputs.\n",
    "        y_true : torch.Tensor\n",
    "            The ground truth.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The loss value.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            # Use the second-to-last dimension for averaging over k heads.\n",
    "            k = y_pred.shape[-2]\n",
    "            target = y_true.repeat_interleave(k)\n",
    "            return F.cross_entropy(y_pred.flatten(0, 1), target)\n",
    "        else:\n",
    "            # TabM produces k predictions. Each of them must be trained separately.\n",
    "            # For student model (regression on logits), y_pred.shape == (batch_size, k)\n",
    "            k = y_pred.shape[-1 if len(y_pred.shape) <= 2 else -2]\n",
    "\n",
    "            # Flatten the predictions and repeat the targets\n",
    "            return F.mse_loss(\n",
    "                y_pred.flatten(0, 1),\n",
    "                (\n",
    "                    y_true.repeat_interleave(k)\n",
    "                    if self.model.share_training_batches\n",
    "                    else y_true\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def evaluate_tabm(self, part: str) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the model on a data partition.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        part : str\n",
    "            The data partition to use ('train' or 'val').\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The accuracy score.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        with self.evaluation_mode():\n",
    "            eval_batch_size = 8096\n",
    "\n",
    "            if self.task_type == \"binary\":\n",
    "                indices = torch.arange(\n",
    "                    len(self.data[part][\"y\"]), device=self.device\n",
    "                ).split(eval_batch_size)\n",
    "                y_pred = torch.cat(\n",
    "                    [self.apply_model(part, idx) for idx in indices]\n",
    "                ).cpu()\n",
    "                y_pred = torch.softmax(y_pred, dim=-1).mean(1)\n",
    "                y_true = self.data[part][\"y\"].cpu().numpy()\n",
    "                return float(accuracy_score(y_true, y_pred.argmax(1)))\n",
    "            else:\n",
    "                y_pred = (\n",
    "                    torch.cat(\n",
    "                        [\n",
    "                            self.apply_model(part, idx)\n",
    "                            for idx in torch.arange(\n",
    "                                len(self.data[part][\"y\"]), device=self.device\n",
    "                            ).split(eval_batch_size)\n",
    "                        ]\n",
    "                    )\n",
    "                    .cpu()\n",
    "                    .numpy()\n",
    "                )\n",
    "\n",
    "                # Transform the predictions back to the original label space\n",
    "                if hasattr(self, \"regression_label_stats\"):\n",
    "                    y_pred = (\n",
    "                        y_pred * self.regression_label_stats[\"std\"]\n",
    "                        + self.regression_label_stats[\"mean\"]\n",
    "                    )\n",
    "\n",
    "                # Compute the mean of the k predictions\n",
    "                y_pred = y_pred.mean(1)\n",
    "\n",
    "                y_true = self.data[part][\"y\"].cpu().numpy()\n",
    "\n",
    "                # For student model, we want to minimize MSE, so return negative MSE\n",
    "                # The higher the value, the better (consistent with example.ipynb)\n",
    "                score = -(mean_squared_error(y_true, y_pred) ** 0.5)\n",
    "                return float(score)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on the given data using the trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array-like of shape (n_samples,)\n",
    "            The predicted class indices.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            self.model.eval()\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(X_tensor)\n",
    "                # Since share_training_batches is True, average over the second dimension.\n",
    "                probs = torch.softmax(logits, dim=-1).mean(1)\n",
    "                return probs.cpu().numpy()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "            with torch.no_grad():\n",
    "                # TabM produces output with shape (batch_size, k) or (batch_size, k, n_outputs)\n",
    "                logits = self.model(X_tensor)\n",
    "\n",
    "                # For student model (regression), we need to average over the k predictions\n",
    "                # Check the dimensions of logits and average appropriately\n",
    "                if logits.ndim > 2:  # Case: (batch_size, k, n_outputs)\n",
    "                    logits = logits.mean(1)  # Average over k dimension\n",
    "                elif (\n",
    "                    logits.shape[1] > 1\n",
    "                    and hasattr(self.model, \"k\")\n",
    "                    and self.model.k > 1\n",
    "                ):\n",
    "                    # Case: (batch_size, k) for single output regression\n",
    "                    logits = logits.mean(\n",
    "                        1, keepdim=True\n",
    "                    )  # Average and keep output dimension\n",
    "\n",
    "                return logits.cpu().numpy().squeeze(-1)  # Squeeze to remove last dimension if needed\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"\n",
    "        Count the number of trainable parameters in the model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The total number of trainable parameters.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"model\"):\n",
    "            raise ValueError(\"The model has not been trained yet\")\n",
    "        return sum(p.numel() for p in self.model.parameters() if p.requires_grad)    \n",
    "\n",
    "\n",
    "class MLPStudentModel(StudentModelBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a TabM teacher model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments to pass to the base class constructor.\n",
    "        \"\"\"\n",
    "        self.device = kwargs.pop(\"device\")\n",
    "        self.config = kwargs.pop(\"config\")\n",
    "        self.task_type = kwargs.pop(\"task_type\")\n",
    "        self.hyperparams = kwargs.pop(\"hyperparams\")\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Set up AMP if CUDA is available.\n",
    "        if torch.cuda.is_available():\n",
    "            self.amp_dtype = (\n",
    "                torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "            )\n",
    "            self.amp_enabled = True\n",
    "        else:\n",
    "            self.amp_dtype = None\n",
    "            self.amp_enabled = False\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the TabM model using the provided data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target data.\n",
    "        **training_params : dict\n",
    "            Additional training parameters to pass to the TabM model's fit method.\n",
    "        \"\"\"\n",
    "        # Get parameters\n",
    "        n_blocks = self.hyperparams.get(\"n_blocks\")\n",
    "        d_block = self.hyperparams.get(\"d_block\")\n",
    "        dropout = self.hyperparams.get(\"dropout\")\n",
    "        lr = self.hyperparams.get(\"learning_rate\")\n",
    "        weight_decay = self.hyperparams.get(\"weight_decay\")\n",
    "\n",
    "        # Set model architecture\n",
    "        arch_type = \"plain\"\n",
    "        bins = None\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        if self.task_type == \"binary\":\n",
    "            n_classes = len(np.unique(y))\n",
    "            self.n_classes = n_classes\n",
    "            cat_idx = []  # Assumes all features are continuous\n",
    "\n",
    "            # Initialize model\n",
    "            compile_model = False\n",
    "            self.model = Model(\n",
    "                n_num_features=n_features,\n",
    "                cat_cardinalities=cat_idx,\n",
    "                n_classes=n_classes,\n",
    "                backbone={\n",
    "                    \"type\": \"MLP\",\n",
    "                    \"n_blocks\": n_blocks,\n",
    "                    \"d_block\": d_block,\n",
    "                    \"dropout\": dropout,\n",
    "                },\n",
    "                bins=bins,\n",
    "                num_embeddings=(\n",
    "                    None\n",
    "                    if bins is None\n",
    "                    else {\n",
    "                        \"type\": \"PiecewiseLinearEmbeddings\",\n",
    "                        \"d_embedding\": 16,\n",
    "                        \"activation\": False,\n",
    "                        \"version\": \"B\",\n",
    "                    }\n",
    "                ),\n",
    "                arch_type=arch_type,\n",
    "                k=None,\n",
    "                share_training_batches=True,\n",
    "            ).to(self.device)\n",
    "\n",
    "        else:\n",
    "            if len(y.shape) > 1 and y.shape[1] > 1:\n",
    "                # Multiclass logits\n",
    "                n_outputs = y.shape[1]\n",
    "            else:\n",
    "                # Binary or single-output regression\n",
    "                n_outputs = 1\n",
    "\n",
    "            # Initialize model\n",
    "            compile_model = False\n",
    "            self.model = Model(\n",
    "                n_num_features=n_features,\n",
    "                cat_cardinalities=[],  # Assumes all features are continuous\n",
    "                n_classes=n_outputs,\n",
    "                backbone={\n",
    "                    \"type\": \"MLP\",\n",
    "                    \"n_blocks\": n_blocks,\n",
    "                    \"d_block\": d_block,\n",
    "                    \"dropout\": dropout,\n",
    "                },\n",
    "                bins=bins,\n",
    "                num_embeddings=(\n",
    "                    None\n",
    "                    if bins is None\n",
    "                    else {\n",
    "                        \"type\": \"PiecewiseLinearEmbeddings\",\n",
    "                        \"d_embedding\": 16,\n",
    "                        \"activation\": False,\n",
    "                        \"version\": \"B\",\n",
    "                    }\n",
    "                ),\n",
    "                arch_type=arch_type,\n",
    "                k=None,\n",
    "                share_training_batches=True,\n",
    "            ).to(self.device)\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            make_parameter_groups(self.model), lr=lr, weight_decay=weight_decay\n",
    "        )\n",
    "        self.evaluation_mode = torch.no_grad if compile_model else torch.inference_mode\n",
    "\n",
    "        # Prepare data\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        if self.task_type == \"binary\":\n",
    "            y_tensor = torch.tensor(y, dtype=torch.long, device=self.device)\n",
    "        else:\n",
    "            y_tensor = torch.tensor(y, dtype=torch.float32, device=self.device)\n",
    "        n_samples = X_tensor.size(0)\n",
    "        indices = torch.randperm(n_samples)\n",
    "        train_end = int(0.8 * n_samples)\n",
    "        train_idx, val_idx = indices[:train_end], indices[train_end:]\n",
    "        self.data = {\n",
    "            \"train\": {\"x_cont\": X_tensor[train_idx], \"y\": y_tensor[train_idx]},\n",
    "            \"val\": {\"x_cont\": X_tensor[val_idx], \"y\": y_tensor[val_idx]},\n",
    "        }\n",
    "        Y_train = self.data[\"train\"][\"y\"]\n",
    "\n",
    "        # Training parameters\n",
    "        n_epochs = 100000\n",
    "        patience = 16\n",
    "        batch_size = 256\n",
    "        train_size = len(Y_train)\n",
    "        best = {\"val\": -float(\"inf\"), \"epoch\": -1}\n",
    "        remaining_patience = patience\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(n_epochs):\n",
    "            batches = torch.randperm(train_size, device=self.device).split(batch_size)\n",
    "            for batch_idx in batches:\n",
    "                self.model.train()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.loss_fn(\n",
    "                    self.apply_model(\"train\", batch_idx), Y_train[batch_idx]\n",
    "                )\n",
    "                if self.amp_enabled and self.amp_dtype == torch.float16:\n",
    "                    scaler = torch.cuda.amp.GradScaler()\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(self.optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "            val_score = self.evaluate_tabm(\"val\")\n",
    "            if val_score > best[\"val\"]:\n",
    "                best = {\"val\": val_score, \"epoch\": epoch}\n",
    "                remaining_patience = patience\n",
    "            else:\n",
    "                remaining_patience -= 1\n",
    "            if remaining_patience < 0:\n",
    "                break\n",
    "\n",
    "    def apply_model(self, part: str, idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply the model to a batch of data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        part : str\n",
    "            The data partition to use ('train' or 'val').\n",
    "        idx : torch.Tensor\n",
    "            The indices of the samples to use.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The model output.\n",
    "        \"\"\"\n",
    "        with torch.autocast(\n",
    "            self.device.type, enabled=self.amp_enabled, dtype=self.amp_dtype\n",
    "        ):\n",
    "            x_cont = self.data[part][\"x_cont\"][idx]\n",
    "            x_cat = self.data[part].get(\"x_cat\")\n",
    "            if self.task_type == \"binary\":\n",
    "                return self.model(x_cont, x_cat).float()\n",
    "            else:\n",
    "                return (\n",
    "                    self.model(x_cont, x_cat)\n",
    "                    .squeeze(-1)  # Remove the last dimension for regression tasks\n",
    "                    .float()\n",
    "                )\n",
    "\n",
    "    def loss_fn(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : torch.Tensor\n",
    "            The predicted outputs.\n",
    "        y_true : torch.Tensor\n",
    "            The ground truth.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The loss value.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            # Use the second-to-last dimension for averaging over k heads.\n",
    "            k = y_pred.shape[-2]\n",
    "            target = y_true.repeat_interleave(k)\n",
    "            return F.cross_entropy(y_pred.flatten(0, 1), target)\n",
    "        else:\n",
    "            # TabM produces k predictions. Each of them must be trained separately.\n",
    "            # For student model (regression on logits), y_pred.shape == (batch_size, k)\n",
    "            k = y_pred.shape[-1 if len(y_pred.shape) <= 2 else -2]\n",
    "\n",
    "            # Flatten the predictions and repeat the targets\n",
    "            return F.mse_loss(\n",
    "                y_pred.flatten(0, 1),\n",
    "                (\n",
    "                    y_true.repeat_interleave(k)\n",
    "                    if self.model.share_training_batches\n",
    "                    else y_true\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def evaluate_tabm(self, part: str) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the model on a data partition.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        part : str\n",
    "            The data partition to use ('train' or 'val').\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The accuracy score.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        with self.evaluation_mode():\n",
    "            eval_batch_size = 8096\n",
    "\n",
    "            if self.task_type == \"binary\":\n",
    "                indices = torch.arange(\n",
    "                    len(self.data[part][\"y\"]), device=self.device\n",
    "                ).split(eval_batch_size)\n",
    "                y_pred = torch.cat(\n",
    "                    [self.apply_model(part, idx) for idx in indices]\n",
    "                ).cpu()\n",
    "                y_pred = torch.softmax(y_pred, dim=-1).mean(1)\n",
    "                y_true = self.data[part][\"y\"].cpu().numpy()\n",
    "                return float(accuracy_score(y_true, y_pred.argmax(1)))\n",
    "            else:\n",
    "                y_pred = (\n",
    "                    torch.cat(\n",
    "                        [\n",
    "                            self.apply_model(part, idx)\n",
    "                            for idx in torch.arange(\n",
    "                                len(self.data[part][\"y\"]), device=self.device\n",
    "                            ).split(eval_batch_size)\n",
    "                        ]\n",
    "                    )\n",
    "                    .cpu()\n",
    "                    .numpy()\n",
    "                )\n",
    "\n",
    "                # Transform the predictions back to the original label space\n",
    "                if hasattr(self, \"regression_label_stats\"):\n",
    "                    y_pred = (\n",
    "                        y_pred * self.regression_label_stats[\"std\"]\n",
    "                        + self.regression_label_stats[\"mean\"]\n",
    "                    )\n",
    "\n",
    "                # Compute the mean of the k predictions\n",
    "                y_pred = y_pred.mean(1)\n",
    "\n",
    "                y_true = self.data[part][\"y\"].cpu().numpy()\n",
    "\n",
    "                # For student model, we want to minimize MSE, so return negative MSE\n",
    "                # The higher the value, the better (consistent with example.ipynb)\n",
    "                score = -(mean_squared_error(y_true, y_pred) ** 0.5)\n",
    "                return float(score)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on the given data using the trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array-like of shape (n_samples,)\n",
    "            The predicted class indices.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            self.model.eval()\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(X_tensor)\n",
    "                # Since share_training_batches is True, average over the second dimension.\n",
    "                probs = torch.softmax(logits, dim=-1).mean(1)\n",
    "                return probs.cpu().numpy()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "            with torch.no_grad():\n",
    "                # TabM produces output with shape (batch_size, k) or (batch_size, k, n_outputs)\n",
    "                logits = self.model(X_tensor)\n",
    "\n",
    "                # For student model (regression), we need to average over the k predictions\n",
    "                # Check the dimensions of logits and average appropriately\n",
    "                if logits.ndim > 2:  # Case: (batch_size, k, n_outputs)\n",
    "                    logits = logits.mean(1)  # Average over k dimension\n",
    "                elif (\n",
    "                    logits.shape[1] > 1\n",
    "                    and hasattr(self.model, \"k\")\n",
    "                    and self.model.k > 1\n",
    "                ):\n",
    "                    # Case: (batch_size, k) for single output regression\n",
    "                    logits = logits.mean(\n",
    "                        1, keepdim=True\n",
    "                    )  # Average and keep output dimension\n",
    "\n",
    "                return logits.cpu().numpy().squeeze(-1)  # Squeeze to remove last dimension if needed\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"\n",
    "        Count the number of trainable parameters in the model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The total number of trainable parameters.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"model\"):\n",
    "            raise ValueError(\"The model has not been trained yet\")\n",
    "        return sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "class RandomForestStudentModel(StudentModelBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a RandomForest teacher model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments to pass to the base class constructor.\n",
    "        \"\"\"\n",
    "        self.device = kwargs.pop(\"device\")\n",
    "        self.config = kwargs.pop(\"config\")\n",
    "        self.task_type = kwargs.pop(\"task_type\")\n",
    "        self.hyperparams = kwargs.pop(\"hyperparams\")\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the RandomForest teacher model using the provided data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target data.\n",
    "        **training_params : dict\n",
    "            Additional training parameters (not used for RandomForest).\n",
    "        \"\"\"\n",
    "        config = self.config\n",
    "        random_state = config[\"training\"][\"random_state\"]\n",
    "\n",
    "        if self.task_type == \"binary\":\n",
    "            # Initialize the RandomForest model with the provided hyperparameters\n",
    "            self.model = RandomForestClassifier(\n",
    "                random_state=random_state,\n",
    "                n_jobs=-1,  # Use all available CPU cores\n",
    "                **self.hyperparams,\n",
    "            )\n",
    "        else:\n",
    "            self.model = RandomForestRegressor(\n",
    "                random_state=random_state,\n",
    "                n_jobs=-1,  # Use all available CPU cores\n",
    "                **self.hyperparams,\n",
    "            )\n",
    "\n",
    "        # Train the model\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make probability predictions on the given data using the trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array-like of shape (n_samples, n_classes)\n",
    "            The predicted probabilities for each class.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            return self.model.predict(X)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"\n",
    "        Count the number of parameters in the trained RandomForest model.\n",
    "\n",
    "        For each tree, we count:\n",
    "        - The feature indices at each node (1 parameter per node except leaf nodes)\n",
    "        - The thresholds at each node (1 parameter per node except leaf nodes)\n",
    "        - The values at each leaf node (1 parameter per leaf node)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The number of parameters in the model.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"model\") or self.model is None:\n",
    "            raise ValueError(\"The model has not been trained yet.\")\n",
    "\n",
    "        n_params = 0\n",
    "        for tree in self.model.estimators_:\n",
    "            tree = tree.tree_\n",
    "            # Number of nodes in the tree\n",
    "            n_nodes = tree.node_count\n",
    "            # Number of leaf nodes\n",
    "            n_leaves = tree.n_leaves\n",
    "            # Number of internal nodes\n",
    "            n_internal = n_nodes - n_leaves\n",
    "            # Each internal node has a feature index and a threshold\n",
    "            n_params += 2 * n_internal\n",
    "\n",
    "            # Each leaf node has value(s) - depends on task type\n",
    "            if self.task_type == \"binary\":\n",
    "                # For classification, each leaf stores class distribution\n",
    "                n_params += n_leaves * self.model.n_classes_\n",
    "            else:\n",
    "                # For regression, each leaf stores a single value\n",
    "                n_params += n_leaves\n",
    "\n",
    "        return n_params\n",
    "    \n",
    "def get_student_model(config: dict, task_type: Literal[\"binary\", \"regression\"], device, hyperparams, cat_cols=None) -> StudentModelBase:\n",
    "    \"\"\"\n",
    "    Returns an instance of the teacher model based on the configuration.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict\n",
    "        Configuration dictionary containing model and dataset details.\n",
    "    device : torch.device\n",
    "        The device to use for training (CPU or GPU).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TeacherModelBase\n",
    "        An instance of the teacher model.\n",
    "    \"\"\"\n",
    "    model_type = config[\"model\"][\"student_model\"]\n",
    "    if model_type == \"catboost\":\n",
    "        return CatBoostStudentModel(config=config, task_type=task_type, device=device, hyperparams=hyperparams)\n",
    "    elif model_type == \"grande\":\n",
    "        return GRANDEStudentModel(config=config, task_type=task_type, device=device, hyperparams=hyperparams, cat_cols=cat_cols)\n",
    "    elif model_type == \"tabm\":\n",
    "        return TabMStudentModel(config=config, task_type=task_type, device=device, hyperparams=hyperparams)\n",
    "    elif model_type == \"mlp\":\n",
    "        return MLPStudentModel(config=config, task_type=task_type, device=device, hyperparams=hyperparams)\n",
    "    elif model_type == \"random_forest\":\n",
    "        return RandomForestStudentModel(config=config, task_type=task_type, device=device, hyperparams=hyperparams)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown teacher type: {config['model']['student_model']}. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725d7a3c",
   "metadata": {},
   "source": [
    "# HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab890760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_hyperparameters(trial: optuna.Trial, model_type: str, task_type: str, use_hpo: bool) -> dict:\n",
    "    \"\"\"\n",
    "    Suggest hyperparameters for the given model type using Optuna.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    trial : optuna.Trial\n",
    "        Optuna trial object for suggesting hyperparameters.\n",
    "    model_type : str\n",
    "        Type of model ('tabpfn', 'catboost', etc.).\n",
    "    task_type : str\n",
    "        Type of task ('binary', 'regression').\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of suggested hyperparameters.\n",
    "    \"\"\"\n",
    "    hyperparameter = {}\n",
    "    \n",
    "    if model_type == \"catboost\":\n",
    "        if use_hpo:\n",
    "            hyperparameter = {\n",
    "                \"iterations\": trial.suggest_int(\"iterations\", 512, 4096),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 2, 12),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "                \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 0.5, 30),\n",
    "                \"boosting_type\": \"Plain\",\n",
    "            }\n",
    "        else:\n",
    "            hyperparameter = {\n",
    "                \"iterations\": 1000,\n",
    "                \"max_depth\": 6,\n",
    "                \"l2_leaf_reg\": 3,\n",
    "                \"boosting_type\": \"Plain\",\n",
    "            }\n",
    "        if task_type == \"binary\":\n",
    "            hyperparameter[\"loss_function\"] = \"Logloss\"\n",
    "        else:\n",
    "            hyperparameter[\"loss_function\"] = \"RMSE\"\n",
    "    \n",
    "    elif model_type == \"grande\":\n",
    "        if use_hpo:\n",
    "            hyperparameter = {\n",
    "                \"depth\": trial.suggest_int(\"depth\", 3, 7),\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 512, 4096),\n",
    "                \"learning_rate_weights\": trial.suggest_float(\"learning_rate_weights\", 0.0001, 0.05, log=True),\n",
    "                \"learning_rate_index\": trial.suggest_float(\"learning_rate_index\", 0.001, 0.2, log=True),\n",
    "                \"learning_rate_values\": trial.suggest_float(\"learning_rate_values\", 0.001, 0.2, log=True),\n",
    "                \"learning_rate_leaf\": trial.suggest_float(\"learning_rate_leaf\", 0.001, 0.2, log=True),\n",
    "                \"optimizer\": \"adam\",\n",
    "                \"cosine_decay_steps\": trial.suggest_categorical(\"cosine_decay_steps\", [0.0, 0.1, 1.0, 100.0, 1000.0]),\n",
    "                \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.75),\n",
    "                \"selected_variables\": trial.suggest_float(\"selected_variables\", 0.0, 1.0),\n",
    "                \"data_subset_fraction\": trial.suggest_float(\"data_subset_fraction\", 0.1, 1.0),\n",
    "                \"focal_loss\": False,\n",
    "                \"temperature\": 0.0,\n",
    "                \"from_logits\": True,\n",
    "                \"use_class_weights\": True,\n",
    "            }\n",
    "        else:\n",
    "            hyperparameter = {\n",
    "                \"depth\": 5,\n",
    "                \"n_estimators\": 2048,\n",
    "                \"learning_rate_weights\": 0.005,\n",
    "                \"learning_rate_index\": 0.01,\n",
    "                \"learning_rate_values\": 0.01,\n",
    "                \"learning_rate_leaf\": 0.01,\n",
    "                \"optimizer\": \"adam\",\n",
    "                \"cosine_decay_steps\": 0,\n",
    "                \"dropout\": 0.0,\n",
    "                \"selected_variables\": 0.8,\n",
    "                \"data_subset_fraction\": 1.0,\n",
    "                \"focal_loss\": False,\n",
    "                \"temperature\": 0.0,\n",
    "                \"from_logits\": True,\n",
    "                \"use_class_weights\": True,\n",
    "            }\n",
    "        if task_type == \"binary\":\n",
    "            hyperparameter[\"loss\"] = \"crossentropy\"\n",
    "        else:\n",
    "            hyperparameter[\"loss\"] = \"mse\"\n",
    "\n",
    "    elif model_type == \"tabm\":\n",
    "        if use_hpo:\n",
    "            hyperparameter = {\n",
    "                \"n_blocks\": trial.suggest_int(\"n_blocks\", 1, 5),\n",
    "                \"d_block\": trial.suggest_int(\"d_block\", 64, 1024),\n",
    "                \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.5),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.0001, 0.005, log=True),\n",
    "                \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0001, 0.1, log=True),\n",
    "            }\n",
    "        else:\n",
    "            hyperparameter = {\n",
    "                \"n_blocks\": 3,\n",
    "                \"d_block\": 512,\n",
    "                \"dropout\": 0.1,\n",
    "                \"learning_rate\": 0.002,\n",
    "                \"weight_decay\": 0.0003,\n",
    "            }\n",
    "\n",
    "    elif model_type == \"mlp\":\n",
    "        if use_hpo:\n",
    "            hyperparameter = {\n",
    "                \"n_blocks\": trial.suggest_int(\"n_blocks\", 1, 5),\n",
    "                \"d_block\": trial.suggest_int(\"d_block\", 64, 1024),\n",
    "                \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.5),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.0001, 0.005, log=True),\n",
    "                \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0001, 0.1, log=True),\n",
    "            }\n",
    "        else:\n",
    "            hyperparameter = {\n",
    "                \"n_blocks\": 3,\n",
    "                \"d_block\": 512,\n",
    "                \"dropout\": 0.1,\n",
    "                \"learning_rate\": 0.002,\n",
    "                \"weight_decay\": 0.0003,\n",
    "            }\n",
    "\n",
    "    elif model_type == \"random_forest\":\n",
    "        if use_hpo:\n",
    "            hyperparameter = {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 4096),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 5, 100),\n",
    "                \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "                \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "                \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", 0.3, 0.5, 0.7, 1.0]),\n",
    "                \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "            }\n",
    "            if task_type == \"binary\":\n",
    "                hyperparameter[\"criterion\"] = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n",
    "            else:\n",
    "                hyperparameter[\"criterion\"] = trial.suggest_categorical(\"criterion\", [\"squared_error\", \"absolute_error\", \"poisson\", \"friedman_mse\"])\n",
    "        else:\n",
    "            hyperparameter = {\n",
    "                \"n_estimators\": 1000,\n",
    "                \"max_depth\": None,\n",
    "                \"min_samples_split\": 2,\n",
    "                \"min_samples_leaf\": 1,\n",
    "            }\n",
    "            if task_type == \"binary\":\n",
    "                hyperparameter[\"criterion\"] = \"gini\"\n",
    "                hyperparameter[\"max_features\"] = \"sqrt\"\n",
    "            else:\n",
    "                hyperparameter[\"criterion\"] = \"squared_error\"\n",
    "                hyperparameter[\"max_features\"] = 1.0\n",
    "\n",
    "    return hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02857a",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59b406d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data\": {\n",
    "        \"datasets\": [\n",
    "                     23381, # Binary classification datasets\n",
    "                       197, # Regression datasets\n",
    "                     ],  \n",
    "        \"cache_dir_path\": \"data/cache/\",\n",
    "        \"fold_indices_path\": \"data/fold_indices/\",\n",
    "        \"optuna_db_path\": \"data/optuna_db/\",\n",
    "        \"output_dir_path\": \"data/output/\",\n",
    "        \"outer_folds_path\": \"data/outer_fold/\",\n",
    "        \"results_dir_path\": \"results/\",\n",
    "    },\n",
    "\n",
    "    \"preprocessing\": {\n",
    "        \"threshold_high_cardinality\": 10,  \n",
    "    },\n",
    "\n",
    "    \"model\": {\n",
    "        \"teacher_model\": \"tabpfn\",   # Options: 'tabpfn', 'grande', 'catboost', 'tabm', 'mlp', 'random_forest'\n",
    "        \"student_model\": \"grande\", # Options: 'grande', 'catboost', 'tabm', 'mlp', 'random_forest'\n",
    "    },\n",
    "\n",
    "    \"training\": {\n",
    "        \"use_hpo\": True,\n",
    "        \"train_on_logits\": True,\n",
    "        \"trials\": 5,\n",
    "        \"random_state\": 42,\n",
    "        \"outer_folds\": 5,\n",
    "        \"inner_folds\": 2,\n",
    "    },\n",
    "\n",
    "    \"teacher_models\": {\n",
    "        \"tabpfn\": {\n",
    "            \"preprocessing\": \"nn\",  \n",
    "        },\n",
    "        \"catboost\": {\n",
    "            \"preprocessing\": \"tree\", \n",
    "        },\n",
    "        \"grande\": {\n",
    "            \"preprocessing\": \"grande\",  \n",
    "        },\n",
    "        \"tabm\": {\n",
    "            \"preprocessing\": \"nn\",  \n",
    "        },\n",
    "        \"mlp\": {\n",
    "            \"preprocessing\": \"nn\",  \n",
    "        },\n",
    "        \"random_forest\": {\n",
    "            \"preprocessing\": \"tree\",  \n",
    "        },\n",
    "    },\n",
    "\n",
    "    \"student_models\": {\n",
    "        \"catboost\": {\n",
    "            \"preprocessing\": \"tree\",  \n",
    "        },\n",
    "        \"grande\": {\n",
    "            \"preprocessing\": \"grande\",  \n",
    "        },\n",
    "        \"tabm\": {\n",
    "            \"preprocessing\": \"nn\",  \n",
    "        },\n",
    "        \"mlp\": {\n",
    "            \"preprocessing\": \"nn\",  \n",
    "        },\n",
    "        \"random_forest\": {\n",
    "            \"preprocessing\": \"tree\",  \n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17729927",
   "metadata": {},
   "source": [
    "# Train Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9dce1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:33:31,467 - __main__ - INFO - Loading configuration...\n",
      "2025-06-06 11:33:31,468 - __main__ - INFO - Loading dataset 23381 from cache at 'data/cache/openml_dataset_23381.pkl'...\n",
      "2025-06-06 11:33:31,470 - __main__ - INFO - Dataset 23381 loaded successfully with task type: binary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:33:31,622 - __main__ - INFO - Using GPU: NVIDIA GeForce RTX 2080 Ti\n",
      "2025-06-06 11:33:31,635 - __main__ - INFO - Random seed set to 42\n",
      "2025-06-06 11:33:31,637 - __main__ - INFO - Outer Fold 1 - Train Indices: [ 1  3  5  6  7  8  9 10 11 12], Test Indices: [ 0  2  4 18 19 27 35 39 57 59]\n",
      "2025-06-06 11:33:31,637 - __main__ - INFO - -------------------- Outer Fold 1 --------------------\n",
      "2025-06-06 11:33:31,639 - __main__ - INFO - Starting hyperparameter optimization for outer fold 1...\n",
      "[I 2025-06-06 11:33:32,544] A new study created in RDB with name: 23381.1.tabpfn\n",
      "2025-06-06 11:33:32,603 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 2, 3, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:34,475 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-06 11:33:38,036 - __main__ - INFO - \t Balanced Accuracy: 0.5881, F1 Score: 0.5807, ROC AUC: 0.6429\n",
      "2025-06-06 11:33:38,051 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 1, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:39,518 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-06 11:33:40,089 - __main__ - INFO - \t Balanced Accuracy: 0.5300, F1 Score: 0.4800, ROC AUC: 0.6055\n",
      "[I 2025-06-06 11:33:40,175] Trial 0 finished with value: 0.5303360998631095 and parameters: {}. Best is trial 0 with value: 0.5303360998631095.\n",
      "2025-06-06 11:33:40,201 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 2, 3, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:40,923 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-06 11:33:41,467 - __main__ - INFO - \t Balanced Accuracy: 0.5881, F1 Score: 0.5807, ROC AUC: 0.6429\n",
      "2025-06-06 11:33:41,478 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 1, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:41,526 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-06 11:33:42,069 - __main__ - INFO - \t Balanced Accuracy: 0.5300, F1 Score: 0.4800, ROC AUC: 0.6055\n",
      "[I 2025-06-06 11:33:42,141] Trial 1 finished with value: 0.5303360998631095 and parameters: {}. Best is trial 0 with value: 0.5303360998631095.\n",
      "2025-06-06 11:33:42,166 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 2, 3, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:42,213 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-06 11:33:42,757 - __main__ - INFO - \t Balanced Accuracy: 0.5881, F1 Score: 0.5807, ROC AUC: 0.6429\n",
      "2025-06-06 11:33:42,769 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 1, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:42,816 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-06 11:33:43,358 - __main__ - INFO - \t Balanced Accuracy: 0.5300, F1 Score: 0.4800, ROC AUC: 0.6055\n",
      "[I 2025-06-06 11:33:43,437] Trial 2 finished with value: 0.5303360998631095 and parameters: {}. Best is trial 0 with value: 0.5303360998631095.\n",
      "2025-06-06 11:33:43,463 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 2, 3, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:43,509 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-06 11:33:44,054 - __main__ - INFO - \t Balanced Accuracy: 0.5881, F1 Score: 0.5807, ROC AUC: 0.6429\n",
      "2025-06-06 11:33:44,066 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 1, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:44,113 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-06 11:33:44,653 - __main__ - INFO - \t Balanced Accuracy: 0.5300, F1 Score: 0.4800, ROC AUC: 0.6055\n",
      "[I 2025-06-06 11:33:44,731] Trial 3 finished with value: 0.5303360998631095 and parameters: {}. Best is trial 0 with value: 0.5303360998631095.\n",
      "2025-06-06 11:33:44,758 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 2, 3, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:44,804 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-06 11:33:45,355 - __main__ - INFO - \t Balanced Accuracy: 0.5881, F1 Score: 0.5807, ROC AUC: 0.6429\n",
      "2025-06-06 11:33:45,367 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 1, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:45,414 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-06 11:33:45,952 - __main__ - INFO - \t Balanced Accuracy: 0.5300, F1 Score: 0.4800, ROC AUC: 0.6055\n",
      "[I 2025-06-06 11:33:46,027] Trial 4 finished with value: 0.5303360998631095 and parameters: {}. Best is trial 0 with value: 0.5303360998631095.\n",
      "2025-06-06 11:33:46,037 - __main__ - INFO - Best hyperparameters for fold 1: {}\n",
      "2025-06-06 11:33:46,046 - __main__ - INFO - Best score: 0.5303\n",
      "2025-06-06 11:33:46,047 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [4, 5, 7, 9] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:46,094 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-06 11:33:46,094 - __main__ - INFO - Retraining Model on Outer Fold 1\n",
      "2025-06-06 11:33:46,929 - __main__ - INFO - \t Inference Time: 0.32540 seconds\n",
      "2025-06-06 11:33:46,935 - __main__ - INFO - \t Balanced Accuracy: 0.5936, F1 Score: 0.5924, ROC AUC: 0.6782\n",
      "2025-06-06 11:33:46,936 - __main__ - INFO - Outer Fold 2 - Train Indices: [ 0  1  2  3  4  5  7  8  9 13], Test Indices: [ 6 10 11 12 15 16 20 21 36 45]\n",
      "2025-06-06 11:33:46,937 - __main__ - INFO - -------------------- Outer Fold 2 --------------------\n",
      "2025-06-06 11:33:46,939 - __main__ - INFO - Starting hyperparameter optimization for outer fold 2...\n",
      "[I 2025-06-06 11:33:46,992] A new study created in RDB with name: 23381.2.tabpfn\n",
      "2025-06-06 11:33:47,048 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [2, 3, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:47,095 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-06 11:33:47,653 - __main__ - INFO - \t Balanced Accuracy: 0.6043, F1 Score: 0.5989, ROC AUC: 0.6622\n",
      "2025-06-06 11:33:47,665 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 2, 3, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:47,712 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-06 11:33:48,253 - __main__ - INFO - \t Balanced Accuracy: 0.5993, F1 Score: 0.5897, ROC AUC: 0.6607\n",
      "[I 2025-06-06 11:33:48,331] Trial 0 finished with value: 0.594337035513506 and parameters: {}. Best is trial 0 with value: 0.594337035513506.\n",
      "2025-06-06 11:33:48,356 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [2, 3, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:48,403 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-06 11:33:48,953 - __main__ - INFO - \t Balanced Accuracy: 0.6043, F1 Score: 0.5989, ROC AUC: 0.6622\n",
      "2025-06-06 11:33:48,965 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 2, 3, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:49,013 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-06 11:33:49,556 - __main__ - INFO - \t Balanced Accuracy: 0.5993, F1 Score: 0.5897, ROC AUC: 0.6607\n",
      "[I 2025-06-06 11:33:49,634] Trial 1 finished with value: 0.594337035513506 and parameters: {}. Best is trial 0 with value: 0.594337035513506.\n",
      "2025-06-06 11:33:49,660 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [2, 3, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:49,707 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-06 11:33:50,251 - __main__ - INFO - \t Balanced Accuracy: 0.6043, F1 Score: 0.5989, ROC AUC: 0.6622\n",
      "2025-06-06 11:33:50,263 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 2, 3, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:50,310 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-06 11:33:50,852 - __main__ - INFO - \t Balanced Accuracy: 0.5993, F1 Score: 0.5897, ROC AUC: 0.6607\n",
      "[I 2025-06-06 11:33:50,931] Trial 2 finished with value: 0.594337035513506 and parameters: {}. Best is trial 0 with value: 0.594337035513506.\n",
      "2025-06-06 11:33:50,956 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [2, 3, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:51,003 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-06 11:33:51,550 - __main__ - INFO - \t Balanced Accuracy: 0.6043, F1 Score: 0.5989, ROC AUC: 0.6622\n",
      "2025-06-06 11:33:51,561 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 2, 3, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:51,609 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-06 11:33:52,151 - __main__ - INFO - \t Balanced Accuracy: 0.5993, F1 Score: 0.5897, ROC AUC: 0.6607\n",
      "[I 2025-06-06 11:33:52,231] Trial 3 finished with value: 0.594337035513506 and parameters: {}. Best is trial 0 with value: 0.594337035513506.\n",
      "2025-06-06 11:33:52,257 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [2, 3, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:52,304 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-06 11:33:52,847 - __main__ - INFO - \t Balanced Accuracy: 0.6043, F1 Score: 0.5989, ROC AUC: 0.6622\n",
      "2025-06-06 11:33:52,859 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 2, 3, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:52,906 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-06 11:33:53,449 - __main__ - INFO - \t Balanced Accuracy: 0.5993, F1 Score: 0.5897, ROC AUC: 0.6607\n",
      "[I 2025-06-06 11:33:53,526] Trial 4 finished with value: 0.594337035513506 and parameters: {}. Best is trial 0 with value: 0.594337035513506.\n",
      "2025-06-06 11:33:53,535 - __main__ - INFO - Best hyperparameters for fold 2: {}\n",
      "2025-06-06 11:33:53,545 - __main__ - INFO - Best score: 0.5943\n",
      "2025-06-06 11:33:53,546 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:53,582 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-06 11:33:53,583 - __main__ - INFO - Retraining Model on Outer Fold 2\n",
      "2025-06-06 11:33:54,261 - __main__ - INFO - \t Inference Time: 0.34541 seconds\n",
      "2025-06-06 11:33:54,267 - __main__ - INFO - \t Balanced Accuracy: 0.5308, F1 Score: 0.5175, ROC AUC: 0.4840\n",
      "2025-06-06 11:33:54,268 - __main__ - INFO - Outer Fold 3 - Train Indices: [ 0  2  3  4  5  6  7  9 10 11], Test Indices: [ 1  8 14 25 28 30 32 34 42 44]\n",
      "2025-06-06 11:33:54,269 - __main__ - INFO - -------------------- Outer Fold 3 --------------------\n",
      "2025-06-06 11:33:54,270 - __main__ - INFO - Starting hyperparameter optimization for outer fold 3...\n",
      "[I 2025-06-06 11:33:54,321] A new study created in RDB with name: 23381.3.tabpfn\n",
      "2025-06-06 11:33:54,373 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 1, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:54,419 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-06 11:33:54,959 - __main__ - INFO - \t Balanced Accuracy: 0.5626, F1 Score: 0.5480, ROC AUC: 0.6283\n",
      "2025-06-06 11:33:54,971 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [3, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:55,019 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-06 11:33:55,557 - __main__ - INFO - \t Balanced Accuracy: 0.5564, F1 Score: 0.5145, ROC AUC: 0.6416\n",
      "[I 2025-06-06 11:33:55,634] Trial 0 finished with value: 0.5312642886876169 and parameters: {}. Best is trial 0 with value: 0.5312642886876169.\n",
      "2025-06-06 11:33:55,658 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 1, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:55,705 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-06 11:33:56,240 - __main__ - INFO - \t Balanced Accuracy: 0.5626, F1 Score: 0.5480, ROC AUC: 0.6283\n",
      "2025-06-06 11:33:56,253 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [3, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:56,300 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-06 11:33:56,843 - __main__ - INFO - \t Balanced Accuracy: 0.5564, F1 Score: 0.5145, ROC AUC: 0.6416\n",
      "[I 2025-06-06 11:33:56,924] Trial 1 finished with value: 0.5312642886876169 and parameters: {}. Best is trial 0 with value: 0.5312642886876169.\n",
      "2025-06-06 11:33:56,952 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 1, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:56,999 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-06 11:33:57,535 - __main__ - INFO - \t Balanced Accuracy: 0.5626, F1 Score: 0.5480, ROC AUC: 0.6283\n",
      "2025-06-06 11:33:57,547 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [3, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:57,594 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-06 11:33:58,133 - __main__ - INFO - \t Balanced Accuracy: 0.5564, F1 Score: 0.5145, ROC AUC: 0.6416\n",
      "[I 2025-06-06 11:33:58,206] Trial 2 finished with value: 0.5312642886876169 and parameters: {}. Best is trial 0 with value: 0.5312642886876169.\n",
      "2025-06-06 11:33:58,231 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 1, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:58,278 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-06 11:33:58,824 - __main__ - INFO - \t Balanced Accuracy: 0.5626, F1 Score: 0.5480, ROC AUC: 0.6283\n",
      "2025-06-06 11:33:58,834 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [3, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:58,882 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-06 11:33:59,420 - __main__ - INFO - \t Balanced Accuracy: 0.5564, F1 Score: 0.5145, ROC AUC: 0.6416\n",
      "[I 2025-06-06 11:33:59,491] Trial 3 finished with value: 0.5312642886876169 and parameters: {}. Best is trial 0 with value: 0.5312642886876169.\n",
      "2025-06-06 11:33:59,516 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 1, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:33:59,563 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-06 11:34:00,104 - __main__ - INFO - \t Balanced Accuracy: 0.5626, F1 Score: 0.5480, ROC AUC: 0.6283\n",
      "2025-06-06 11:34:00,115 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [3, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:00,162 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-06 11:34:00,701 - __main__ - INFO - \t Balanced Accuracy: 0.5564, F1 Score: 0.5145, ROC AUC: 0.6416\n",
      "[I 2025-06-06 11:34:00,773] Trial 4 finished with value: 0.5312642886876169 and parameters: {}. Best is trial 0 with value: 0.5312642886876169.\n",
      "2025-06-06 11:34:00,783 - __main__ - INFO - Best hyperparameters for fold 3: {}\n",
      "2025-06-06 11:34:00,792 - __main__ - INFO - Best score: 0.5313\n",
      "2025-06-06 11:34:00,793 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [2, 5, 7, 8, 9] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:00,829 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-06 11:34:00,830 - __main__ - INFO - Retraining Model on Outer Fold 3\n",
      "2025-06-06 11:34:01,745 - __main__ - INFO - \t Inference Time: 0.34576 seconds\n",
      "2025-06-06 11:34:01,751 - __main__ - INFO - \t Balanced Accuracy: 0.6195, F1 Score: 0.6179, ROC AUC: 0.6445\n",
      "2025-06-06 11:34:01,752 - __main__ - INFO - Outer Fold 4 - Train Indices: [ 0  1  2  3  4  6  8  9 10 11], Test Indices: [ 5  7 13 22 24 26 33 38 43 46]\n",
      "2025-06-06 11:34:01,753 - __main__ - INFO - -------------------- Outer Fold 4 --------------------\n",
      "2025-06-06 11:34:01,755 - __main__ - INFO - Starting hyperparameter optimization for outer fold 4...\n",
      "[I 2025-06-06 11:34:01,805] A new study created in RDB with name: 23381.4.tabpfn\n",
      "2025-06-06 11:34:01,857 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 2, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:01,904 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-06 11:34:02,445 - __main__ - INFO - \t Balanced Accuracy: 0.5326, F1 Score: 0.4770, ROC AUC: 0.5968\n",
      "2025-06-06 11:34:02,458 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:02,505 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-06 11:34:03,049 - __main__ - INFO - \t Balanced Accuracy: 0.5523, F1 Score: 0.5366, ROC AUC: 0.5721\n",
      "[I 2025-06-06 11:34:03,135] Trial 0 finished with value: 0.5067987979277282 and parameters: {}. Best is trial 0 with value: 0.5067987979277282.\n",
      "2025-06-06 11:34:03,161 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 2, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:03,207 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-06 11:34:03,740 - __main__ - INFO - \t Balanced Accuracy: 0.5326, F1 Score: 0.4770, ROC AUC: 0.5968\n",
      "2025-06-06 11:34:03,754 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:03,802 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-06 11:34:04,340 - __main__ - INFO - \t Balanced Accuracy: 0.5523, F1 Score: 0.5366, ROC AUC: 0.5721\n",
      "[I 2025-06-06 11:34:04,462] Trial 1 finished with value: 0.5067987979277282 and parameters: {}. Best is trial 0 with value: 0.5067987979277282.\n",
      "2025-06-06 11:34:04,497 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 2, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:04,543 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-06 11:34:05,079 - __main__ - INFO - \t Balanced Accuracy: 0.5326, F1 Score: 0.4770, ROC AUC: 0.5968\n",
      "2025-06-06 11:34:05,090 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:05,137 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-06 11:34:05,991 - __main__ - INFO - \t Balanced Accuracy: 0.5523, F1 Score: 0.5366, ROC AUC: 0.5721\n",
      "[I 2025-06-06 11:34:06,062] Trial 2 finished with value: 0.5067987979277282 and parameters: {}. Best is trial 0 with value: 0.5067987979277282.\n",
      "2025-06-06 11:34:06,087 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 2, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:06,134 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-06 11:34:06,669 - __main__ - INFO - \t Balanced Accuracy: 0.5326, F1 Score: 0.4770, ROC AUC: 0.5968\n",
      "2025-06-06 11:34:06,679 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:06,727 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-06 11:34:07,267 - __main__ - INFO - \t Balanced Accuracy: 0.5523, F1 Score: 0.5366, ROC AUC: 0.5721\n",
      "[I 2025-06-06 11:34:07,338] Trial 3 finished with value: 0.5067987979277282 and parameters: {}. Best is trial 0 with value: 0.5067987979277282.\n",
      "2025-06-06 11:34:07,362 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 2, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:07,408 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-06 11:34:07,939 - __main__ - INFO - \t Balanced Accuracy: 0.5326, F1 Score: 0.4770, ROC AUC: 0.5968\n",
      "2025-06-06 11:34:07,951 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:07,998 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-06 11:34:08,541 - __main__ - INFO - \t Balanced Accuracy: 0.5523, F1 Score: 0.5366, ROC AUC: 0.5721\n",
      "[I 2025-06-06 11:34:08,613] Trial 4 finished with value: 0.5067987979277282 and parameters: {}. Best is trial 0 with value: 0.5067987979277282.\n",
      "2025-06-06 11:34:08,623 - __main__ - INFO - Best hyperparameters for fold 4: {}\n",
      "2025-06-06 11:34:08,632 - __main__ - INFO - Best score: 0.5068\n",
      "2025-06-06 11:34:08,632 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [3, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:08,669 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-06 11:34:08,670 - __main__ - INFO - Retraining Model on Outer Fold 4\n",
      "2025-06-06 11:34:09,616 - __main__ - INFO - \t Inference Time: 0.33801 seconds\n",
      "2025-06-06 11:34:09,622 - __main__ - INFO - \t Balanced Accuracy: 0.6605, F1 Score: 0.6615, ROC AUC: 0.7159\n",
      "2025-06-06 11:34:09,623 - __main__ - INFO - Outer Fold 5 - Train Indices: [ 0  1  2  4  5  6  7  8 10 11], Test Indices: [ 3  9 17 23 29 31 37 40 41 54]\n",
      "2025-06-06 11:34:09,624 - __main__ - INFO - -------------------- Outer Fold 5 --------------------\n",
      "2025-06-06 11:34:09,626 - __main__ - INFO - Starting hyperparameter optimization for outer fold 5...\n",
      "[I 2025-06-06 11:34:09,676] A new study created in RDB with name: 23381.5.tabpfn\n",
      "2025-06-06 11:34:09,729 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:09,775 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-06 11:34:10,316 - __main__ - INFO - \t Balanced Accuracy: 0.4957, F1 Score: 0.3651, ROC AUC: 0.6339\n",
      "2025-06-06 11:34:10,330 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [2, 3, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:10,377 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-06 11:34:10,918 - __main__ - INFO - \t Balanced Accuracy: 0.5382, F1 Score: 0.5072, ROC AUC: 0.5898\n",
      "[I 2025-06-06 11:34:11,011] Trial 0 finished with value: 0.43611583351936756 and parameters: {}. Best is trial 0 with value: 0.43611583351936756.\n",
      "2025-06-06 11:34:11,041 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:11,088 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-06 11:34:11,632 - __main__ - INFO - \t Balanced Accuracy: 0.4957, F1 Score: 0.3651, ROC AUC: 0.6339\n",
      "2025-06-06 11:34:11,645 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [2, 3, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:11,692 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-06 11:34:12,237 - __main__ - INFO - \t Balanced Accuracy: 0.5382, F1 Score: 0.5072, ROC AUC: 0.5898\n",
      "[I 2025-06-06 11:34:12,324] Trial 1 finished with value: 0.43611583351936756 and parameters: {}. Best is trial 0 with value: 0.43611583351936756.\n",
      "2025-06-06 11:34:12,350 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:12,397 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-06 11:34:12,938 - __main__ - INFO - \t Balanced Accuracy: 0.4957, F1 Score: 0.3651, ROC AUC: 0.6339\n",
      "2025-06-06 11:34:12,950 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [2, 3, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:12,997 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-06 11:34:13,538 - __main__ - INFO - \t Balanced Accuracy: 0.5382, F1 Score: 0.5072, ROC AUC: 0.5898\n",
      "[I 2025-06-06 11:34:13,612] Trial 2 finished with value: 0.43611583351936756 and parameters: {}. Best is trial 0 with value: 0.43611583351936756.\n",
      "2025-06-06 11:34:13,637 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:13,683 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-06 11:34:14,225 - __main__ - INFO - \t Balanced Accuracy: 0.4957, F1 Score: 0.3651, ROC AUC: 0.6339\n",
      "2025-06-06 11:34:14,236 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [2, 3, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:14,283 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-06 11:34:14,827 - __main__ - INFO - \t Balanced Accuracy: 0.5382, F1 Score: 0.5072, ROC AUC: 0.5898\n",
      "[I 2025-06-06 11:34:14,903] Trial 3 finished with value: 0.43611583351936756 and parameters: {}. Best is trial 0 with value: 0.43611583351936756.\n",
      "2025-06-06 11:34:14,928 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:14,974 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-06 11:34:15,521 - __main__ - INFO - \t Balanced Accuracy: 0.4957, F1 Score: 0.3651, ROC AUC: 0.6339\n",
      "2025-06-06 11:34:15,533 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [2, 3, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:15,580 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-06 11:34:16,121 - __main__ - INFO - \t Balanced Accuracy: 0.5382, F1 Score: 0.5072, ROC AUC: 0.5898\n",
      "[I 2025-06-06 11:34:16,203] Trial 4 finished with value: 0.43611583351936756 and parameters: {}. Best is trial 0 with value: 0.43611583351936756.\n",
      "2025-06-06 11:34:16,216 - __main__ - INFO - Best hyperparameters for fold 5: {}\n",
      "2025-06-06 11:34:16,228 - __main__ - INFO - Best score: 0.4361\n",
      "2025-06-06 11:34:16,229 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 4, 5, 8, 9] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-06 11:34:16,276 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-06 11:34:16,276 - __main__ - INFO - Retraining Model on Outer Fold 5\n",
      "2025-06-06 11:34:16,972 - __main__ - INFO - \t Inference Time: 0.33977 seconds\n",
      "2025-06-06 11:34:16,978 - __main__ - INFO - \t Balanced Accuracy: 0.6084, F1 Score: 0.5872, ROC AUC: 0.6925\n",
      "2025-06-06 11:34:16,985 - __main__ - INFO - Outer fold metrics saved to: data/outer_fold/teacher/hpo/23381_tabpfn.csv\n",
      "2025-06-06 11:34:16,986 - __main__ - INFO - === FINAL RESULTS FOR DATASET 23381 ===\n",
      "2025-06-06 11:34:16,987 - __main__ - INFO - Balanced Accuracy: 0.6025 ± 0.0472\n",
      "2025-06-06 11:34:16,987 - __main__ - INFO - F1 Score: 0.5953 ± 0.0525\n",
      "2025-06-06 11:34:16,988 - __main__ - INFO - ROC AUC: 0.6430 ± 0.0926\n",
      "2025-06-06 11:34:16,988 - __main__ - INFO - Mean Inference Time: 0.3389 ± 0.0083\n",
      "2025-06-06 11:34:16,989 - __main__ - INFO - Mean Parameters: 7244554.0000 ± 0.0000\n",
      "2025-06-06 11:34:16,994 - __main__ - INFO - Summary statistics saved to: results/teacher/23381_results.json\n",
      "2025-06-06 11:34:16,995 - __main__ - INFO - Fold indices file already exists: data/fold_indices/dataset_23381.json\n",
      "2025-06-06 11:34:17,000 - __main__ - INFO - tabpfn outputs saved to: data/output/hpo/teacher/23381_tabpfn.csv\n",
      "2025-06-06 11:34:17,000 - __main__ - INFO - Loading configuration...\n",
      "2025-06-06 11:34:17,001 - __main__ - INFO - Loading dataset 197 from cache at 'data/cache/openml_dataset_197.pkl'...\n",
      "2025-06-06 11:34:17,003 - __main__ - INFO - Dataset 197 loaded successfully with task type: regression\n",
      "2025-06-06 11:34:17,005 - __main__ - INFO - Using GPU: NVIDIA GeForce RTX 2080 Ti\n",
      "2025-06-06 11:34:17,007 - __main__ - INFO - Random seed set to 42\n",
      "2025-06-06 11:34:17,008 - __main__ - INFO - Outer Fold 1 - Train Indices: [0 1 2 3 4 5 6 7 8 9], Test Indices: [17 19 23 26 31 33 37 41 48 50]\n",
      "2025-06-06 11:34:17,009 - __main__ - INFO - -------------------- Outer Fold 1 --------------------\n",
      "2025-06-06 11:34:17,010 - __main__ - INFO - Starting hyperparameter optimization for outer fold 1...\n",
      "[I 2025-06-06 11:34:17,062] A new study created in RDB with name: 197.1.tabpfn\n",
      "2025-06-06 11:34:17,122 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:34:17,147 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-06 11:34:21,500 - __main__ - INFO - \t MAE: 1.8251, MSE: 7.8640, RMSE: 2.8043, R^2: 0.9778\n",
      "2025-06-06 11:34:21,514 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:34:21,539 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-06 11:34:25,845 - __main__ - INFO - \t MAE: 1.7853, MSE: 7.2915, RMSE: 2.7003, R^2: 0.9787\n",
      "[I 2025-06-06 11:34:25,933] Trial 0 finished with value: -1.8052287040813624 and parameters: {}. Best is trial 0 with value: -1.8052287040813624.\n",
      "2025-06-06 11:34:25,963 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:34:25,988 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-06 11:34:30,263 - __main__ - INFO - \t MAE: 1.8251, MSE: 7.8640, RMSE: 2.8043, R^2: 0.9778\n",
      "2025-06-06 11:34:30,277 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:34:30,302 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-06 11:34:34,614 - __main__ - INFO - \t MAE: 1.7853, MSE: 7.2915, RMSE: 2.7003, R^2: 0.9787\n",
      "[I 2025-06-06 11:34:34,704] Trial 1 finished with value: -1.8052287040813624 and parameters: {}. Best is trial 0 with value: -1.8052287040813624.\n",
      "2025-06-06 11:34:34,731 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:34:34,756 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-06 11:34:39,046 - __main__ - INFO - \t MAE: 1.8251, MSE: 7.8640, RMSE: 2.8043, R^2: 0.9778\n",
      "2025-06-06 11:34:39,057 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:34:39,083 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-06 11:34:43,408 - __main__ - INFO - \t MAE: 1.7853, MSE: 7.2915, RMSE: 2.7003, R^2: 0.9787\n",
      "[I 2025-06-06 11:34:43,492] Trial 2 finished with value: -1.8052287040813624 and parameters: {}. Best is trial 0 with value: -1.8052287040813624.\n",
      "2025-06-06 11:34:43,515 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:34:43,540 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-06 11:34:47,833 - __main__ - INFO - \t MAE: 1.8251, MSE: 7.8640, RMSE: 2.8043, R^2: 0.9778\n",
      "2025-06-06 11:34:47,844 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:34:47,870 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-06 11:34:52,197 - __main__ - INFO - \t MAE: 1.7853, MSE: 7.2915, RMSE: 2.7003, R^2: 0.9787\n",
      "[I 2025-06-06 11:34:52,289] Trial 3 finished with value: -1.8052287040813624 and parameters: {}. Best is trial 0 with value: -1.8052287040813624.\n",
      "2025-06-06 11:34:52,315 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:34:52,340 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-06 11:34:56,643 - __main__ - INFO - \t MAE: 1.8251, MSE: 7.8640, RMSE: 2.8043, R^2: 0.9778\n",
      "2025-06-06 11:34:56,818 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:34:56,844 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-06 11:35:01,200 - __main__ - INFO - \t MAE: 1.7853, MSE: 7.2915, RMSE: 2.7003, R^2: 0.9787\n",
      "[I 2025-06-06 11:35:01,289] Trial 4 finished with value: -1.8052287040813624 and parameters: {}. Best is trial 0 with value: -1.8052287040813624.\n",
      "2025-06-06 11:35:01,299 - __main__ - INFO - Best hyperparameters for fold 1: {}\n",
      "2025-06-06 11:35:01,308 - __main__ - INFO - Best score: -1.8052\n",
      "2025-06-06 11:35:01,309 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:35:01,444 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-06 11:35:01,445 - __main__ - INFO - Retraining Model on Outer Fold 1\n",
      "2025-06-06 11:35:08,925 - __main__ - INFO - \t Inference Time: 5.56781 seconds\n",
      "2025-06-06 11:35:08,928 - __main__ - INFO - \t MAE: 1.7086, MSE: 6.8293, RMSE: 2.6133, R^2: 0.9772\n",
      "2025-06-06 11:35:08,930 - __main__ - INFO - Outer Fold 2 - Train Indices: [ 1  2  3  4  5  6  7  9 10 11], Test Indices: [ 0  8 12 14 15 29 30 43 44 45]\n",
      "2025-06-06 11:35:08,931 - __main__ - INFO - -------------------- Outer Fold 2 --------------------\n",
      "2025-06-06 11:35:08,933 - __main__ - INFO - Starting hyperparameter optimization for outer fold 2...\n",
      "[I 2025-06-06 11:35:09,001] A new study created in RDB with name: 197.2.tabpfn\n",
      "2025-06-06 11:35:09,058 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:35:09,082 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-06 11:35:13,414 - __main__ - INFO - \t MAE: 1.7891, MSE: 7.3824, RMSE: 2.7171, R^2: 0.9783\n",
      "2025-06-06 11:35:13,428 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:35:13,453 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-06 11:35:17,805 - __main__ - INFO - \t MAE: 1.7721, MSE: 7.5325, RMSE: 2.7445, R^2: 0.9781\n",
      "[I 2025-06-06 11:35:17,895] Trial 0 finished with value: -1.7806263618376197 and parameters: {}. Best is trial 0 with value: -1.7806263618376197.\n",
      "2025-06-06 11:35:17,921 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:35:17,946 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-06 11:35:22,279 - __main__ - INFO - \t MAE: 1.7891, MSE: 7.3824, RMSE: 2.7171, R^2: 0.9783\n",
      "2025-06-06 11:35:22,293 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:35:22,319 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-06 11:35:26,650 - __main__ - INFO - \t MAE: 1.7721, MSE: 7.5325, RMSE: 2.7445, R^2: 0.9781\n",
      "[I 2025-06-06 11:35:26,732] Trial 1 finished with value: -1.7806263618376197 and parameters: {}. Best is trial 0 with value: -1.7806263618376197.\n",
      "2025-06-06 11:35:26,758 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:35:26,783 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-06 11:35:31,114 - __main__ - INFO - \t MAE: 1.7891, MSE: 7.3824, RMSE: 2.7171, R^2: 0.9783\n",
      "2025-06-06 11:35:31,125 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:35:31,150 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-06 11:35:35,486 - __main__ - INFO - \t MAE: 1.7721, MSE: 7.5325, RMSE: 2.7445, R^2: 0.9781\n",
      "[I 2025-06-06 11:35:35,566] Trial 2 finished with value: -1.7806263618376197 and parameters: {}. Best is trial 0 with value: -1.7806263618376197.\n",
      "2025-06-06 11:35:35,590 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:35:35,615 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-06 11:35:39,949 - __main__ - INFO - \t MAE: 1.7891, MSE: 7.3824, RMSE: 2.7171, R^2: 0.9783\n",
      "2025-06-06 11:35:39,961 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:35:39,986 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-06 11:35:44,330 - __main__ - INFO - \t MAE: 1.7721, MSE: 7.5325, RMSE: 2.7445, R^2: 0.9781\n",
      "[I 2025-06-06 11:35:44,415] Trial 3 finished with value: -1.7806263618376197 and parameters: {}. Best is trial 0 with value: -1.7806263618376197.\n",
      "2025-06-06 11:35:44,439 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:35:44,464 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-06 11:35:48,804 - __main__ - INFO - \t MAE: 1.7891, MSE: 7.3824, RMSE: 2.7171, R^2: 0.9783\n",
      "2025-06-06 11:35:48,815 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:35:48,841 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-06 11:35:53,194 - __main__ - INFO - \t MAE: 1.7721, MSE: 7.5325, RMSE: 2.7445, R^2: 0.9781\n",
      "[I 2025-06-06 11:35:53,284] Trial 4 finished with value: -1.7806263618376197 and parameters: {}. Best is trial 0 with value: -1.7806263618376197.\n",
      "2025-06-06 11:35:53,294 - __main__ - INFO - Best hyperparameters for fold 2: {}\n",
      "2025-06-06 11:35:53,304 - __main__ - INFO - Best score: -1.7806\n",
      "2025-06-06 11:35:53,305 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:35:53,440 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-06 11:35:53,441 - __main__ - INFO - Retraining Model on Outer Fold 2\n",
      "2025-06-06 11:36:00,969 - __main__ - INFO - \t Inference Time: 5.61581 seconds\n",
      "2025-06-06 11:36:00,973 - __main__ - INFO - \t MAE: 1.7240, MSE: 7.1619, RMSE: 2.6762, R^2: 0.9779\n",
      "2025-06-06 11:36:00,975 - __main__ - INFO - Outer Fold 3 - Train Indices: [ 0  1  2  3  4  5  7  8  9 10], Test Indices: [ 6 18 24 25 32 39 49 52 62 67]\n",
      "2025-06-06 11:36:00,975 - __main__ - INFO - -------------------- Outer Fold 3 --------------------\n",
      "2025-06-06 11:36:00,977 - __main__ - INFO - Starting hyperparameter optimization for outer fold 3...\n",
      "[I 2025-06-06 11:36:01,037] A new study created in RDB with name: 197.3.tabpfn\n",
      "2025-06-06 11:36:01,088 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:36:01,113 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-06 11:36:05,469 - __main__ - INFO - \t MAE: 1.7968, MSE: 7.4709, RMSE: 2.7333, R^2: 0.9787\n",
      "2025-06-06 11:36:05,482 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:36:05,507 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-06 11:36:09,853 - __main__ - INFO - \t MAE: 1.8106, MSE: 7.8012, RMSE: 2.7931, R^2: 0.9773\n",
      "[I 2025-06-06 11:36:09,947] Trial 0 finished with value: -1.803708309584505 and parameters: {}. Best is trial 0 with value: -1.803708309584505.\n",
      "2025-06-06 11:36:09,974 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:36:09,999 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-06 11:36:14,353 - __main__ - INFO - \t MAE: 1.7968, MSE: 7.4709, RMSE: 2.7333, R^2: 0.9787\n",
      "2025-06-06 11:36:14,364 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:36:14,390 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-06 11:36:18,733 - __main__ - INFO - \t MAE: 1.8106, MSE: 7.8012, RMSE: 2.7931, R^2: 0.9773\n",
      "[I 2025-06-06 11:36:18,825] Trial 1 finished with value: -1.803708309584505 and parameters: {}. Best is trial 0 with value: -1.803708309584505.\n",
      "2025-06-06 11:36:18,851 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:36:18,875 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-06 11:36:23,243 - __main__ - INFO - \t MAE: 1.7968, MSE: 7.4709, RMSE: 2.7333, R^2: 0.9787\n",
      "2025-06-06 11:36:23,256 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:36:23,282 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-06 11:36:27,632 - __main__ - INFO - \t MAE: 1.8106, MSE: 7.8012, RMSE: 2.7931, R^2: 0.9773\n",
      "[I 2025-06-06 11:36:27,722] Trial 2 finished with value: -1.803708309584505 and parameters: {}. Best is trial 0 with value: -1.803708309584505.\n",
      "2025-06-06 11:36:27,748 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:36:27,773 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-06 11:36:32,133 - __main__ - INFO - \t MAE: 1.7968, MSE: 7.4709, RMSE: 2.7333, R^2: 0.9787\n",
      "2025-06-06 11:36:32,145 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:36:32,170 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-06 11:36:36,513 - __main__ - INFO - \t MAE: 1.8106, MSE: 7.8012, RMSE: 2.7931, R^2: 0.9773\n",
      "[I 2025-06-06 11:36:36,603] Trial 3 finished with value: -1.803708309584505 and parameters: {}. Best is trial 0 with value: -1.803708309584505.\n",
      "2025-06-06 11:36:36,632 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:36:36,656 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-06 11:36:41,015 - __main__ - INFO - \t MAE: 1.7968, MSE: 7.4709, RMSE: 2.7333, R^2: 0.9787\n",
      "2025-06-06 11:36:41,028 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:36:41,053 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-06 11:36:45,395 - __main__ - INFO - \t MAE: 1.8106, MSE: 7.8012, RMSE: 2.7931, R^2: 0.9773\n",
      "[I 2025-06-06 11:36:45,479] Trial 4 finished with value: -1.803708309584505 and parameters: {}. Best is trial 0 with value: -1.803708309584505.\n",
      "2025-06-06 11:36:45,490 - __main__ - INFO - Best hyperparameters for fold 3: {}\n",
      "2025-06-06 11:36:45,499 - __main__ - INFO - Best score: -1.8037\n",
      "2025-06-06 11:36:45,500 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:36:45,635 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-06 11:36:45,636 - __main__ - INFO - Retraining Model on Outer Fold 3\n",
      "2025-06-06 11:36:53,219 - __main__ - INFO - \t Inference Time: 5.63170 seconds\n",
      "2025-06-06 11:36:53,223 - __main__ - INFO - \t MAE: 1.6785, MSE: 6.6808, RMSE: 2.5847, R^2: 0.9780\n",
      "2025-06-06 11:36:53,225 - __main__ - INFO - Outer Fold 4 - Train Indices: [ 0  3  4  5  6  8  9 12 14 15], Test Indices: [ 1  2  7 10 11 13 20 21 22 27]\n",
      "2025-06-06 11:36:53,225 - __main__ - INFO - -------------------- Outer Fold 4 --------------------\n",
      "2025-06-06 11:36:53,228 - __main__ - INFO - Starting hyperparameter optimization for outer fold 4...\n",
      "[I 2025-06-06 11:36:53,298] A new study created in RDB with name: 197.4.tabpfn\n",
      "2025-06-06 11:36:53,354 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:36:53,379 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-06 11:36:57,722 - __main__ - INFO - \t MAE: 1.7320, MSE: 6.9052, RMSE: 2.6278, R^2: 0.9773\n",
      "2025-06-06 11:36:57,736 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:36:57,761 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-06 11:37:02,143 - __main__ - INFO - \t MAE: 1.8873, MSE: 9.3607, RMSE: 3.0595, R^2: 0.9740\n",
      "[I 2025-06-06 11:37:02,250] Trial 0 finished with value: -1.8096591037404512 and parameters: {}. Best is trial 0 with value: -1.8096591037404512.\n",
      "2025-06-06 11:37:02,279 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:37:02,304 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-06 11:37:06,648 - __main__ - INFO - \t MAE: 1.7320, MSE: 6.9052, RMSE: 2.6278, R^2: 0.9773\n",
      "2025-06-06 11:37:06,659 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:37:06,684 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-06 11:37:11,069 - __main__ - INFO - \t MAE: 1.8873, MSE: 9.3607, RMSE: 3.0595, R^2: 0.9740\n",
      "[I 2025-06-06 11:37:11,153] Trial 1 finished with value: -1.8096591037404512 and parameters: {}. Best is trial 0 with value: -1.8096591037404512.\n",
      "2025-06-06 11:37:11,181 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:37:11,206 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-06 11:37:15,555 - __main__ - INFO - \t MAE: 1.7320, MSE: 6.9052, RMSE: 2.6278, R^2: 0.9773\n",
      "2025-06-06 11:37:15,567 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:37:15,593 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-06 11:37:19,988 - __main__ - INFO - \t MAE: 1.8873, MSE: 9.3607, RMSE: 3.0595, R^2: 0.9740\n",
      "[I 2025-06-06 11:37:20,073] Trial 2 finished with value: -1.8096591037404512 and parameters: {}. Best is trial 0 with value: -1.8096591037404512.\n",
      "2025-06-06 11:37:20,100 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:37:20,125 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-06 11:37:24,466 - __main__ - INFO - \t MAE: 1.7320, MSE: 6.9052, RMSE: 2.6278, R^2: 0.9773\n",
      "2025-06-06 11:37:24,481 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:37:24,506 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-06 11:37:28,883 - __main__ - INFO - \t MAE: 1.8873, MSE: 9.3607, RMSE: 3.0595, R^2: 0.9740\n",
      "[I 2025-06-06 11:37:28,970] Trial 3 finished with value: -1.8096591037404512 and parameters: {}. Best is trial 0 with value: -1.8096591037404512.\n",
      "2025-06-06 11:37:28,994 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:37:29,018 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-06 11:37:33,359 - __main__ - INFO - \t MAE: 1.7320, MSE: 6.9052, RMSE: 2.6278, R^2: 0.9773\n",
      "2025-06-06 11:37:33,371 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:37:33,397 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-06 11:37:37,782 - __main__ - INFO - \t MAE: 1.8873, MSE: 9.3607, RMSE: 3.0595, R^2: 0.9740\n",
      "[I 2025-06-06 11:37:37,863] Trial 4 finished with value: -1.8096591037404512 and parameters: {}. Best is trial 0 with value: -1.8096591037404512.\n",
      "2025-06-06 11:37:37,872 - __main__ - INFO - Best hyperparameters for fold 4: {}\n",
      "2025-06-06 11:37:37,882 - __main__ - INFO - Best score: -1.8097\n",
      "2025-06-06 11:37:37,882 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:37:38,018 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-06 11:37:38,019 - __main__ - INFO - Retraining Model on Outer Fold 4\n",
      "2025-06-06 11:37:45,616 - __main__ - INFO - \t Inference Time: 5.65129 seconds\n",
      "2025-06-06 11:37:45,619 - __main__ - INFO - \t MAE: 1.7425, MSE: 7.3322, RMSE: 2.7078, R^2: 0.9798\n",
      "2025-06-06 11:37:45,622 - __main__ - INFO - Outer Fold 5 - Train Indices: [ 0  1  2  6  7  8 10 11 12 13], Test Indices: [ 3  4  5  9 16 34 54 55 64 66]\n",
      "2025-06-06 11:37:45,622 - __main__ - INFO - -------------------- Outer Fold 5 --------------------\n",
      "2025-06-06 11:37:45,625 - __main__ - INFO - Starting hyperparameter optimization for outer fold 5...\n",
      "[I 2025-06-06 11:37:45,680] A new study created in RDB with name: 197.5.tabpfn\n",
      "2025-06-06 11:37:45,733 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:37:45,758 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-06 11:37:50,136 - __main__ - INFO - \t MAE: 1.8524, MSE: 8.8294, RMSE: 2.9714, R^2: 0.9739\n",
      "2025-06-06 11:37:50,149 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:37:50,175 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-06 11:37:54,568 - __main__ - INFO - \t MAE: 1.7751, MSE: 7.3752, RMSE: 2.7157, R^2: 0.9760\n",
      "[I 2025-06-06 11:37:54,652] Trial 0 finished with value: -1.8137639332079547 and parameters: {}. Best is trial 0 with value: -1.8137639332079547.\n",
      "2025-06-06 11:37:54,676 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:37:54,701 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-06 11:37:59,421 - __main__ - INFO - \t MAE: 1.8524, MSE: 8.8294, RMSE: 2.9714, R^2: 0.9739\n",
      "2025-06-06 11:37:59,434 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:37:59,459 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-06 11:38:03,866 - __main__ - INFO - \t MAE: 1.7751, MSE: 7.3752, RMSE: 2.7157, R^2: 0.9760\n",
      "[I 2025-06-06 11:38:04,779] Trial 1 finished with value: -1.8137639332079547 and parameters: {}. Best is trial 0 with value: -1.8137639332079547.\n",
      "2025-06-06 11:38:04,827 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:38:04,852 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-06 11:38:09,268 - __main__ - INFO - \t MAE: 1.8524, MSE: 8.8294, RMSE: 2.9714, R^2: 0.9739\n",
      "2025-06-06 11:38:09,283 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:38:09,308 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-06 11:38:13,708 - __main__ - INFO - \t MAE: 1.7751, MSE: 7.3752, RMSE: 2.7157, R^2: 0.9760\n",
      "[I 2025-06-06 11:38:13,801] Trial 2 finished with value: -1.8137639332079547 and parameters: {}. Best is trial 0 with value: -1.8137639332079547.\n",
      "2025-06-06 11:38:13,829 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:38:13,854 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-06 11:38:18,236 - __main__ - INFO - \t MAE: 1.8524, MSE: 8.8294, RMSE: 2.9714, R^2: 0.9739\n",
      "2025-06-06 11:38:18,249 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:38:18,274 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-06 11:38:22,675 - __main__ - INFO - \t MAE: 1.7751, MSE: 7.3752, RMSE: 2.7157, R^2: 0.9760\n",
      "[I 2025-06-06 11:38:22,757] Trial 3 finished with value: -1.8137639332079547 and parameters: {}. Best is trial 0 with value: -1.8137639332079547.\n",
      "2025-06-06 11:38:22,781 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:38:22,806 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-06 11:38:27,191 - __main__ - INFO - \t MAE: 1.8524, MSE: 8.8294, RMSE: 2.9714, R^2: 0.9739\n",
      "2025-06-06 11:38:27,203 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:38:27,228 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-06 11:38:31,633 - __main__ - INFO - \t MAE: 1.7751, MSE: 7.3752, RMSE: 2.7157, R^2: 0.9760\n",
      "[I 2025-06-06 11:38:31,717] Trial 4 finished with value: -1.8137639332079547 and parameters: {}. Best is trial 0 with value: -1.8137639332079547.\n",
      "2025-06-06 11:38:31,726 - __main__ - INFO - Best hyperparameters for fold 5: {}\n",
      "2025-06-06 11:38:31,735 - __main__ - INFO - Best score: -1.8138\n",
      "2025-06-06 11:38:31,736 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "2025-06-06 11:38:31,872 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-06 11:38:31,872 - __main__ - INFO - Retraining Model on Outer Fold 5\n",
      "2025-06-06 11:38:39,511 - __main__ - INFO - \t Inference Time: 5.65003 seconds\n",
      "2025-06-06 11:38:39,514 - __main__ - INFO - \t MAE: 1.7888, MSE: 7.6974, RMSE: 2.7744, R^2: 0.9808\n",
      "2025-06-06 11:38:39,518 - __main__ - INFO - Outer fold metrics saved to: data/outer_fold/teacher/hpo/197_tabpfn.csv\n",
      "2025-06-06 11:38:39,520 - __main__ - INFO - === FINAL RESULTS FOR DATASET 197 ===\n",
      "2025-06-06 11:38:39,521 - __main__ - INFO - MAE: 1.7285 ± 0.0411\n",
      "2025-06-06 11:38:39,521 - __main__ - INFO - MSE: 7.1403 ± 0.4048\n",
      "2025-06-06 11:38:39,522 - __main__ - INFO - RMSE: 2.6713 ± 0.0756\n",
      "2025-06-06 11:38:39,522 - __main__ - INFO - R2: 0.9787 ± 0.0015\n",
      "2025-06-06 11:38:39,523 - __main__ - INFO - Mean Inference Time: 5.6233 ± 0.0343\n",
      "2025-06-06 11:38:39,523 - __main__ - INFO - Mean Parameters: 11081864.0000 ± 0.0000\n",
      "2025-06-06 11:38:39,527 - __main__ - INFO - Summary statistics saved to: results/teacher/197_results.json\n",
      "2025-06-06 11:38:39,528 - __main__ - INFO - Fold indices file already exists: data/fold_indices/dataset_197.json\n",
      "2025-06-06 11:38:39,542 - __main__ - INFO - tabpfn outputs saved to: data/output/hpo/teacher/197_tabpfn.csv\n"
     ]
    }
   ],
   "source": [
    "# Get list of datasets to process from configuration\n",
    "datasets = config[\"data\"][\"datasets\"]\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINING LOOP - Process each dataset independently\n",
    "# =============================================================================\n",
    "for dataset_id in datasets:\n",
    "    # -------------------------------------------------------------------------\n",
    "    # SETUP AND INITIALIZATION\n",
    "    # -------------------------------------------------------------------------\n",
    "    logger = setup_logging()\n",
    "    logger.info(\"Loading configuration...\")\n",
    "\n",
    "    # Extract model configuration for this run\n",
    "    model_type = config[\"model\"][\"teacher_model\"]\n",
    "    preprocessing_type = config[\"teacher_models\"][model_type][\"preprocessing\"]\n",
    "    use_hpo = config[\"training\"][\"use_hpo\"]\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 0: DATA LOADING\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Load dataset from OpenML with caching for efficiency\n",
    "    X, y, cat_cols, _, task_type = load_dataset(\n",
    "        dataset_id=dataset_id,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 1: INFRASTRUCTURE SETUP\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Note: Checking for existing results is placeholder for future implementation\n",
    "    summary_file = os.path.join(config[\"data\"][\"results_dir_path\"], \"teacher\", f\"{dataset_id}_results.json\")\n",
    "    if os.path.exists(summary_file):\n",
    "        with open(summary_file, 'r') as f:\n",
    "            existing_results = json.load(f)\n",
    "        \n",
    "        # Check if we already have results with the same configuration\n",
    "        config_exists = False\n",
    "        for key, result in existing_results.items():\n",
    "            if (result.get('model_type') == model_type and \n",
    "                result.get('use_hpo') == use_hpo and \n",
    "                result.get('seed') == config[\"training\"][\"random_state\"]):\n",
    "                config_exists = True\n",
    "                logger.info(f\"Results already exist for dataset {dataset_id} with model {model_type}, HPO: {use_hpo}, seed: {config['training']['random_state']}\")\n",
    "                break\n",
    "        \n",
    "        if config_exists:\n",
    "            logger.info(f\"Skipping dataset {dataset_id} - results already computed\")\n",
    "            continue\n",
    "\n",
    "    # Configure GPU/CPU usage for training\n",
    "    device = check_GPU_availability()\n",
    "\n",
    "    # Set random seed for reproducibility across all libraries\n",
    "    set_seed(config[\"training\"][\"random_state\"])\n",
    "    logger.info(f\"Random seed set to {config['training']['random_state']}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 2: INITIALIZE DATA STRUCTURES\n",
    "    # -------------------------------------------------------------------------\n",
    "    # List to store predictions from each outer fold\n",
    "    output_dfs = []\n",
    "\n",
    "    outer_fold_scores = []\n",
    "    \n",
    "    # Dictionary to store all fold indices for reproducibility and student training\n",
    "    # Structure: {\"outer_folds\": {fold_id: {train_idx, test_idx}},\n",
    "    #            \"inner_folds\": {outer_fold_id: {inner_fold_id: {train_idx, val_idx}}}}\n",
    "    fold_indices = {\n",
    "        \"outer_folds\": {},\n",
    "        \"inner_folds\": {}\n",
    "    }\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 3: OUTER CROSS-VALIDATION SETUP\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Choose appropriate CV strategy based on task type to maintain class balance\n",
    "    if task_type == \"binary\":\n",
    "        outer_cv = StratifiedKFold(\n",
    "            n_splits=config[\"training\"][\"outer_folds\"],\n",
    "            shuffle=True,\n",
    "            random_state=config[\"training\"][\"random_state\"],\n",
    "        )\n",
    "    else:\n",
    "        outer_cv = KFold(\n",
    "            n_splits=config[\"training\"][\"outer_folds\"],\n",
    "            shuffle=True,\n",
    "            random_state=config[\"training\"][\"random_state\"],\n",
    "        )\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 4: OUTER CROSS-VALIDATION LOOP\n",
    "    # =========================================================================\n",
    "    # Each iteration provides one unbiased performance estimate\n",
    "    for outer_fold, (train_idx, test_idx) in enumerate(outer_cv.split(X, y), start=1):\n",
    "\n",
    "        # Log the first 10 indices of the outer fold for debugging\n",
    "        logger.info(f\"Outer Fold {outer_fold} - Train Indices: {train_idx[:10]}, Test Indices: {test_idx[:10]}\")\n",
    "\n",
    "        logger.info(f\"-------------------- Outer Fold {outer_fold} --------------------\")\n",
    "        \n",
    "        # ---------------------------------------------------------------------\n",
    "        # FOLD INDEX MANAGEMENT\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Store outer fold indices for later use in student training and validation\n",
    "        fold_indices[\"outer_folds\"][f\"fold_{outer_fold}\"] = {\n",
    "            \"train_idx\": train_idx.tolist(),\n",
    "            \"test_idx\": test_idx.tolist()\n",
    "        }\n",
    "\n",
    "        # Split data according to current outer fold\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # INNER CROSS-VALIDATION SETUP (for model validation)\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Choose appropriate CV strategy for inner folds\n",
    "        if task_type == \"binary\":\n",
    "            inner_cv = StratifiedKFold(\n",
    "                n_splits=config[\"training\"][\"inner_folds\"],\n",
    "                shuffle=True,\n",
    "                random_state=config[\"training\"][\"random_state\"],\n",
    "            )\n",
    "        else:\n",
    "            inner_cv = KFold(\n",
    "                n_splits=config[\"training\"][\"inner_folds\"],\n",
    "                shuffle=True,\n",
    "                random_state=config[\"training\"][\"random_state\"],\n",
    "            )\n",
    "        \n",
    "        # Initialize storage for inner fold indices within this outer fold\n",
    "        fold_indices[\"inner_folds\"][f\"outer_fold_{outer_fold}\"] = {}\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 5: INNER CROSS-VALIDATION LOOP (hyperparameter validation) \n",
    "        # =====================================================================\n",
    "        def objective(trial):\n",
    "\n",
    "            hyperparams = suggest_hyperparameters(\n",
    "                trial=trial,\n",
    "                model_type=model_type,\n",
    "                task_type=task_type,\n",
    "                use_hpo=use_hpo,\n",
    "            )\n",
    "\n",
    "            inner_fold_scores = []\n",
    "            val_metrics_list = []\n",
    "\n",
    "            # This loop would typically be used for hyperparameter optimization\n",
    "            for inner_fold, (inner_train_index, inner_val_index) in enumerate(inner_cv.split(X_train, y_train), start=1):\n",
    "                \n",
    "                # -----------------------------------------------------------------\n",
    "                # INDEX MANAGEMENT (Critical for avoiding data leakage)\n",
    "                # -----------------------------------------------------------------\n",
    "                # Convert relative indices (within outer training set) to absolute indices\n",
    "                absolute_inner_train_idx = train_idx[inner_train_index]\n",
    "                absolute_inner_val_idx = train_idx[inner_val_index]\n",
    "                \n",
    "                # Store inner fold indices using absolute indices for consistency\n",
    "                # Only store indices for the first trial\n",
    "                if trial.number == 0:  \n",
    "                    fold_indices[\"inner_folds\"][f\"outer_fold_{outer_fold}\"][f\"inner_fold_{inner_fold}\"] = {\n",
    "                        \"train_idx\": absolute_inner_train_idx.tolist(),\n",
    "                        \"val_idx\": absolute_inner_val_idx.tolist()\n",
    "                    }\n",
    "\n",
    "                # Split inner training data using relative indices\n",
    "                X_inner_train, X_inner_val = X_train.iloc[inner_train_index], X_train.iloc[inner_val_index]\n",
    "                y_inner_train, y_inner_val = y_train[inner_train_index], y_train[inner_val_index]\n",
    "\n",
    "                # -----------------------------------------------------------------\n",
    "                # PREPROCESSING: Apply model-specific data transformations\n",
    "                # -----------------------------------------------------------------\n",
    "                X_inner_train, X_inner_val = preprocess(\n",
    "                    X_inner_train,\n",
    "                    y_inner_train,\n",
    "                    X_inner_val, \n",
    "                    cat_cols,\n",
    "                    config,\n",
    "                    preprocessing_type=preprocessing_type,\n",
    "                )\n",
    "\n",
    "                # -----------------------------------------------------------------\n",
    "                # MODEL TRAINING: Train teacher model on inner training data\n",
    "                # -----------------------------------------------------------------\n",
    "                model = get_teacher_model(config=config, task_type=task_type, device=device, hyperparams=hyperparams, cat_cols=cat_cols)\n",
    "                logger.info(f\"Training Model on Outer Fold {outer_fold}, Inner Fold {inner_fold}...\")\n",
    "                model.train(X_inner_train, y_inner_train)\n",
    "\n",
    "                # -----------------------------------------------------------------\n",
    "                # VALIDATION: Evaluate model performance on inner validation set\n",
    "                # -----------------------------------------------------------------\n",
    "                val_preds = model.predict(X_inner_val)\n",
    "                val_metrics = model.evaluate(val_preds, y_inner_val)\n",
    "\n",
    "                # Store metrics from this fold for later mean calculation\n",
    "                val_metrics_list.append(val_metrics)\n",
    "\n",
    "                if task_type == \"binary\":\n",
    "                    inner_fold_scores.append(val_metrics[\"f1\"])\n",
    "                    trial.report(np.mean(inner_fold_scores), step=inner_fold)\n",
    "                else:\n",
    "                    inner_fold_scores.append(-val_metrics[\"mae\"])\n",
    "                    trial.report(np.mean(inner_fold_scores), step=inner_fold)\n",
    "\n",
    "                # Check if the trial should be pruned (With 2 inner folds, pruning not recommended)\n",
    "                # if trial.should_prune():\n",
    "                #     logger.info(\"Trial pruned.\")\n",
    "                #     raise optuna.TrialPruned()\n",
    "\n",
    "            # Calculate and set mean metrics as user attributes after all inner folds\n",
    "            if val_metrics_list:\n",
    "                # Get all metric keys from the first fold\n",
    "                metric_keys = val_metrics_list[0].keys()\n",
    "                \n",
    "                for metric_key in metric_keys:\n",
    "                    # Calculate mean across all inner folds for this metric\n",
    "                    metric_values = [fold_metrics[metric_key] for fold_metrics in val_metrics_list]\n",
    "                    mean_metric = np.mean(metric_values)\n",
    "                    trial.set_user_attr(f\"mean_{metric_key}\", mean_metric)\n",
    "\n",
    "            return np.mean(inner_fold_scores)\n",
    "            \n",
    "        logger.info(f\"Starting hyperparameter optimization for outer fold {outer_fold}...\")\n",
    "\n",
    "        study_kwargs = dict(\n",
    "            direction=\"maximize\",\n",
    "            study_name=f\"{dataset_id}.{outer_fold}.{model_type}\",\n",
    "            load_if_exists=True,\n",
    "            sampler=optuna.samplers.TPESampler(seed=config[\"training\"][\"random_state\"]),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps=1),\n",
    "        )\n",
    "\n",
    "        if use_hpo:\n",
    "            os.makedirs(config[\"data\"][\"optuna_db_path\"], exist_ok=True)\n",
    "            study_kwargs[\"storage\"] = f\"sqlite:///{config['data']['optuna_db_path']}/optuna.db\"\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            **study_kwargs,\n",
    "        )\n",
    "\n",
    "        # Optimize\n",
    "        completed_trials = len(study.trials)\n",
    "        remaining_trials = config[\"training\"][\"trials\"] - completed_trials\n",
    "\n",
    "        if remaining_trials > 0:\n",
    "            default_hyperparams = suggest_hyperparameters(None, model_type, task_type, False)\n",
    "            study.enqueue_trial(default_hyperparams)\n",
    "            study.optimize(\n",
    "                objective, \n",
    "                n_trials=remaining_trials if use_hpo else 1,\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\"The study has already reached the maximum number of trials.\")\n",
    "\n",
    "        if use_hpo:\n",
    "            # When HPO is used, trial.suggest_ methods are called, populating trial.params\n",
    "            best_hyperparams = study.best_trial.params \n",
    "        else:\n",
    "            # When not using HPO, a single trial runs with default parameters.\n",
    "            best_hyperparams = suggest_hyperparameters(trial=None, model_type=model_type, task_type=task_type, use_hpo=False)\n",
    "\n",
    "        logger.info(f\"Best hyperparameters for fold {outer_fold}: {best_hyperparams}\")\n",
    "        logger.info(f\"Best score: {study.best_value:.4f}\")\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 6: FINAL MODEL TRAINING (on complete outer training set)\n",
    "        # =====================================================================        \n",
    "        # Apply same preprocessing to outer training and test sets\n",
    "        X_train, X_test = preprocess(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            cat_cols,\n",
    "            config,\n",
    "            preprocessing_type=preprocessing_type,  \n",
    "        )\n",
    "        \n",
    "        # Train final model on complete outer training set\n",
    "        logger.info(\"------------------------------------------------------\")\n",
    "        logger.info(f\"Retraining Model on Outer Fold {outer_fold}\")\n",
    "\n",
    "        model = get_teacher_model(\n",
    "            config=config,\n",
    "            task_type=task_type,\n",
    "            device=device,\n",
    "            hyperparams=best_hyperparams,  # Use best hyperparameters from HPO\n",
    "            cat_cols=cat_cols,  \n",
    "        )\n",
    "        model.train(X_train, y_train)\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 7: FINAL EVALUATION (unbiased performance on outer test set)\n",
    "        # =====================================================================\n",
    "        # This provides the unbiased performance estimate for this fold\n",
    "        start_time = time.time()\n",
    "        test_preds = model.predict(X_test)\n",
    "        end_time = time.time() - start_time\n",
    "        logger.info(f\"\\t Inference Time: {end_time:.5f} seconds\")\n",
    "        test_metrics = model.evaluate(test_preds, y_test)\n",
    "\n",
    "        # Store outer fold score for later analysis\n",
    "        outer_fold_results = {\n",
    "            \"fold\": outer_fold,\n",
    "            \"seed\": config[\"training\"][\"random_state\"],\n",
    "            \"inference_time\": end_time,\n",
    "            **test_metrics,\n",
    "        }\n",
    "        outer_fold_scores.append(outer_fold_results)\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 8: STORE PREDICTIONS FOR STUDENT TRAINING\n",
    "        # =====================================================================\n",
    "        # Save predictions with their corresponding dataset indices\n",
    "        # These will be used as targets for training student models\n",
    "        output_dfs.append(pd.DataFrame({\n",
    "            \"index\": test_idx,\n",
    "            \"output\": test_preds[:, 1] if task_type == \"binary\" else test_preds\n",
    "        }))\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 9: SAVE RESULTS AND METADATA\n",
    "    # =========================================================================\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # SAVE OUTER FOLD METRICS\n",
    "    # -------------------------------------------------------------------------\n",
    "    outer_fold_df = pd.DataFrame(outer_fold_scores)\n",
    "    sub_folder = \"hpo\" if use_hpo else \"default\"\n",
    "    output_dir = os.path.join(config[\"data\"][\"outer_folds_path\"], \"teacher\", sub_folder)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    metrics_file = os.path.join(output_dir, f\"{dataset_id}_{model_type}.csv\")\n",
    "    outer_fold_df.to_csv(metrics_file, index=False)\n",
    "    logger.info(f\"Outer fold metrics saved to: {metrics_file}\")\n",
    "\n",
    "    # Calculate and log overall performance across all folds\n",
    "    mean_inference_time = outer_fold_df['inference_time'].mean()\n",
    "    std_inference_time = outer_fold_df['inference_time'].std()\n",
    "    mean_parameters = outer_fold_df['parameters'].mean()\n",
    "    std_parameters = outer_fold_df['parameters'].std()\n",
    "\n",
    "    if task_type == \"binary\":\n",
    "        mean_acc = outer_fold_df['acc'].mean()\n",
    "        std_acc = outer_fold_df['acc'].std()\n",
    "        mean_f1 = outer_fold_df['f1'].mean()\n",
    "        std_f1 = outer_fold_df['f1'].std()\n",
    "        mean_roc = outer_fold_df['roc'].mean()\n",
    "        std_roc = outer_fold_df['roc'].std()\n",
    "        \n",
    "        logger.info(f\"=== FINAL RESULTS FOR DATASET {dataset_id} ===\")\n",
    "        logger.info(f\"Balanced Accuracy: {mean_acc:.4f} ± {std_acc:.4f}\")\n",
    "        logger.info(f\"F1 Score: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "        logger.info(f\"ROC AUC: {mean_roc:.4f} ± {std_roc:.4f}\")\n",
    "    else:\n",
    "        mean_mae = outer_fold_df['mae'].mean()\n",
    "        std_mae = outer_fold_df['mae'].std()\n",
    "        mean_mse = outer_fold_df['mse'].mean()\n",
    "        std_mse = outer_fold_df['mse'].std()\n",
    "        mean_rmse = outer_fold_df['rmse'].mean()\n",
    "        std_rmse = outer_fold_df['rmse'].std()\n",
    "        mean_r2 = outer_fold_df['r2'].mean()\n",
    "        std_r2 = outer_fold_df['r2'].std()\n",
    "\n",
    "        \n",
    "        logger.info(f\"=== FINAL RESULTS FOR DATASET {dataset_id} ===\")\n",
    "        logger.info(f\"MAE: {mean_mae:.4f} ± {std_mae:.4f}\")\n",
    "        logger.info(f\"MSE: {mean_mse:.4f} ± {std_mse:.4f}\")\n",
    "        logger.info(f\"RMSE: {mean_rmse:.4f} ± {std_rmse:.4f}\")\n",
    "        logger.info(f\"R2: {mean_r2:.4f} ± {std_r2:.4f}\")\n",
    "    \n",
    "    logger.info(f\"Mean Inference Time: {mean_inference_time:.4f} ± {std_inference_time:.4f}\")\n",
    "    logger.info(f\"Mean Parameters: {mean_parameters:.4f} ± {std_parameters:.4f}\")\n",
    "\n",
    "    # Save summary statistics as well\n",
    "    summary_stats = {\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"model_type\": model_type,\n",
    "        \"task_type\": task_type,\n",
    "        \"seed\": config[\"training\"][\"random_state\"],\n",
    "        \"use_hpo\": config[\"training\"][\"use_hpo\"],\n",
    "        \"mean_inference_time\": mean_inference_time,\n",
    "        \"std_inference_time\": std_inference_time,\n",
    "        \"mean_parameters\": mean_parameters,\n",
    "        \"std_parameters\": std_parameters,\n",
    "    }\n",
    "\n",
    "    if task_type == \"binary\":\n",
    "        summary_stats.update({\n",
    "            \"mean_acc\": mean_acc,\n",
    "            \"std_acc\": std_acc,\n",
    "            \"mean_f1\": mean_f1,\n",
    "            \"std_f1\": std_f1,\n",
    "            \"mean_roc\": mean_roc,\n",
    "            \"std_roc\": std_roc,\n",
    "        })\n",
    "    else:\n",
    "        summary_stats.update({\n",
    "            \"mean_mae\": mean_mae,\n",
    "            \"std_mae\": std_mae,\n",
    "            \"mean_mse\": mean_mse,\n",
    "            \"std_mse\": std_mse,\n",
    "            \"mean_rmse\": mean_rmse,\n",
    "            \"std_rmse\": std_rmse,\n",
    "            \"mean_r2\": mean_r2,\n",
    "            \"std_r2\": std_r2,\n",
    "        })\n",
    "\n",
    "    # Load existing summary file if it exists, otherwise create new\n",
    "    summary_file = os.path.join(config[\"data\"][\"results_dir_path\"], \"teacher\", f\"{dataset_id}_results.json\")\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(summary_file), exist_ok=True)\n",
    "    if os.path.exists(summary_file):\n",
    "        with open(summary_file, 'r') as f:\n",
    "            all_results = json.load(f)\n",
    "    else:\n",
    "        all_results = {}\n",
    "\n",
    "    # Add current model results to the dataset summary\n",
    "    # Use simple incremental numbering\n",
    "    next_num = len(all_results) + 1\n",
    "    model_key = str(next_num)\n",
    "\n",
    "    # Add current model results to the dataset summary\n",
    "    all_results[model_key] = summary_stats\n",
    "\n",
    "    # Save updated summary\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    logger.info(f\"Summary statistics saved to: {summary_file}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # SAVE FOLD INDICES (for reproducibility and student training)\n",
    "    # -------------------------------------------------------------------------\n",
    "    fold_indices_file = os.path.join(config[\"data\"][\"fold_indices_path\"], f\"dataset_{dataset_id}.json\")\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(config[\"data\"][\"fold_indices_path\"], exist_ok=True)\n",
    "    if not os.path.exists(fold_indices_file):\n",
    "        with open(fold_indices_file, 'w') as f:\n",
    "            json.dump(fold_indices, f, indent=2)\n",
    "        logger.info(f\"Fold indices saved to: {fold_indices_file}\")\n",
    "    else:\n",
    "        logger.info(f\"Fold indices file already exists: {fold_indices_file}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # SAVE TEACHER PREDICTIONS (targets for student training)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Combine predictions from all outer folds\n",
    "    if output_dfs:  # Check if we have any DataFrames to concatenate\n",
    "        output_df = pd.concat(output_dfs, ignore_index=True)\n",
    "    else:\n",
    "        output_df = pd.DataFrame(columns=[\"index\", \"output\"])\n",
    "    output_df = output_df.sort_values(by=\"index\")\n",
    "    \n",
    "    # Create the full directory structure\n",
    "    sub_folder = \"hpo\" if use_hpo else \"default\"\n",
    "    output_dir = os.path.join(config[\"data\"][\"output_dir_path\"], sub_folder, \"teacher\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    output_file = os.path.join(output_dir, f\"{dataset_id}_{model_type}.csv\")\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "    logger.info(f\"{config['model']['teacher_model']} outputs saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c1e150",
   "metadata": {},
   "source": [
    "# Train Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289fc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:38:39,611 - __main__ - INFO - Loading configuration...\n",
      "2025-06-06 11:38:39,612 - __main__ - INFO - Loading dataset 23381 from cache at 'data/cache/openml_dataset_23381.pkl'...\n",
      "2025-06-06 11:38:39,614 - __main__ - INFO - Dataset 23381 loaded successfully with task type: binary\n",
      "2025-06-06 11:38:39,615 - __main__ - INFO - Using GPU: NVIDIA GeForce RTX 2080 Ti\n",
      "2025-06-06 11:38:39,616 - __main__ - INFO - Random seed set to 42\n",
      "2025-06-06 11:38:39,619 - __main__ - INFO - Loaded fold indices from: data/fold_indices/dataset_23381.json\n",
      "2025-06-06 11:38:39,621 - __main__ - INFO - Loaded tabpfn outputs from: data/output/hpo/teacher/23381_tabpfn.csv\n",
      "2025-06-06 11:38:39,622 - __main__ - INFO - Outer Fold 1 - Train Indices: [ 1  3  5  6  7  8  9 10 11 12], Test Indices: [ 0  2  4 18 19 27 35 39 57 59]\n",
      "2025-06-06 11:38:39,623 - __main__ - INFO - -------------------- Outer Fold 1 --------------------\n",
      "2025-06-06 11:38:39,624 - __main__ - INFO - Starting hyperparameter optimization for outer fold 1...\n",
      "[I 2025-06-06 11:38:39,679] A new study created in RDB with name: 23381.1.grande(tabpfn).re\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:38:39,817 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:38:39,818 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:38:40.010593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6753 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:83:00.0, compute capability: 7.5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749202725.382210 2089115 service.cc:145] XLA service 0x7fae68014c40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1749202725.382263 2089115 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2025-06-06 11:38:45.488689: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-06-06 11:38:45.859701: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "I0000 00:00:1749202726.789521 2089115 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:38:54,153 - __main__ - INFO - \t Balanced Accuracy: 0.5782, F1 Score: 0.5597, ROC AUC: 0.6962\n",
      "2025-06-06 11:38:54,159 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.7824, F1 Score: 0.8108, ROC AUC: 0.9757, KL Divergence: 0.0080\n",
      "2025-06-06 11:38:54,161 - __main__ - INFO - \t Fidelity - MAE: 0.1909, MSE: 0.0686, RMSE: 0.2620, R^2: 0.7019\n",
      "2025-06-06 11:38:54,176 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:38:54,176 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:39:04,801 - __main__ - INFO - \t Balanced Accuracy: 0.5938, F1 Score: 0.5711, ROC AUC: 0.6739\n",
      "2025-06-06 11:39:04,807 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.8813, F1 Score: 0.9078, ROC AUC: 0.9912, KL Divergence: 0.0053\n",
      "2025-06-06 11:39:04,809 - __main__ - INFO - \t Fidelity - MAE: 0.1541, MSE: 0.0473, RMSE: 0.2174, R^2: 0.7282\n",
      "[I 2025-06-06 11:39:04,959] Trial 0 finished with value: -0.1725302307846853 and parameters: {'depth': 5, 'n_estimators': 2048, 'learning_rate_weights': 0.005, 'learning_rate_index': 0.01, 'learning_rate_values': 0.01, 'learning_rate_leaf': 0.01, 'cosine_decay_steps': 0.0, 'dropout': 0.0, 'selected_variables': 0.8, 'data_subset_fraction': 1.0}. Best is trial 0 with value: -0.1725302307846853.\n",
      "2025-06-06 11:39:05,102 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:39:05,103 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "W0000 00:00:1749202748.463336 2089116 random_ops.cc:59] Warning: Using tf.random.uniform with XLA compilation will ignore seeds; consider using tf.random.stateless_uniform instead if reproducible behavior is desired. grande_2_1/dropout/random_uniform/RandomUniform\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fafb856b3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:39:20,041 - tensorflow - WARNING - 5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fafb856b3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fafb856b3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 11:39:20,713 - tensorflow - WARNING - 6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fafb856b3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2025-06-06 11:39:20,753 - __main__ - INFO - \t Balanced Accuracy: 0.5000, F1 Score: 0.3671, ROC AUC: 0.6987\n",
      "2025-06-06 11:39:20,759 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.5000, F1 Score: 0.4135, ROC AUC: 0.9317, KL Divergence: 0.0175\n",
      "2025-06-06 11:39:20,761 - __main__ - INFO - \t Fidelity - MAE: 0.2749, MSE: 0.1449, RMSE: 0.3806, R^2: 0.3709\n",
      "2025-06-06 11:39:20,774 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:39:20,775 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:39:36,553 - __main__ - INFO - \t Balanced Accuracy: 0.5868, F1 Score: 0.5675, ROC AUC: 0.6325\n",
      "2025-06-06 11:39:36,558 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9156, F1 Score: 0.9276, ROC AUC: 0.9870, KL Divergence: 0.0069\n",
      "2025-06-06 11:39:36,560 - __main__ - INFO - \t Fidelity - MAE: 0.1615, MSE: 0.0602, RMSE: 0.2453, R^2: 0.6539\n",
      "[I 2025-06-06 11:39:36,706] Trial 1 finished with value: -0.21819283613730311 and parameters: {'depth': 4, 'n_estimators': 3920, 'learning_rate_weights': 0.009454306819536176, 'learning_rate_index': 0.02385234757844707, 'learning_rate_values': 0.0022856175997064752, 'learning_rate_leaf': 0.002285325525633921, 'cosine_decay_steps': 0.1, 'dropout': 0.7274323891214958, 'selected_variables': 0.8324426408004217, 'data_subset_fraction': 0.29110519961044856}. Best is trial 0 with value: -0.1725302307846853.\n",
      "2025-06-06 11:39:36,841 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:39:36,842 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:39:49,442 - __main__ - INFO - \t Balanced Accuracy: 0.6059, F1 Score: 0.6017, ROC AUC: 0.7015\n",
      "2025-06-06 11:39:49,448 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.8855, F1 Score: 0.8952, ROC AUC: 0.9838, KL Divergence: 0.0065\n",
      "2025-06-06 11:39:49,451 - __main__ - INFO - \t Fidelity - MAE: 0.1676, MSE: 0.0564, RMSE: 0.2374, R^2: 0.7552\n",
      "2025-06-06 11:39:49,463 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:39:49,464 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:40:02,739 - __main__ - INFO - \t Balanced Accuracy: 0.5868, F1 Score: 0.5675, ROC AUC: 0.6620\n",
      "2025-06-06 11:40:02,745 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9156, F1 Score: 0.9276, ROC AUC: 0.9902, KL Divergence: 0.0047\n",
      "2025-06-06 11:40:02,747 - __main__ - INFO - \t Fidelity - MAE: 0.1436, MSE: 0.0418, RMSE: 0.2044, R^2: 0.7599\n",
      "[I 2025-06-06 11:40:02,890] Trial 2 finished with value: -0.15559717006796292 and parameters: {'depth': 3, 'n_estimators': 1169, 'learning_rate_weights': 0.0006624310605949989, 'learning_rate_index': 0.016124278458562614, 'learning_rate_values': 0.009860942908083906, 'learning_rate_leaf': 0.004678719265016202, 'cosine_decay_steps': 0.0, 'dropout': 0.5888819710447601, 'selected_variables': 0.19967378215835974, 'data_subset_fraction': 0.5628109945722505}. Best is trial 2 with value: -0.15559717006796292.\n",
      "2025-06-06 11:40:03,021 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:40:03,022 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:40:11,973 - __main__ - INFO - \t Balanced Accuracy: 0.5786, F1 Score: 0.5449, ROC AUC: 0.6908\n",
      "2025-06-06 11:40:11,979 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.5846, F1 Score: 0.5852, ROC AUC: 0.7840, KL Divergence: 0.0216\n",
      "2025-06-06 11:40:11,981 - __main__ - INFO - \t Fidelity - MAE: 0.3168, MSE: 0.1804, RMSE: 0.4247, R^2: 0.2167\n",
      "2025-06-06 11:40:11,993 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:40:11,994 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:40:20,890 - __main__ - INFO - \t Balanced Accuracy: 0.5534, F1 Score: 0.5325, ROC AUC: 0.6022\n",
      "2025-06-06 11:40:20,896 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.6875, F1 Score: 0.6875, ROC AUC: 0.8389, KL Divergence: 0.0210\n",
      "2025-06-06 11:40:20,898 - __main__ - INFO - \t Fidelity - MAE: 0.3101, MSE: 0.1773, RMSE: 0.4211, R^2: -0.0193\n",
      "[I 2025-06-06 11:40:21,039] Trial 3 finished with value: -0.31343375734140466 and parameters: {'depth': 5, 'n_estimators': 678, 'learning_rate_weights': 0.004362599362560562, 'learning_rate_index': 0.002468204407989012, 'learning_rate_values': 0.001411515549344833, 'learning_rate_leaf': 0.15255065745117383, 'cosine_decay_steps': 0.0, 'dropout': 0.330114370304701, 'selected_variables': 0.12203823484477883, 'data_subset_fraction': 0.5456592191001431}. Best is trial 2 with value: -0.15559717006796292.\n",
      "2025-06-06 11:40:21,154 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:40:21,155 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:40:31,267 - __main__ - INFO - \t Balanced Accuracy: 0.5000, F1 Score: 0.3671, ROC AUC: 0.7054\n",
      "2025-06-06 11:40:31,273 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.5000, F1 Score: 0.4135, ROC AUC: 0.8251, KL Divergence: 0.0210\n",
      "2025-06-06 11:40:31,275 - __main__ - INFO - \t Fidelity - MAE: 0.3077, MSE: 0.1742, RMSE: 0.4174, R^2: 0.2435\n",
      "2025-06-06 11:40:31,286 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:40:31,287 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:40:45,679 - __main__ - INFO - \t Balanced Accuracy: 0.5766, F1 Score: 0.5557, ROC AUC: 0.6644\n",
      "2025-06-06 11:40:45,685 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.8844, F1 Score: 0.8955, ROC AUC: 0.9853, KL Divergence: 0.0062\n",
      "2025-06-06 11:40:45,687 - __main__ - INFO - \t Fidelity - MAE: 0.1652, MSE: 0.0543, RMSE: 0.2329, R^2: 0.6880\n",
      "[I 2025-06-06 11:40:45,825] Trial 4 finished with value: -0.23644378859807358 and parameters: {'depth': 3, 'n_estimators': 3771, 'learning_rate_weights': 0.0004993895014755662, 'learning_rate_index': 0.03345674213696821, 'learning_rate_values': 0.00521502669741166, 'learning_rate_leaf': 0.015728674194978587, 'cosine_decay_steps': 1.0, 'dropout': 0.6711205128207366, 'selected_variables': 0.5978999788110851, 'data_subset_fraction': 0.9296868115208051}. Best is trial 2 with value: -0.15559717006796292.\n",
      "2025-06-06 11:40:45,836 - __main__ - INFO - Best hyperparameters for fold 1: {'depth': 3, 'n_estimators': 1169, 'learning_rate_weights': 0.0006624310605949989, 'learning_rate_index': 0.016124278458562614, 'learning_rate_values': 0.009860942908083906, 'learning_rate_leaf': 0.004678719265016202, 'cosine_decay_steps': 0.0, 'dropout': 0.5888819710447601, 'selected_variables': 0.19967378215835974, 'data_subset_fraction': 0.5628109945722505}\n",
      "2025-06-06 11:40:45,846 - __main__ - INFO - Best score: -0.1556\n",
      "2025-06-06 11:40:45,846 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:40:45,847 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-06 11:40:45,847 - __main__ - INFO - Retraining Model on Outer Fold 1\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:40:56,881 - __main__ - INFO - \t Inference Time: 1.25282 seconds\n",
      "2025-06-06 11:40:56,887 - __main__ - INFO - \t Balanced Accuracy: 0.5903, F1 Score: 0.5877, ROC AUC: 0.6905\n",
      "2025-06-06 11:40:56,893 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9688, F1 Score: 0.9766, ROC AUC: 0.9995, KL Divergence: 0.0024\n",
      "2025-06-06 11:40:56,895 - __main__ - INFO - \t Fidelity - MAE: 0.1111, MSE: 0.0209, RMSE: 0.1446, R^2: 0.9091\n",
      "2025-06-06 11:40:56,896 - __main__ - INFO - Outer Fold 2 - Train Indices: [ 0  1  2  3  4  5  7  8  9 13], Test Indices: [ 6 10 11 12 15 16 20 21 36 45]\n",
      "2025-06-06 11:40:56,896 - __main__ - INFO - -------------------- Outer Fold 2 --------------------\n",
      "2025-06-06 11:40:56,898 - __main__ - INFO - Starting hyperparameter optimization for outer fold 2...\n",
      "[I 2025-06-06 11:40:56,950] A new study created in RDB with name: 23381.2.grande(tabpfn).re\n",
      "2025-06-06 11:40:57,092 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:40:57,093 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:41:07,331 - __main__ - INFO - \t Balanced Accuracy: 0.6394, F1 Score: 0.6366, ROC AUC: 0.7155\n",
      "2025-06-06 11:41:07,337 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9469, F1 Score: 0.9548, ROC AUC: 0.9910, KL Divergence: 0.0023\n",
      "2025-06-06 11:41:07,339 - __main__ - INFO - \t Fidelity - MAE: 0.1071, MSE: 0.0193, RMSE: 0.1390, R^2: 0.8791\n",
      "2025-06-06 11:41:07,353 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:41:07,354 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:41:18,753 - __main__ - INFO - \t Balanced Accuracy: 0.5901, F1 Score: 0.5753, ROC AUC: 0.6993\n",
      "2025-06-06 11:41:18,759 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.8812, F1 Score: 0.9089, ROC AUC: 0.9904, KL Divergence: 0.0028\n",
      "2025-06-06 11:41:18,761 - __main__ - INFO - \t Fidelity - MAE: 0.1123, MSE: 0.0231, RMSE: 0.1520, R^2: 0.8702\n",
      "[I 2025-06-06 11:41:18,906] Trial 0 finished with value: -0.1096847962533699 and parameters: {'depth': 5, 'n_estimators': 2048, 'learning_rate_weights': 0.005, 'learning_rate_index': 0.01, 'learning_rate_values': 0.01, 'learning_rate_leaf': 0.01, 'cosine_decay_steps': 0.0, 'dropout': 0.0, 'selected_variables': 0.8, 'data_subset_fraction': 1.0}. Best is trial 0 with value: -0.1096847962533699.\n",
      "2025-06-06 11:41:19,028 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:41:19,029 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:41:34,264 - __main__ - INFO - \t Balanced Accuracy: 0.6367, F1 Score: 0.6349, ROC AUC: 0.6668\n",
      "2025-06-06 11:41:34,269 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9493, F1 Score: 0.9493, ROC AUC: 0.9891, KL Divergence: 0.0036\n",
      "2025-06-06 11:41:34,271 - __main__ - INFO - \t Fidelity - MAE: 0.1251, MSE: 0.0300, RMSE: 0.1732, R^2: 0.8123\n",
      "2025-06-06 11:41:34,283 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:41:34,283 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:41:49,377 - __main__ - INFO - \t Balanced Accuracy: 0.6026, F1 Score: 0.5960, ROC AUC: 0.6226\n",
      "2025-06-06 11:41:49,383 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9288, F1 Score: 0.9342, ROC AUC: 0.9754, KL Divergence: 0.0055\n",
      "2025-06-06 11:41:49,385 - __main__ - INFO - \t Fidelity - MAE: 0.1484, MSE: 0.0460, RMSE: 0.2145, R^2: 0.7416\n",
      "[I 2025-06-06 11:41:49,532] Trial 1 finished with value: -0.13674122791891863 and parameters: {'depth': 4, 'n_estimators': 3920, 'learning_rate_weights': 0.009454306819536176, 'learning_rate_index': 0.02385234757844707, 'learning_rate_values': 0.0022856175997064752, 'learning_rate_leaf': 0.002285325525633921, 'cosine_decay_steps': 0.1, 'dropout': 0.7274323891214958, 'selected_variables': 0.8324426408004217, 'data_subset_fraction': 0.29110519961044856}. Best is trial 0 with value: -0.1096847962533699.\n",
      "2025-06-06 11:41:49,655 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:41:49,655 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:42:01,111 - __main__ - INFO - \t Balanced Accuracy: 0.6281, F1 Score: 0.6264, ROC AUC: 0.7108\n",
      "2025-06-06 11:42:01,117 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9678, F1 Score: 0.9624, ROC AUC: 0.9952, KL Divergence: 0.0021\n",
      "2025-06-06 11:42:01,119 - __main__ - INFO - \t Fidelity - MAE: 0.1043, MSE: 0.0179, RMSE: 0.1339, R^2: 0.8879\n",
      "2025-06-06 11:42:01,132 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:42:01,133 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:42:15,259 - __main__ - INFO - \t Balanced Accuracy: 0.6129, F1 Score: 0.6072, ROC AUC: 0.6831\n",
      "2025-06-06 11:42:15,264 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9418, F1 Score: 0.9474, ROC AUC: 0.9905, KL Divergence: 0.0022\n",
      "2025-06-06 11:42:15,266 - __main__ - INFO - \t Fidelity - MAE: 0.1050, MSE: 0.0190, RMSE: 0.1377, R^2: 0.8935\n",
      "[I 2025-06-06 11:42:15,418] Trial 2 finished with value: -0.10465762300389359 and parameters: {'depth': 3, 'n_estimators': 1169, 'learning_rate_weights': 0.0006624310605949989, 'learning_rate_index': 0.016124278458562614, 'learning_rate_values': 0.009860942908083906, 'learning_rate_leaf': 0.004678719265016202, 'cosine_decay_steps': 0.0, 'dropout': 0.5888819710447601, 'selected_variables': 0.19967378215835974, 'data_subset_fraction': 0.5628109945722505}. Best is trial 2 with value: -0.10465762300389359.\n",
      "2025-06-06 11:42:15,545 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:42:15,546 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:42:24,114 - __main__ - INFO - \t Balanced Accuracy: 0.5788, F1 Score: 0.5670, ROC AUC: 0.6713\n",
      "2025-06-06 11:42:24,119 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.7103, F1 Score: 0.7200, ROC AUC: 0.8519, KL Divergence: 0.0144\n",
      "2025-06-06 11:42:24,121 - __main__ - INFO - \t Fidelity - MAE: 0.2484, MSE: 0.1180, RMSE: 0.3435, R^2: 0.2620\n",
      "2025-06-06 11:42:24,134 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:42:24,135 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:42:33,336 - __main__ - INFO - \t Balanced Accuracy: 0.5355, F1 Score: 0.5083, ROC AUC: 0.6452\n",
      "2025-06-06 11:42:33,341 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.6479, F1 Score: 0.6604, ROC AUC: 0.8223, KL Divergence: 0.0149\n",
      "2025-06-06 11:42:33,343 - __main__ - INFO - \t Fidelity - MAE: 0.2654, MSE: 0.1224, RMSE: 0.3499, R^2: 0.3126\n",
      "[I 2025-06-06 11:42:33,506] Trial 3 finished with value: -0.2569121482949203 and parameters: {'depth': 5, 'n_estimators': 678, 'learning_rate_weights': 0.004362599362560562, 'learning_rate_index': 0.002468204407989012, 'learning_rate_values': 0.001411515549344833, 'learning_rate_leaf': 0.15255065745117383, 'cosine_decay_steps': 0.0, 'dropout': 0.330114370304701, 'selected_variables': 0.12203823484477883, 'data_subset_fraction': 0.5456592191001431}. Best is trial 2 with value: -0.10465762300389359.\n",
      "2025-06-06 11:42:33,638 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:42:33,639 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:42:47,504 - __main__ - INFO - \t Balanced Accuracy: 0.6383, F1 Score: 0.6374, ROC AUC: 0.7045\n",
      "2025-06-06 11:42:47,510 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9424, F1 Score: 0.9373, ROC AUC: 0.9926, KL Divergence: 0.0024\n",
      "2025-06-06 11:42:47,513 - __main__ - INFO - \t Fidelity - MAE: 0.1094, MSE: 0.0204, RMSE: 0.1428, R^2: 0.8725\n",
      "2025-06-06 11:42:47,525 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:42:47,525 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:43:01,544 - __main__ - INFO - \t Balanced Accuracy: 0.6172, F1 Score: 0.6114, ROC AUC: 0.6884\n",
      "2025-06-06 11:43:01,549 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9192, F1 Score: 0.9271, ROC AUC: 0.9886, KL Divergence: 0.0028\n",
      "2025-06-06 11:43:01,551 - __main__ - INFO - \t Fidelity - MAE: 0.1143, MSE: 0.0237, RMSE: 0.1539, R^2: 0.8671\n",
      "[I 2025-06-06 11:43:01,687] Trial 4 finished with value: -0.1118440454550344 and parameters: {'depth': 3, 'n_estimators': 3771, 'learning_rate_weights': 0.0004993895014755662, 'learning_rate_index': 0.03345674213696821, 'learning_rate_values': 0.00521502669741166, 'learning_rate_leaf': 0.015728674194978587, 'cosine_decay_steps': 1.0, 'dropout': 0.6711205128207366, 'selected_variables': 0.5978999788110851, 'data_subset_fraction': 0.9296868115208051}. Best is trial 2 with value: -0.10465762300389359.\n",
      "2025-06-06 11:43:01,698 - __main__ - INFO - Best hyperparameters for fold 2: {'depth': 3, 'n_estimators': 1169, 'learning_rate_weights': 0.0006624310605949989, 'learning_rate_index': 0.016124278458562614, 'learning_rate_values': 0.009860942908083906, 'learning_rate_leaf': 0.004678719265016202, 'cosine_decay_steps': 0.0, 'dropout': 0.5888819710447601, 'selected_variables': 0.19967378215835974, 'data_subset_fraction': 0.5628109945722505}\n",
      "2025-06-06 11:43:01,709 - __main__ - INFO - Best score: -0.1047\n",
      "2025-06-06 11:43:01,710 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:43:01,710 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-06 11:43:01,711 - __main__ - INFO - Retraining Model on Outer Fold 2\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:43:12,970 - __main__ - INFO - \t Inference Time: 1.22912 seconds\n",
      "2025-06-06 11:43:12,976 - __main__ - INFO - \t Balanced Accuracy: 0.5447, F1 Score: 0.5250, ROC AUC: 0.5456\n",
      "2025-06-06 11:43:12,982 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9200, F1 Score: 0.9435, ROC AUC: 0.9947, KL Divergence: 0.0120\n",
      "2025-06-06 11:43:12,984 - __main__ - INFO - \t Fidelity - MAE: 0.2526, MSE: 0.1079, RMSE: 0.3285, R^2: 0.7191\n",
      "2025-06-06 11:43:12,985 - __main__ - INFO - Outer Fold 3 - Train Indices: [ 0  2  3  4  5  6  7  9 10 11], Test Indices: [ 1  8 14 25 28 30 32 34 42 44]\n",
      "2025-06-06 11:43:12,986 - __main__ - INFO - -------------------- Outer Fold 3 --------------------\n",
      "2025-06-06 11:43:12,988 - __main__ - INFO - Starting hyperparameter optimization for outer fold 3...\n",
      "[I 2025-06-06 11:43:13,366] A new study created in RDB with name: 23381.3.grande(tabpfn).re\n",
      "2025-06-06 11:43:13,936 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:43:13,937 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:43:23,578 - __main__ - INFO - \t Balanced Accuracy: 0.5636, F1 Score: 0.5442, ROC AUC: 0.6103\n",
      "2025-06-06 11:43:23,583 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.8676, F1 Score: 0.8916, ROC AUC: 0.9878, KL Divergence: 0.0052\n",
      "2025-06-06 11:43:23,586 - __main__ - INFO - \t Fidelity - MAE: 0.1560, MSE: 0.0450, RMSE: 0.2122, R^2: 0.7651\n",
      "2025-06-06 11:43:24,105 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:43:24,106 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:43:35,065 - __main__ - INFO - \t Balanced Accuracy: 0.6232, F1 Score: 0.6184, ROC AUC: 0.6786\n",
      "2025-06-06 11:43:35,071 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9075, F1 Score: 0.9150, ROC AUC: 0.9696, KL Divergence: 0.0069\n",
      "2025-06-06 11:43:35,073 - __main__ - INFO - \t Fidelity - MAE: 0.1717, MSE: 0.0605, RMSE: 0.2460, R^2: 0.7548\n",
      "[I 2025-06-06 11:43:35,403] Trial 0 finished with value: -0.1638039928323875 and parameters: {'depth': 5, 'n_estimators': 2048, 'learning_rate_weights': 0.005, 'learning_rate_index': 0.01, 'learning_rate_values': 0.01, 'learning_rate_leaf': 0.01, 'cosine_decay_steps': 0.0, 'dropout': 0.0, 'selected_variables': 0.8, 'data_subset_fraction': 1.0}. Best is trial 0 with value: -0.1638039928323875.\n",
      "2025-06-06 11:43:35,821 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:43:35,822 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:43:50,891 - __main__ - INFO - \t Balanced Accuracy: 0.5815, F1 Score: 0.5674, ROC AUC: 0.6287\n",
      "2025-06-06 11:43:50,897 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9117, F1 Score: 0.9294, ROC AUC: 0.9854, KL Divergence: 0.0064\n",
      "2025-06-06 11:43:50,899 - __main__ - INFO - \t Fidelity - MAE: 0.1636, MSE: 0.0551, RMSE: 0.2347, R^2: 0.7127\n",
      "2025-06-06 11:43:51,233 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:43:51,234 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:44:09,398 - __main__ - INFO - \t Balanced Accuracy: 0.6275, F1 Score: 0.6227, ROC AUC: 0.6718\n",
      "2025-06-06 11:44:09,404 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9238, F1 Score: 0.9342, ROC AUC: 0.9733, KL Divergence: 0.0075\n",
      "2025-06-06 11:44:09,406 - __main__ - INFO - \t Fidelity - MAE: 0.1706, MSE: 0.0653, RMSE: 0.2556, R^2: 0.7354\n",
      "[I 2025-06-06 11:44:10,594] Trial 1 finished with value: -0.16710774152370977 and parameters: {'depth': 4, 'n_estimators': 3920, 'learning_rate_weights': 0.009454306819536176, 'learning_rate_index': 0.02385234757844707, 'learning_rate_values': 0.0022856175997064752, 'learning_rate_leaf': 0.002285325525633921, 'cosine_decay_steps': 0.1, 'dropout': 0.7274323891214958, 'selected_variables': 0.8324426408004217, 'data_subset_fraction': 0.29110519961044856}. Best is trial 0 with value: -0.1638039928323875.\n",
      "2025-06-06 11:44:10,821 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:44:10,822 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:44:24,434 - __main__ - INFO - \t Balanced Accuracy: 0.5739, F1 Score: 0.5559, ROC AUC: 0.6303\n",
      "2025-06-06 11:44:24,440 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.8811, F1 Score: 0.9061, ROC AUC: 0.9907, KL Divergence: 0.0042\n",
      "2025-06-06 11:44:24,442 - __main__ - INFO - \t Fidelity - MAE: 0.1420, MSE: 0.0366, RMSE: 0.1913, R^2: 0.8090\n",
      "2025-06-06 11:44:24,930 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:44:24,931 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:44:36,676 - __main__ - INFO - \t Balanced Accuracy: 0.6258, F1 Score: 0.6197, ROC AUC: 0.6669\n",
      "2025-06-06 11:44:36,681 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9049, F1 Score: 0.9200, ROC AUC: 0.9805, KL Divergence: 0.0067\n",
      "2025-06-06 11:44:36,684 - __main__ - INFO - \t Fidelity - MAE: 0.1670, MSE: 0.0588, RMSE: 0.2424, R^2: 0.7620\n",
      "[I 2025-06-06 11:44:37,456] Trial 2 finished with value: -0.15449392343785534 and parameters: {'depth': 3, 'n_estimators': 1169, 'learning_rate_weights': 0.0006624310605949989, 'learning_rate_index': 0.016124278458562614, 'learning_rate_values': 0.009860942908083906, 'learning_rate_leaf': 0.004678719265016202, 'cosine_decay_steps': 0.0, 'dropout': 0.5888819710447601, 'selected_variables': 0.19967378215835974, 'data_subset_fraction': 0.5628109945722505}. Best is trial 2 with value: -0.15449392343785534.\n",
      "2025-06-06 11:44:40,493 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:44:40,494 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:44:49,107 - __main__ - INFO - \t Balanced Accuracy: 0.5505, F1 Score: 0.5053, ROC AUC: 0.5666\n",
      "2025-06-06 11:44:49,113 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.6097, F1 Score: 0.6230, ROC AUC: 0.8105, KL Divergence: 0.0190\n",
      "2025-06-06 11:44:49,115 - __main__ - INFO - \t Fidelity - MAE: 0.3031, MSE: 0.1597, RMSE: 0.3996, R^2: 0.1672\n",
      "2025-06-06 11:44:49,134 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:44:49,135 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:44:57,696 - __main__ - INFO - \t Balanced Accuracy: 0.5507, F1 Score: 0.5328, ROC AUC: 0.6263\n",
      "2025-06-06 11:44:57,702 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.6618, F1 Score: 0.6711, ROC AUC: 0.7525, KL Divergence: 0.0238\n",
      "2025-06-06 11:44:57,704 - __main__ - INFO - \t Fidelity - MAE: 0.3514, MSE: 0.1989, RMSE: 0.4460, R^2: 0.1944\n",
      "[I 2025-06-06 11:45:01,764] Trial 3 finished with value: -0.32722049572714024 and parameters: {'depth': 5, 'n_estimators': 678, 'learning_rate_weights': 0.004362599362560562, 'learning_rate_index': 0.002468204407989012, 'learning_rate_values': 0.001411515549344833, 'learning_rate_leaf': 0.15255065745117383, 'cosine_decay_steps': 0.0, 'dropout': 0.330114370304701, 'selected_variables': 0.12203823484477883, 'data_subset_fraction': 0.5456592191001431}. Best is trial 2 with value: -0.15449392343785534.\n",
      "2025-06-06 11:45:02,580 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:45:02,581 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:45:13,930 - __main__ - INFO - \t Balanced Accuracy: 0.5534, F1 Score: 0.5325, ROC AUC: 0.6095\n",
      "2025-06-06 11:45:13,935 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.8541, F1 Score: 0.8772, ROC AUC: 0.9789, KL Divergence: 0.0059\n",
      "2025-06-06 11:45:13,937 - __main__ - INFO - \t Fidelity - MAE: 0.1659, MSE: 0.0507, RMSE: 0.2252, R^2: 0.7355\n",
      "2025-06-06 11:45:13,960 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:45:13,960 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:45:23,596 - __main__ - INFO - \t Balanced Accuracy: 0.5788, F1 Score: 0.5670, ROC AUC: 0.6287\n",
      "2025-06-06 11:45:23,602 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.7029, F1 Score: 0.7114, ROC AUC: 0.8026, KL Divergence: 0.0206\n",
      "2025-06-06 11:45:23,604 - __main__ - INFO - \t Fidelity - MAE: 0.3200, MSE: 0.1722, RMSE: 0.4150, R^2: 0.3023\n",
      "[I 2025-06-06 11:45:23,897] Trial 4 finished with value: -0.24294625129269687 and parameters: {'depth': 3, 'n_estimators': 3771, 'learning_rate_weights': 0.0004993895014755662, 'learning_rate_index': 0.03345674213696821, 'learning_rate_values': 0.00521502669741166, 'learning_rate_leaf': 0.015728674194978587, 'cosine_decay_steps': 1.0, 'dropout': 0.6711205128207366, 'selected_variables': 0.5978999788110851, 'data_subset_fraction': 0.9296868115208051}. Best is trial 2 with value: -0.15449392343785534.\n",
      "2025-06-06 11:45:23,921 - __main__ - INFO - Best hyperparameters for fold 3: {'depth': 3, 'n_estimators': 1169, 'learning_rate_weights': 0.0006624310605949989, 'learning_rate_index': 0.016124278458562614, 'learning_rate_values': 0.009860942908083906, 'learning_rate_leaf': 0.004678719265016202, 'cosine_decay_steps': 0.0, 'dropout': 0.5888819710447601, 'selected_variables': 0.19967378215835974, 'data_subset_fraction': 0.5628109945722505}\n",
      "2025-06-06 11:45:23,939 - __main__ - INFO - Best score: -0.1545\n",
      "2025-06-06 11:45:23,939 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:45:23,940 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-06 11:45:23,941 - __main__ - INFO - Retraining Model on Outer Fold 3\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:45:33,219 - __main__ - INFO - \t Inference Time: 1.07436 seconds\n",
      "2025-06-06 11:45:33,225 - __main__ - INFO - \t Balanced Accuracy: 0.5837, F1 Score: 0.5766, ROC AUC: 0.7172\n",
      "2025-06-06 11:45:33,230 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9483, F1 Score: 0.9624, ROC AUC: 0.9985, KL Divergence: 0.0029\n",
      "2025-06-06 11:45:33,232 - __main__ - INFO - \t Fidelity - MAE: 0.1192, MSE: 0.0247, RMSE: 0.1572, R^2: 0.8671\n",
      "2025-06-06 11:45:33,233 - __main__ - INFO - Outer Fold 4 - Train Indices: [ 0  1  2  3  4  6  8  9 10 11], Test Indices: [ 5  7 13 22 24 26 33 38 43 46]\n",
      "2025-06-06 11:45:33,234 - __main__ - INFO - -------------------- Outer Fold 4 --------------------\n",
      "2025-06-06 11:45:33,236 - __main__ - INFO - Starting hyperparameter optimization for outer fold 4...\n",
      "[I 2025-06-06 11:45:33,492] A new study created in RDB with name: 23381.4.grande(tabpfn).re\n",
      "2025-06-06 11:45:33,759 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:45:33,759 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:45:45,180 - __main__ - INFO - \t Balanced Accuracy: 0.6020, F1 Score: 0.5904, ROC AUC: 0.6631\n",
      "2025-06-06 11:45:45,186 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.8928, F1 Score: 0.9178, ROC AUC: 0.9937, KL Divergence: 0.0050\n",
      "2025-06-06 11:45:45,188 - __main__ - INFO - \t Fidelity - MAE: 0.1500, MSE: 0.0436, RMSE: 0.2088, R^2: 0.8226\n",
      "2025-06-06 11:45:45,209 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:45:45,210 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:45:56,419 - __main__ - INFO - \t Balanced Accuracy: 0.5702, F1 Score: 0.5590, ROC AUC: 0.6889\n",
      "2025-06-06 11:45:56,425 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9357, F1 Score: 0.9388, ROC AUC: 0.9895, KL Divergence: 0.0043\n",
      "2025-06-06 11:45:56,427 - __main__ - INFO - \t Fidelity - MAE: 0.1402, MSE: 0.0378, RMSE: 0.1945, R^2: 0.8209\n",
      "[I 2025-06-06 11:45:56,767] Trial 0 finished with value: -0.14511289858705628 and parameters: {'depth': 5, 'n_estimators': 2048, 'learning_rate_weights': 0.005, 'learning_rate_index': 0.01, 'learning_rate_values': 0.01, 'learning_rate_leaf': 0.01, 'cosine_decay_steps': 0.0, 'dropout': 0.0, 'selected_variables': 0.8, 'data_subset_fraction': 1.0}. Best is trial 0 with value: -0.14511289858705628.\n",
      "2025-06-06 11:45:56,894 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:45:56,895 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:46:12,121 - __main__ - INFO - \t Balanced Accuracy: 0.6010, F1 Score: 0.5929, ROC AUC: 0.6446\n",
      "2025-06-06 11:46:12,127 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9400, F1 Score: 0.9536, ROC AUC: 0.9887, KL Divergence: 0.0087\n",
      "2025-06-06 11:46:12,129 - __main__ - INFO - \t Fidelity - MAE: 0.1899, MSE: 0.0748, RMSE: 0.2735, R^2: 0.6957\n",
      "2025-06-06 11:46:12,141 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:46:12,142 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:46:31,520 - __main__ - INFO - \t Balanced Accuracy: 0.5523, F1 Score: 0.5366, ROC AUC: 0.6701\n",
      "2025-06-06 11:46:31,525 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9186, F1 Score: 0.9305, ROC AUC: 0.9907, KL Divergence: 0.0047\n",
      "2025-06-06 11:46:31,527 - __main__ - INFO - \t Fidelity - MAE: 0.1390, MSE: 0.0410, RMSE: 0.2026, R^2: 0.8057\n",
      "[I 2025-06-06 11:46:31,674] Trial 1 finished with value: -0.1644191896088416 and parameters: {'depth': 4, 'n_estimators': 3920, 'learning_rate_weights': 0.009454306819536176, 'learning_rate_index': 0.02385234757844707, 'learning_rate_values': 0.0022856175997064752, 'learning_rate_leaf': 0.002285325525633921, 'cosine_decay_steps': 0.1, 'dropout': 0.7274323891214958, 'selected_variables': 0.8324426408004217, 'data_subset_fraction': 0.29110519961044856}. Best is trial 0 with value: -0.14511289858705628.\n",
      "2025-06-06 11:46:31,806 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:46:31,806 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:46:44,020 - __main__ - INFO - \t Balanced Accuracy: 0.6010, F1 Score: 0.5929, ROC AUC: 0.6576\n",
      "2025-06-06 11:46:44,026 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9400, F1 Score: 0.9536, ROC AUC: 0.9967, KL Divergence: 0.0056\n",
      "2025-06-06 11:46:44,028 - __main__ - INFO - \t Fidelity - MAE: 0.1594, MSE: 0.0490, RMSE: 0.2214, R^2: 0.8006\n",
      "2025-06-06 11:46:44,040 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:46:44,040 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:46:58,357 - __main__ - INFO - \t Balanced Accuracy: 0.5556, F1 Score: 0.5438, ROC AUC: 0.6592\n",
      "2025-06-06 11:46:58,363 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9595, F1 Score: 0.9595, ROC AUC: 0.9907, KL Divergence: 0.0036\n",
      "2025-06-06 11:46:58,365 - __main__ - INFO - \t Fidelity - MAE: 0.1313, MSE: 0.0318, RMSE: 0.1783, R^2: 0.8494\n",
      "[I 2025-06-06 11:46:58,502] Trial 2 finished with value: -0.14536145804115036 and parameters: {'depth': 3, 'n_estimators': 1169, 'learning_rate_weights': 0.0006624310605949989, 'learning_rate_index': 0.016124278458562614, 'learning_rate_values': 0.009860942908083906, 'learning_rate_leaf': 0.004678719265016202, 'cosine_decay_steps': 0.0, 'dropout': 0.5888819710447601, 'selected_variables': 0.19967378215835974, 'data_subset_fraction': 0.5628109945722505}. Best is trial 0 with value: -0.14511289858705628.\n",
      "2025-06-06 11:46:58,634 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:46:58,635 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:47:07,230 - __main__ - INFO - \t Balanced Accuracy: 0.5478, F1 Score: 0.5075, ROC AUC: 0.5405\n",
      "2025-06-06 11:47:07,236 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.6520, F1 Score: 0.6719, ROC AUC: 0.8033, KL Divergence: 0.0215\n",
      "2025-06-06 11:47:07,238 - __main__ - INFO - \t Fidelity - MAE: 0.3134, MSE: 0.1811, RMSE: 0.4255, R^2: 0.2634\n",
      "2025-06-06 11:47:07,251 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:47:07,251 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:47:15,812 - __main__ - INFO - \t Balanced Accuracy: 0.5532, F1 Score: 0.5028, ROC AUC: 0.5737\n",
      "2025-06-06 11:47:15,818 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.6062, F1 Score: 0.6190, ROC AUC: 0.7560, KL Divergence: 0.0212\n",
      "2025-06-06 11:47:15,820 - __main__ - INFO - \t Fidelity - MAE: 0.3257, MSE: 0.1773, RMSE: 0.4211, R^2: 0.1605\n",
      "[I 2025-06-06 11:47:15,969] Trial 3 finished with value: -0.3195798732685109 and parameters: {'depth': 5, 'n_estimators': 678, 'learning_rate_weights': 0.004362599362560562, 'learning_rate_index': 0.002468204407989012, 'learning_rate_values': 0.001411515549344833, 'learning_rate_leaf': 0.15255065745117383, 'cosine_decay_steps': 0.0, 'dropout': 0.330114370304701, 'selected_variables': 0.12203823484477883, 'data_subset_fraction': 0.5456592191001431}. Best is trial 0 with value: -0.14511289858705628.\n",
      "2025-06-06 11:47:16,093 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:47:16,094 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:47:27,613 - __main__ - INFO - \t Balanced Accuracy: 0.5842, F1 Score: 0.5676, ROC AUC: 0.6615\n",
      "2025-06-06 11:47:27,618 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.8645, F1 Score: 0.8949, ROC AUC: 0.9868, KL Divergence: 0.0079\n",
      "2025-06-06 11:47:27,620 - __main__ - INFO - \t Fidelity - MAE: 0.1942, MSE: 0.0685, RMSE: 0.2617, R^2: 0.7215\n",
      "2025-06-06 11:47:27,633 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:47:27,634 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:47:42,253 - __main__ - INFO - \t Balanced Accuracy: 0.5540, F1 Score: 0.5403, ROC AUC: 0.6521\n",
      "2025-06-06 11:47:42,258 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9390, F1 Score: 0.9452, ROC AUC: 0.9915, KL Divergence: 0.0042\n",
      "2025-06-06 11:47:42,260 - __main__ - INFO - \t Fidelity - MAE: 0.1408, MSE: 0.0367, RMSE: 0.1916, R^2: 0.8261\n",
      "[I 2025-06-06 11:47:42,394] Trial 4 finished with value: -0.1675199684036332 and parameters: {'depth': 3, 'n_estimators': 3771, 'learning_rate_weights': 0.0004993895014755662, 'learning_rate_index': 0.03345674213696821, 'learning_rate_values': 0.00521502669741166, 'learning_rate_leaf': 0.015728674194978587, 'cosine_decay_steps': 1.0, 'dropout': 0.6711205128207366, 'selected_variables': 0.5978999788110851, 'data_subset_fraction': 0.9296868115208051}. Best is trial 0 with value: -0.14511289858705628.\n",
      "2025-06-06 11:47:42,405 - __main__ - INFO - Best hyperparameters for fold 4: {'depth': 5, 'n_estimators': 2048, 'learning_rate_weights': 0.005, 'learning_rate_index': 0.01, 'learning_rate_values': 0.01, 'learning_rate_leaf': 0.01, 'cosine_decay_steps': 0.0, 'dropout': 0.0, 'selected_variables': 0.8, 'data_subset_fraction': 1.0}\n",
      "2025-06-06 11:47:42,415 - __main__ - INFO - Best score: -0.1451\n",
      "2025-06-06 11:47:42,416 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:47:42,416 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-06 11:47:42,416 - __main__ - INFO - Retraining Model on Outer Fold 4\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:47:53,292 - __main__ - INFO - \t Inference Time: 1.30892 seconds\n",
      "2025-06-06 11:47:53,298 - __main__ - INFO - \t Balanced Accuracy: 0.6195, F1 Score: 0.6179, ROC AUC: 0.6872\n",
      "2025-06-06 11:47:53,304 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9029, F1 Score: 0.9029, ROC AUC: 0.9820, KL Divergence: 0.0029\n",
      "2025-06-06 11:47:53,306 - __main__ - INFO - \t Fidelity - MAE: 0.1309, MSE: 0.0249, RMSE: 0.1579, R^2: 0.8336\n",
      "2025-06-06 11:47:53,307 - __main__ - INFO - Outer Fold 5 - Train Indices: [ 0  1  2  4  5  6  7  8 10 11], Test Indices: [ 3  9 17 23 29 31 37 40 41 54]\n",
      "2025-06-06 11:47:53,307 - __main__ - INFO - -------------------- Outer Fold 5 --------------------\n",
      "2025-06-06 11:47:53,309 - __main__ - INFO - Starting hyperparameter optimization for outer fold 5...\n",
      "[I 2025-06-06 11:47:53,363] A new study created in RDB with name: 23381.5.grande(tabpfn).re\n",
      "2025-06-06 11:47:53,504 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:47:53,505 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:48:04,277 - __main__ - INFO - \t Balanced Accuracy: 0.5864, F1 Score: 0.5776, ROC AUC: 0.6637\n",
      "2025-06-06 11:48:04,282 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.8808, F1 Score: 0.8973, ROC AUC: 0.9880, KL Divergence: 0.0052\n",
      "2025-06-06 11:48:04,284 - __main__ - INFO - \t Fidelity - MAE: 0.1606, MSE: 0.0452, RMSE: 0.2125, R^2: 0.8101\n",
      "2025-06-06 11:48:04,297 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:48:04,298 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:48:16,359 - __main__ - INFO - \t Balanced Accuracy: 0.5967, F1 Score: 0.5889, ROC AUC: 0.6785\n",
      "2025-06-06 11:48:16,365 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9103, F1 Score: 0.9298, ROC AUC: 0.9835, KL Divergence: 0.0058\n",
      "2025-06-06 11:48:16,367 - __main__ - INFO - \t Fidelity - MAE: 0.1589, MSE: 0.0506, RMSE: 0.2249, R^2: 0.7939\n",
      "[I 2025-06-06 11:48:16,524] Trial 0 finished with value: -0.15977051136370307 and parameters: {'depth': 5, 'n_estimators': 2048, 'learning_rate_weights': 0.005, 'learning_rate_index': 0.01, 'learning_rate_values': 0.01, 'learning_rate_leaf': 0.01, 'cosine_decay_steps': 0.0, 'dropout': 0.0, 'selected_variables': 0.8, 'data_subset_fraction': 1.0}. Best is trial 0 with value: -0.15977051136370307.\n",
      "2025-06-06 11:48:16,644 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:48:16,645 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:48:31,612 - __main__ - INFO - \t Balanced Accuracy: 0.5610, F1 Score: 0.5443, ROC AUC: 0.6516\n",
      "2025-06-06 11:48:31,618 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.8404, F1 Score: 0.8667, ROC AUC: 0.9807, KL Divergence: 0.0093\n",
      "2025-06-06 11:48:31,620 - __main__ - INFO - \t Fidelity - MAE: 0.1965, MSE: 0.0790, RMSE: 0.2811, R^2: 0.6679\n",
      "2025-06-06 11:48:31,631 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:48:31,632 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:48:46,887 - __main__ - INFO - \t Balanced Accuracy: 0.5761, F1 Score: 0.5663, ROC AUC: 0.6325\n",
      "2025-06-06 11:48:46,893 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.8981, F1 Score: 0.9171, ROC AUC: 0.9517, KL Divergence: 0.0105\n",
      "2025-06-06 11:48:46,895 - __main__ - INFO - \t Fidelity - MAE: 0.2058, MSE: 0.0905, RMSE: 0.3008, R^2: 0.6315\n",
      "[I 2025-06-06 11:48:47,044] Trial 1 finished with value: -0.2011749873329319 and parameters: {'depth': 4, 'n_estimators': 3920, 'learning_rate_weights': 0.009454306819536176, 'learning_rate_index': 0.02385234757844707, 'learning_rate_values': 0.0022856175997064752, 'learning_rate_leaf': 0.002285325525633921, 'cosine_decay_steps': 0.1, 'dropout': 0.7274323891214958, 'selected_variables': 0.8324426408004217, 'data_subset_fraction': 0.29110519961044856}. Best is trial 0 with value: -0.15977051136370307.\n",
      "2025-06-06 11:48:47,165 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:48:47,166 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:49:00,254 - __main__ - INFO - \t Balanced Accuracy: 0.6135, F1 Score: 0.6112, ROC AUC: 0.6722\n",
      "2025-06-06 11:49:00,260 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9509, F1 Score: 0.9509, ROC AUC: 0.9869, KL Divergence: 0.0043\n",
      "2025-06-06 11:49:00,262 - __main__ - INFO - \t Fidelity - MAE: 0.1504, MSE: 0.0371, RMSE: 0.1926, R^2: 0.8441\n",
      "2025-06-06 11:49:00,273 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:49:00,274 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:49:16,239 - __main__ - INFO - \t Balanced Accuracy: 0.5768, F1 Score: 0.5714, ROC AUC: 0.6834\n",
      "2025-06-06 11:49:16,245 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9342, F1 Score: 0.9387, ROC AUC: 0.9951, KL Divergence: 0.0036\n",
      "2025-06-06 11:49:16,247 - __main__ - INFO - \t Fidelity - MAE: 0.1289, MSE: 0.0319, RMSE: 0.1785, R^2: 0.8702\n",
      "[I 2025-06-06 11:49:16,393] Trial 2 finished with value: -0.139638537591641 and parameters: {'depth': 3, 'n_estimators': 1169, 'learning_rate_weights': 0.0006624310605949989, 'learning_rate_index': 0.016124278458562614, 'learning_rate_values': 0.009860942908083906, 'learning_rate_leaf': 0.004678719265016202, 'cosine_decay_steps': 0.0, 'dropout': 0.5888819710447601, 'selected_variables': 0.19967378215835974, 'data_subset_fraction': 0.5628109945722505}. Best is trial 2 with value: -0.139638537591641.\n",
      "2025-06-06 11:49:16,520 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:49:16,520 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:49:26,383 - __main__ - INFO - \t Balanced Accuracy: 0.5815, F1 Score: 0.5674, ROC AUC: 0.6263\n",
      "2025-06-06 11:49:26,389 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.6686, F1 Score: 0.6800, ROC AUC: 0.7668, KL Divergence: 0.0243\n",
      "2025-06-06 11:49:26,391 - __main__ - INFO - \t Fidelity - MAE: 0.3404, MSE: 0.2032, RMSE: 0.4508, R^2: 0.1457\n",
      "2025-06-06 11:49:26,404 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:49:26,405 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:49:35,134 - __main__ - INFO - \t Balanced Accuracy: 0.5961, F1 Score: 0.5829, ROC AUC: 0.6478\n",
      "2025-06-06 11:49:35,140 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.6557, F1 Score: 0.6667, ROC AUC: 0.7530, KL Divergence: 0.0240\n",
      "2025-06-06 11:49:35,142 - __main__ - INFO - \t Fidelity - MAE: 0.3354, MSE: 0.2012, RMSE: 0.4485, R^2: 0.1804\n",
      "[I 2025-06-06 11:49:35,288] Trial 3 finished with value: -0.3379203529503298 and parameters: {'depth': 5, 'n_estimators': 678, 'learning_rate_weights': 0.004362599362560562, 'learning_rate_index': 0.002468204407989012, 'learning_rate_values': 0.001411515549344833, 'learning_rate_leaf': 0.15255065745117383, 'cosine_decay_steps': 0.0, 'dropout': 0.330114370304701, 'selected_variables': 0.12203823484477883, 'data_subset_fraction': 0.5456592191001431}. Best is trial 2 with value: -0.139638537591641.\n",
      "2025-06-06 11:49:35,421 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:49:35,422 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:49:51,933 - __main__ - INFO - \t Balanced Accuracy: 0.5897, F1 Score: 0.5837, ROC AUC: 0.6636\n",
      "2025-06-06 11:49:51,939 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9158, F1 Score: 0.9248, ROC AUC: 0.9872, KL Divergence: 0.0052\n",
      "2025-06-06 11:49:51,941 - __main__ - INFO - \t Fidelity - MAE: 0.1617, MSE: 0.0448, RMSE: 0.2116, R^2: 0.8118\n",
      "2025-06-06 11:49:51,953 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:49:51,953 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/category_encoders/ordinal.py:210: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[column].fillna(-1, inplace=True)\n",
      "2025-06-06 11:50:05,909 - __main__ - INFO - \t Balanced Accuracy: 0.5881, F1 Score: 0.5807, ROC AUC: 0.6683\n",
      "2025-06-06 11:50:05,914 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9154, F1 Score: 0.9306, ROC AUC: 0.9807, KL Divergence: 0.0062\n",
      "2025-06-06 11:50:05,916 - __main__ - INFO - \t Fidelity - MAE: 0.1646, MSE: 0.0537, RMSE: 0.2318, R^2: 0.7811\n",
      "[I 2025-06-06 11:50:06,075] Trial 4 finished with value: -0.1631416663728817 and parameters: {'depth': 3, 'n_estimators': 3771, 'learning_rate_weights': 0.0004993895014755662, 'learning_rate_index': 0.03345674213696821, 'learning_rate_values': 0.00521502669741166, 'learning_rate_leaf': 0.015728674194978587, 'cosine_decay_steps': 1.0, 'dropout': 0.6711205128207366, 'selected_variables': 0.5978999788110851, 'data_subset_fraction': 0.9296868115208051}. Best is trial 2 with value: -0.139638537591641.\n",
      "2025-06-06 11:50:06,087 - __main__ - INFO - Best hyperparameters for fold 5: {'depth': 3, 'n_estimators': 1169, 'learning_rate_weights': 0.0006624310605949989, 'learning_rate_index': 0.016124278458562614, 'learning_rate_values': 0.009860942908083906, 'learning_rate_leaf': 0.004678719265016202, 'cosine_decay_steps': 0.0, 'dropout': 0.5888819710447601, 'selected_variables': 0.19967378215835974, 'data_subset_fraction': 0.5628109945722505}\n",
      "2025-06-06 11:50:06,098 - __main__ - INFO - Best score: -0.1396\n",
      "2025-06-06 11:50:06,099 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:50:06,100 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-06 11:50:06,100 - __main__ - INFO - Retraining Model on Outer Fold 5\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.cat_columns] = X[self.cat_columns].fillna(self.mode_train_cat)\n",
      "2025-06-06 11:50:19,004 - __main__ - INFO - \t Inference Time: 1.07707 seconds\n",
      "2025-06-06 11:50:19,010 - __main__ - INFO - \t Balanced Accuracy: 0.6593, F1 Score: 0.6553, ROC AUC: 0.7685\n",
      "2025-06-06 11:50:19,016 - __main__ - INFO - \t Fidelity - Balanced Accuracy: 0.9643, F1 Score: 0.9025, ROC AUC: 0.9814, KL Divergence: 0.0036\n",
      "2025-06-06 11:50:19,018 - __main__ - INFO - \t Fidelity - MAE: 0.1408, MSE: 0.0306, RMSE: 0.1750, R^2: 0.6804\n",
      "2025-06-06 11:50:19,021 - __main__ - INFO - Outer fold metrics saved to: data/outer_fold/student/hpo/23381_grande(tabpfn).re.csv\n",
      "2025-06-06 11:50:19,024 - __main__ - INFO - === FINAL RESULTS FOR DATASET 23381 ===\n",
      "2025-06-06 11:50:19,024 - __main__ - INFO - Balanced Accuracy: 0.5995 ± 0.0427\n",
      "2025-06-06 11:50:19,025 - __main__ - INFO - F1 Score: 0.5925 ± 0.0485\n",
      "2025-06-06 11:50:19,025 - __main__ - INFO - ROC AUC: 0.6818 ± 0.0828\n",
      "2025-06-06 11:50:19,026 - __main__ - INFO - Fidelity Accuracy: 0.9408 ± 0.0285\n",
      "2025-06-06 11:50:19,026 - __main__ - INFO - Fidelity F1 Score: 0.9376 ± 0.0339\n",
      "2025-06-06 11:50:19,027 - __main__ - INFO - Fidelity ROC AUC: 0.9912 ± 0.0089\n",
      "2025-06-06 11:50:19,027 - __main__ - INFO - Fidelity KL Divergence: 0.0048 ± 0.0041\n",
      "2025-06-06 11:50:19,029 - __main__ - INFO - Fidelity MAE: 0.1509 ± 0.0580\n",
      "2025-06-06 11:50:19,029 - __main__ - INFO - Fidelity MSE: 0.0418 ± 0.0371\n",
      "2025-06-06 11:50:19,030 - __main__ - INFO - Fidelity RMSE: 0.1926 ± 0.0767\n",
      "2025-06-06 11:50:19,031 - __main__ - INFO - Fidelity R2: 0.8018 ± 0.0980\n",
      "2025-06-06 11:50:19,031 - __main__ - INFO - Mean Inference Time: 1.1885 ± 0.1069\n",
      "2025-06-06 11:50:19,032 - __main__ - INFO - Mean Parameters: 539781.6000 ± 728770.8736\n",
      "2025-06-06 11:50:19,034 - __main__ - INFO - Summary statistics saved to: results/student/23381_results.json\n",
      "2025-06-06 11:50:19,038 - __main__ - INFO - grande outputs saved to: data/output/hpo/student/23381_grande(tabpfn).csv\n",
      "2025-06-06 11:50:19,038 - __main__ - INFO - Loading configuration...\n",
      "2025-06-06 11:50:19,039 - __main__ - INFO - Loading dataset 197 from cache at 'data/cache/openml_dataset_197.pkl'...\n",
      "2025-06-06 11:50:19,043 - __main__ - INFO - Dataset 197 loaded successfully with task type: regression\n",
      "2025-06-06 11:50:19,045 - __main__ - INFO - Using GPU: NVIDIA GeForce RTX 2080 Ti\n",
      "2025-06-06 11:50:19,050 - __main__ - INFO - Random seed set to 42\n",
      "2025-06-06 11:50:19,070 - __main__ - INFO - Loaded fold indices from: data/fold_indices/dataset_197.json\n",
      "2025-06-06 11:50:19,075 - __main__ - INFO - Loaded tabpfn outputs from: data/output/hpo/teacher/197_tabpfn.csv\n",
      "2025-06-06 11:50:19,078 - __main__ - INFO - Outer Fold 1 - Train Indices: [0 1 2 3 4 5 6 7 8 9], Test Indices: [17 19 23 26 31 33 37 41 48 50]\n",
      "2025-06-06 11:50:19,079 - __main__ - INFO - -------------------- Outer Fold 1 --------------------\n",
      "2025-06-06 11:50:19,081 - __main__ - INFO - Starting hyperparameter optimization for outer fold 1...\n",
      "[I 2025-06-06 11:50:19,167] A new study created in RDB with name: 197.1.grande(tabpfn).re\n",
      "2025-06-06 11:50:20,424 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:50:20,425 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/GRANDE/GRANDE.py:128: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.num_columns] = X[self.num_columns].fillna(self.mean_train_num)\n",
      "2025-06-06 11:50:46,054 - __main__ - INFO - \t MAE: 1.9197, MSE: 8.5295, RMSE: 2.9205, R^2: 0.9759\n",
      "2025-06-06 11:50:46,056 - __main__ - INFO - \t Fidelity - MAE: 0.6945, MSE: 0.9898, RMSE: 0.9949, R^2: 0.9966\n",
      "2025-06-06 11:50:46,079 - __main__ - INFO - No preprocessing required for Grande model.\n",
      "2025-06-06 11:50:46,080 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n"
     ]
    }
   ],
   "source": [
    "# Get list of datasets to process from configuration\n",
    "datasets = config[\"data\"][\"datasets\"]\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINING LOOP - Process each dataset independently\n",
    "# =============================================================================\n",
    "for dataset_id in datasets:\n",
    "    # -------------------------------------------------------------------------\n",
    "    # SETUP AND INITIALIZATION\n",
    "    # -------------------------------------------------------------------------\n",
    "    logger = setup_logging()\n",
    "    logger.info(\"Loading configuration...\")\n",
    "\n",
    "    # Extract model configuration for this run\n",
    "    student_model_type = config[\"model\"][\"student_model\"]\n",
    "    teacher_model_type = config[\"model\"][\"teacher_model\"]\n",
    "    preprocessing_type = config[\"student_models\"][student_model_type][\"preprocessing\"]\n",
    "    use_hpo = config[\"training\"][\"use_hpo\"]\n",
    "    train_on_logits = config[\"training\"][\"train_on_logits\"]\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 0: DATA LOADING\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Load dataset from OpenML with caching for efficiency\n",
    "    X, y, cat_cols, _, original_task_type = load_dataset(\n",
    "        dataset_id=dataset_id,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 1: INFRASTRUCTURE SETUP\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Note: Checking for existing results is placeholder for future implementation\n",
    "    model_task_type = \"binary\" if (original_task_type == \"binary\" and not train_on_logits) else \"regression\"\n",
    "    summary_file = os.path.join(config[\"data\"][\"results_dir_path\"], \"student\", f\"{dataset_id}_results.json\")\n",
    "    if os.path.exists(summary_file):\n",
    "        with open(summary_file, 'r') as f:\n",
    "            existing_results = json.load(f)\n",
    "        \n",
    "        # Check if we already have results with the same configuration\n",
    "        config_exists = False\n",
    "        for key, result in existing_results.items():\n",
    "            if (result.get('student_model_type') == student_model_type and \n",
    "                result.get('teacher_model_type') == teacher_model_type and\n",
    "                result.get('student_task_type') == model_task_type and\n",
    "                result.get('use_hpo') == use_hpo and \n",
    "                result.get('seed') == config[\"training\"][\"random_state\"]):\n",
    "                config_exists = True\n",
    "                logger.info(f\"Results already exist for dataset {dataset_id} with model {student_model_type}({teacher_model_type}), HPO: {use_hpo}, seed: {config['training']['random_state']}\")\n",
    "                break\n",
    "        \n",
    "        if config_exists:\n",
    "            logger.info(f\"Skipping dataset {dataset_id} - results already computed\")\n",
    "            continue\n",
    "\n",
    "    # Configure GPU/CPU usage for training\n",
    "    device = check_GPU_availability()\n",
    "\n",
    "    # Set random seed for reproducibility across all libraries\n",
    "    set_seed(config[\"training\"][\"random_state\"])\n",
    "    logger.info(f\"Random seed set to {config['training']['random_state']}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 2: LOAD PREVIOUSLY SAVED FOLD INDICES AND TEACHER PREDICTIONS\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Load the saved fold indices \n",
    "    fold_indices_file = os.path.join(config[\"data\"][\"fold_indices_path\"], f\"dataset_{dataset_id}.json\")\n",
    "    with open(fold_indices_file, 'r') as f:\n",
    "        fold_indices = json.load(f)\n",
    "    logger.info(f\"Loaded fold indices from: {fold_indices_file}\") \n",
    "\n",
    "    # Load the TabPFN outputs (teacher predictions)\n",
    "    subfolder = \"hpo\" if config[\"training\"][\"use_hpo\"] else \"default\"\n",
    "    directory = os.path.join(config[\"data\"][\"output_dir_path\"], subfolder, \"teacher\")\n",
    "    teacher_outputs_file = os.path.join(directory, f\"{dataset_id}_{teacher_model_type}.csv\")\n",
    "    teacher_outputs_df = pd.read_csv(teacher_outputs_file)\n",
    "    logger.info(f\"Loaded {teacher_model_type} outputs from: {teacher_outputs_file}\")\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # STEP 3: PREPROCESS TEACHER OUTPUTS\n",
    "    # --------------------------------------------------------------------------\n",
    "    if original_task_type == \"regression\":\n",
    "        model_task_type = \"regression\"\n",
    "        # For regression, we can use the outputs directly as they are already numeric\n",
    "        teacher_preds = teacher_outputs_df['output'].astype(float)\n",
    "        # Create a mapping from index to outputs for easy lookup\n",
    "        index_to_outputs = dict(zip(teacher_outputs_df['index'].values, teacher_preds))\n",
    "\n",
    "    else:\n",
    "        if train_on_logits:\n",
    "            model_task_type = \"regression\"\n",
    "            # Convert probabilities to logits\n",
    "            # Clip probabilities to avoid log(0) or log(1)\n",
    "            eps = 1e-7\n",
    "            teacher_probs = np.clip(teacher_outputs_df['output'].values, eps, 1 - eps)\n",
    "            teacher_logits = np.log(teacher_probs / (1 - teacher_probs))\n",
    "            # Create a mapping from index to logits for easy lookup\n",
    "            index_to_outputs = dict(zip(teacher_outputs_df['index'].values, teacher_logits))\n",
    "        else:\n",
    "            model_task_type = \"binary\"\n",
    "            # Convert probabilities to binary predictions\n",
    "            teacher_preds = (teacher_outputs_df['output'].values > 0.5).astype(int)\n",
    "            # Create a mapping from index to predictions for easy lookup\n",
    "            index_to_outputs = dict(zip(teacher_outputs_df['index'].values, teacher_preds))\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 4: INITIALIZE DATA STRUCTURES\n",
    "    # -------------------------------------------------------------------------\n",
    "    output_dfs = []\n",
    "\n",
    "    outer_fold_scores = []\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 5: OUTER CROSS-VALIDATION LOOP\n",
    "    # =========================================================================\n",
    "    # Each iteration provides one unbiased performance estimate\n",
    "    for fold_key, fold_data in fold_indices[\"outer_folds\"].items():\n",
    "\n",
    "        # Extract outer fold number and indices from the fold key\n",
    "        outer_fold = int(fold_key.split('_')[1])\n",
    "        train_idx = np.array(fold_data[\"train_idx\"])\n",
    "        test_idx = np.array(fold_data[\"test_idx\"])\n",
    "\n",
    "        # Log the first 10 indices of the outer fold for debugging\n",
    "        logger.info(f\"Outer Fold {outer_fold} - Train Indices: {train_idx[:10]}, Test Indices: {test_idx[:10]}\")\n",
    "\n",
    "        logger.info(f\"-------------------- Outer Fold {outer_fold} --------------------\")\n",
    "\n",
    "        # Split data according to current outer fold\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # INNER CROSS-VALIDATION SETUP (for model validation)\n",
    "        # ---------------------------------------------------------------------\n",
    "        inner_folds_data = fold_indices[\"inner_folds\"][f\"outer_fold_{outer_fold}\"]\n",
    "\n",
    "        # =========================================================================\n",
    "        # STEP 6: INNER CROSS-VALIDATION LOOP (hyperparameter validation)\n",
    "        # =========================================================================\n",
    "        def objective(trial):\n",
    "            \n",
    "            hyperparams = suggest_hyperparameters(\n",
    "                trial=trial,\n",
    "                model_type=student_model_type,\n",
    "                task_type=model_task_type,\n",
    "                use_hpo=use_hpo,\n",
    "            )\n",
    "\n",
    "            inner_fold_scores = []\n",
    "            val_metrics_list = []\n",
    "\n",
    "            # This loop would typically be used for hyperparameter optimization\n",
    "            for inner_fold_key, inner_fold_data in inner_folds_data.items():\n",
    "                inner_fold = int(inner_fold_key.split('_')[2])\n",
    "                \n",
    "                # -----------------------------------------------------------------\n",
    "                # INDEX MANAGEMENT (Critical for avoiding data leakage)\n",
    "                # -----------------------------------------------------------------\n",
    "                # Get absolute indices from saved data\n",
    "                absolute_inner_train_idx = np.array(inner_fold_data[\"train_idx\"])\n",
    "                absolute_inner_val_idx = np.array(inner_fold_data[\"val_idx\"])\n",
    "                \n",
    "                # Convert absolute indices to relative indices for the current outer training set\n",
    "                inner_train_relative = np.where(np.isin(train_idx, absolute_inner_train_idx))[0]\n",
    "                inner_val_relative = np.where(np.isin(train_idx, absolute_inner_val_idx))[0] \n",
    "\n",
    "                # Split inner training data using relative indices\n",
    "                X_inner_train, X_inner_val = X_train.iloc[inner_train_relative], X_train.iloc[inner_val_relative]\n",
    "                y_inner_train, y_inner_val = y_train[inner_train_relative], y_train[inner_val_relative] # Hard labels or Regression targets\n",
    "\n",
    "                # Get teacher outputs for inner training and validation sets\n",
    "                teacher_outputs_inner_train = np.array([index_to_outputs[idx] for idx in absolute_inner_train_idx])\n",
    "                teacher_outputs_inner_val = np.array([index_to_outputs[idx] for idx in absolute_inner_val_idx]) # Probs or Regression logits\n",
    "\n",
    "                # ---------------------------------------------------------------------\n",
    "                # PREPROCESSING: Apply model-specific data transformations\n",
    "                # ---------------------------------------------------------------------\n",
    "                X_inner_train, X_inner_val = preprocess(\n",
    "                    X_inner_train,\n",
    "                    y_inner_train,\n",
    "                    X_inner_val, \n",
    "                    cat_cols,\n",
    "                    config,\n",
    "                    preprocessing_type=preprocessing_type,\n",
    "                )\n",
    "                # ----------------------------------------------------------------------\n",
    "                # MODEL TRAINING: Train student model on inner training data\n",
    "                # ----------------------------------------------------------------------\n",
    "                model = get_student_model(config=config, task_type=model_task_type, device=device, hyperparams=hyperparams, cat_cols=cat_cols)\n",
    "                logger.info(f\"Training Model on Outer Fold {outer_fold}, Inner Fold {inner_fold}...\")\n",
    "                model.train(X_inner_train, teacher_outputs_inner_train)\n",
    "\n",
    "                # ----------------------------------------------------------------------\n",
    "                # VALIDATION: Evaluate model performance on inner validation set\n",
    "                # ----------------------------------------------------------------------\n",
    "                val_preds = model.predict(X_inner_val) # Probs or Regression logits\n",
    "                val_metrics = model.evaluate(y_pred=val_preds, y_true=y_inner_val, y_teacher_true=teacher_outputs_inner_val, original_task_type=original_task_type)\n",
    "\n",
    "                # Store metrics from this fold for later mean calculation\n",
    "                val_metrics_list.append(val_metrics)\n",
    "\n",
    "                if model_task_type == \"binary\":\n",
    "                    inner_fold_scores.append(val_metrics[\"fidelity_f1\"])\n",
    "                    trial.report(np.mean(inner_fold_scores), step=inner_fold)\n",
    "                else:\n",
    "                    inner_fold_scores.append(-val_metrics[\"fidelity_mae\"])\n",
    "                    trial.report(np.mean(inner_fold_scores), step=inner_fold)\n",
    "\n",
    "            # Calculate mean metrics as user attributes after all inner folds\n",
    "            if val_metrics_list:\n",
    "                # Get all metric keys from the first fold\n",
    "                metric_keys = val_metrics_list[0].keys()\n",
    "                \n",
    "                for metric_key in metric_keys:\n",
    "                    # Calculate mean across all inner folds for this metric\n",
    "                    metric_values = [fold_metrics[metric_key] for fold_metrics in val_metrics_list]\n",
    "                    mean_metric = np.mean(metric_values)\n",
    "                    trial.set_user_attr(f\"mean_{metric_key}\", mean_metric)\n",
    "\n",
    "            return np.mean(inner_fold_scores)\n",
    "        \n",
    "        logger.info(f\"Starting hyperparameter optimization for outer fold {outer_fold}...\")\n",
    "\n",
    "        study_kwargs = dict(\n",
    "            direction=\"maximize\",\n",
    "            study_name=f\"{dataset_id}.{outer_fold}.{student_model_type}({teacher_model_type}).{model_task_type[:2]}.{original_task_type[:2]}\",\n",
    "            load_if_exists=True,\n",
    "            sampler=optuna.samplers.TPESampler(seed=config[\"training\"][\"random_state\"]),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps=1),\n",
    "        )\n",
    "\n",
    "        if use_hpo:\n",
    "            os.makedirs(config[\"data\"][\"optuna_db_path\"], exist_ok=True)\n",
    "            study_kwargs[\"storage\"] = f\"sqlite:///{config['data']['optuna_db_path']}/optuna.db\"\n",
    "\n",
    "        study = optuna.create_study(\n",
    "            **study_kwargs,\n",
    "        )\n",
    "\n",
    "        # Optimize\n",
    "        completed_trials = len(study.trials)\n",
    "        remaining_trials = config[\"training\"][\"trials\"] - completed_trials\n",
    "\n",
    "\n",
    "        if remaining_trials > 0:\n",
    "            default_hyperparams = suggest_hyperparameters(None, student_model_type, model_task_type, False)\n",
    "            study.enqueue_trial(default_hyperparams)\n",
    "            study.optimize(\n",
    "                objective, \n",
    "                n_trials=remaining_trials if use_hpo else 1,\n",
    "                show_progress_bar=False\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\"The study has already reached the maximum number of trials.\")\n",
    "\n",
    "        if use_hpo:\n",
    "            # When HPO is used, trial.suggest_ methods are called, populating trial.params\n",
    "            best_hyperparams = study.best_trial.params\n",
    "        else:\n",
    "            # When not using HPO, a single trial runs with default parameters.\n",
    "            best_hyperparams = suggest_hyperparameters(trial=None, model_type=student_model_type, task_type=model_task_type, use_hpo=False)\n",
    "\n",
    "        logger.info(f\"Best hyperparameters for fold {outer_fold}: {best_hyperparams}\")\n",
    "        logger.info(f\"Best score: {study.best_value:.4f}\")\n",
    "\n",
    "        # =========================================================================\n",
    "        # STEP 7: FINAL MODEL TRAINING (on complete outer training set)\n",
    "        # =========================================================================\n",
    "        # Get teacher logits for outer training set\n",
    "        teacher_outputs_train = np.array([index_to_outputs[idx] for idx in train_idx])\n",
    "        # Preprocess the outer training and test sets\n",
    "        X_train, X_test = preprocess(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            cat_cols,\n",
    "            config,\n",
    "            preprocessing_type=preprocessing_type,  \n",
    "        )\n",
    "        \n",
    "        # Train final model on complete outer training set\n",
    "        logger.info(\"------------------------------------------------------\")\n",
    "        logger.info(f\"Retraining Model on Outer Fold {outer_fold}\")\n",
    "        model = get_student_model(\n",
    "            config=config,\n",
    "            task_type=model_task_type,\n",
    "            device=device,\n",
    "            hyperparams=best_hyperparams,  # Use best hyperparameters from HPO\n",
    "            cat_cols=cat_cols,  \n",
    "        )\n",
    "        model.train(X_train, teacher_outputs_train)\n",
    "\n",
    "        # =========================================================================\n",
    "        # STEP 8: FINAL EVALUATION (unbiased performance on outer test set)\n",
    "        # =========================================================================\n",
    "        # This provides the unbiased performance estimate for this fold\n",
    "        start_time = time.time()\n",
    "        test_preds = model.predict(X_test)  # Predicted logits or probabilities\n",
    "        end_time = time.time() - start_time\n",
    "        logger.info(f\"\\t Inference Time: {end_time:.5f} seconds\")\n",
    "        teacher_outputs_test = np.array([index_to_outputs[idx] for idx in test_idx])\n",
    "        test_metrics = model.evaluate(test_preds, y_test, teacher_outputs_test, original_task_type)\n",
    "\n",
    "        # Store outer fold score for later analysis\n",
    "        outer_fold_results = {\n",
    "            \"fold\": outer_fold,\n",
    "            \"seed\": config[\"training\"][\"random_state\"],\n",
    "            \"inference_time\": end_time,\n",
    "            **test_metrics,\n",
    "        }\n",
    "        outer_fold_scores.append(outer_fold_results)\n",
    "\n",
    "        # =========================================================================\n",
    "        # STEP 9: STORE PREDICTIONS FOR STUDENT TRAINING\n",
    "        # =========================================================================\n",
    "        # Save predictions with their corresponding dataset indices\n",
    "        output_dfs.append(pd.DataFrame({\n",
    "            \"index\": test_idx,\n",
    "            \"output\": test_preds[:, 1] if model_task_type == \"binary\" else test_preds  \n",
    "        }))\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 10: SAVE RESULTS AND METADATA\n",
    "    # =========================================================================\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # SAVE OUTER FOLD METRICS\n",
    "    # -------------------------------------------------------------------------\n",
    "    outer_fold_df = pd.DataFrame(outer_fold_scores)\n",
    "    sub_folder = \"hpo\" if use_hpo else \"default\"\n",
    "    output_dir = os.path.join(config[\"data\"][\"outer_folds_path\"], \"student\", sub_folder)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    metrics_file = os.path.join(output_dir, f\"{dataset_id}_{student_model_type}({teacher_model_type}).{model_task_type[:2]}.csv\")\n",
    "    outer_fold_df.to_csv(metrics_file, index=False)\n",
    "    logger.info(f\"Outer fold metrics saved to: {metrics_file}\")\n",
    "\n",
    "    # Calculate and log overall performance across all folds\n",
    "    mean_inference_time = outer_fold_df['inference_time'].mean()\n",
    "    std_inference_time = outer_fold_df['inference_time'].std()\n",
    "    mean_parameters = outer_fold_df['parameters'].mean()\n",
    "    std_parameters = outer_fold_df['parameters'].std()\n",
    "\n",
    "    if original_task_type == \"binary\":\n",
    "        mean_acc = outer_fold_df['acc'].mean()\n",
    "        std_acc = outer_fold_df['acc'].std()\n",
    "        mean_f1 = outer_fold_df['f1'].mean()\n",
    "        std_f1 = outer_fold_df['f1'].std()\n",
    "        mean_roc = outer_fold_df['roc'].mean()\n",
    "        std_roc = outer_fold_df['roc'].std()\n",
    "        \n",
    "        mean_fidelity_acc = outer_fold_df['fidelity_acc'].mean()\n",
    "        std_fidelity_acc = outer_fold_df['fidelity_acc'].std()\n",
    "        mean_fidelity_f1 = outer_fold_df['fidelity_f1'].mean()\n",
    "        std_fidelity_f1 = outer_fold_df['fidelity_f1'].std()\n",
    "        mean_fidelity_roc = outer_fold_df['fidelity_roc'].mean()\n",
    "        std_fidelity_roc = outer_fold_df['fidelity_roc'].std()\n",
    "        mean_fidelity_kl_div = outer_fold_df['fidelity_kl_div'].mean()\n",
    "        std_fidelity_kl_div = outer_fold_df['fidelity_kl_div'].std()\n",
    "        \n",
    "        logger.info(f\"=== FINAL RESULTS FOR DATASET {dataset_id} ===\")\n",
    "        logger.info(f\"Balanced Accuracy: {mean_acc:.4f} ± {std_acc:.4f}\")\n",
    "        logger.info(f\"F1 Score: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "        logger.info(f\"ROC AUC: {mean_roc:.4f} ± {std_roc:.4f}\")\n",
    "        logger.info(f\"Fidelity Accuracy: {mean_fidelity_acc:.4f} ± {std_fidelity_acc:.4f}\")\n",
    "        logger.info(f\"Fidelity F1 Score: {mean_fidelity_f1:.4f} ± {std_fidelity_f1:.4f}\")\n",
    "        logger.info(f\"Fidelity ROC AUC: {mean_fidelity_roc:.4f} ± {std_fidelity_roc:.4f}\")\n",
    "        logger.info(f\"Fidelity KL Divergence: {mean_fidelity_kl_div:.4f} ± {std_fidelity_kl_div:.4f}\")\n",
    "\n",
    "        if model_task_type == \"regression\":\n",
    "            mean_fidelity_mae = outer_fold_df['fidelity_mae'].mean()\n",
    "            std_fidelity_mae = outer_fold_df['fidelity_mae'].std()\n",
    "            mean_fidelity_mse = outer_fold_df['fidelity_mse'].mean()\n",
    "            std_fidelity_mse = outer_fold_df['fidelity_mse'].std()\n",
    "            mean_fidelity_rmse = outer_fold_df['fidelity_rmse'].mean()\n",
    "            std_fidelity_rmse = outer_fold_df['fidelity_rmse'].std()\n",
    "            mean_fidelity_r2 = outer_fold_df['fidelity_r2'].mean()\n",
    "            std_fidelity_r2 = outer_fold_df['fidelity_r2'].std()\n",
    "\n",
    "            logger.info(f\"Fidelity MAE: {mean_fidelity_mae:.4f} ± {std_fidelity_mae:.4f}\")\n",
    "            logger.info(f\"Fidelity MSE: {mean_fidelity_mse:.4f} ± {std_fidelity_mse:.4f}\")\n",
    "            logger.info(f\"Fidelity RMSE: {mean_fidelity_rmse:.4f} ± {std_fidelity_rmse:.4f}\")\n",
    "            logger.info(f\"Fidelity R2: {mean_fidelity_r2:.4f} ± {std_fidelity_r2:.4f}\")\n",
    "\n",
    "    else:\n",
    "        mean_mae = outer_fold_df['mae'].mean()\n",
    "        std_mae = outer_fold_df['mae'].std()\n",
    "        mean_mse = outer_fold_df['mse'].mean()\n",
    "        std_mse = outer_fold_df['mse'].std()\n",
    "        mean_rmse = outer_fold_df['rmse'].mean()\n",
    "        std_rmse = outer_fold_df['rmse'].std()\n",
    "        mean_r2 = outer_fold_df['r2'].mean()\n",
    "        std_r2 = outer_fold_df['r2'].std()\n",
    "\n",
    "        logger.info(f\"=== FINAL RESULTS FOR DATASET {dataset_id} ===\")\n",
    "        logger.info(f\"MAE: {mean_mae:.4f} ± {std_mae:.4f}\")\n",
    "        logger.info(f\"MSE: {mean_mse:.4f} ± {std_mse:.4f}\")\n",
    "        logger.info(f\"RMSE: {mean_rmse:.4f} ± {std_rmse:.4f}\")\n",
    "        logger.info(f\"R2: {mean_r2:.4f} ± {std_r2:.4f}\")\n",
    "\n",
    "        mean_fidelity_mae = outer_fold_df['fidelity_mae'].mean()\n",
    "        std_fidelity_mae = outer_fold_df['fidelity_mae'].std()\n",
    "        mean_fidelity_mse = outer_fold_df['fidelity_mse'].mean()\n",
    "        std_fidelity_mse = outer_fold_df['fidelity_mse'].std()\n",
    "        mean_fidelity_rmse = outer_fold_df['fidelity_rmse'].mean()\n",
    "        std_fidelity_rmse = outer_fold_df['fidelity_rmse'].std()\n",
    "        mean_fidelity_r2 = outer_fold_df['fidelity_r2'].mean()\n",
    "        std_fidelity_r2 = outer_fold_df['fidelity_r2'].std()\n",
    "\n",
    "        logger.info(f\"Fidelity MAE: {mean_fidelity_mae:.4f} ± {std_fidelity_mae:.4f}\")\n",
    "        logger.info(f\"Fidelity MSE: {mean_fidelity_mse:.4f} ± {std_fidelity_mse:.4f}\")\n",
    "        logger.info(f\"Fidelity RMSE: {mean_fidelity_rmse:.4f} ± {std_fidelity_rmse:.4f}\")\n",
    "        logger.info(f\"Fidelity R2: {mean_fidelity_r2:.4f} ± {std_fidelity_r2:.4f}\")\n",
    "\n",
    "    logger.info(f\"Mean Inference Time: {mean_inference_time:.4f} ± {std_inference_time:.4f}\")\n",
    "    logger.info(f\"Mean Parameters: {mean_parameters:.4f} ± {std_parameters:.4f}\")\n",
    "\n",
    "    # Save summary statistics as well\n",
    "    summary_stats = {\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"student_model_type\": student_model_type,\n",
    "        \"teacher_model_type\": teacher_model_type,\n",
    "        \"original_task_type\": original_task_type,\n",
    "        \"student_task_type\": model_task_type,\n",
    "        \"seed\": config[\"training\"][\"random_state\"],\n",
    "        \"use_hpo\": config[\"training\"][\"use_hpo\"],\n",
    "        \"mean_inference_time\": mean_inference_time,\n",
    "        \"std_inference_time\": std_inference_time,\n",
    "        \"mean_parameters\": mean_parameters,\n",
    "        \"std_parameters\": std_parameters,\n",
    "    }\n",
    "\n",
    "    if original_task_type == \"binary\":\n",
    "        summary_stats.update({\n",
    "            \"mean_acc\": mean_acc,\n",
    "            \"std_acc\": std_acc,\n",
    "            \"mean_f1\": mean_f1,\n",
    "            \"std_f1\": std_f1,\n",
    "            \"mean_roc\": mean_roc,\n",
    "            \"std_roc\": std_roc,\n",
    "            \"mean_fidelity_acc\": mean_fidelity_acc,\n",
    "            \"std_fidelity_acc\": std_fidelity_acc,\n",
    "            \"mean_fidelity_f1\": mean_fidelity_f1,\n",
    "            \"std_fidelity_f1\": std_fidelity_f1,\n",
    "            \"mean_fidelity_roc\": mean_fidelity_roc,\n",
    "            \"std_fidelity_roc\": std_fidelity_roc,\n",
    "            \"mean_fidelity_kl_div\": mean_fidelity_kl_div,\n",
    "            \"std_fidelity_kl_div\": std_fidelity_kl_div,\n",
    "        })\n",
    "        if model_task_type == \"regression\":\n",
    "            summary_stats.update({\n",
    "                \"mean_fidelity_mae\": mean_fidelity_mae,\n",
    "                \"std_fidelity_mae\": std_fidelity_mae,\n",
    "                \"mean_fidelity_mse\": mean_fidelity_mse,\n",
    "                \"std_fidelity_mse\": std_fidelity_mse,\n",
    "                \"mean_fidelity_rmse\": mean_fidelity_rmse,\n",
    "                \"std_fidelity_rmse\": std_fidelity_rmse,\n",
    "                \"mean_fidelity_r2\": mean_fidelity_r2,\n",
    "                \"std_fidelity_r2\": std_fidelity_r2,\n",
    "            })\n",
    "    else:\n",
    "        summary_stats.update({\n",
    "            \"mean_mae\": mean_mae,\n",
    "            \"std_mae\": std_mae,\n",
    "            \"mean_mse\": mean_mse,\n",
    "            \"std_mse\": std_mse,\n",
    "            \"mean_rmse\": mean_rmse,\n",
    "            \"std_rmse\": std_rmse,\n",
    "            \"mean_r2\": mean_r2,\n",
    "            \"std_r2\": std_r2,\n",
    "            \"mean_fidelity_mae\": mean_fidelity_mae,\n",
    "            \"std_fidelity_mae\": std_fidelity_mae,\n",
    "            \"mean_fidelity_mse\": mean_fidelity_mse,\n",
    "            \"std_fidelity_mse\": std_fidelity_mse,\n",
    "            \"mean_fidelity_rmse\": mean_fidelity_rmse,\n",
    "            \"std_fidelity_rmse\": std_fidelity_rmse,\n",
    "            \"mean_fidelity_r2\": mean_fidelity_r2,\n",
    "            \"std_fidelity_r2\": std_fidelity_r2,\n",
    "        })\n",
    "\n",
    "    # Load existing summary file if it exists, otherwise create new\n",
    "    summary_file = os.path.join(config[\"data\"][\"results_dir_path\"], \"student\", f\"{dataset_id}_results.json\")\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(summary_file), exist_ok=True)\n",
    "    if os.path.exists(summary_file):\n",
    "        with open(summary_file, 'r') as f:\n",
    "            all_results = json.load(f)\n",
    "    else:\n",
    "        all_results = {}\n",
    "\n",
    "    # Add current model results to the dataset summary\n",
    "    # Use simple incremental numbering\n",
    "    next_num = len(all_results) + 1\n",
    "    model_key = str(next_num)\n",
    "\n",
    "    # Add current model results to the dataset summary\n",
    "    all_results[model_key] = summary_stats\n",
    "\n",
    "    # Save updated summary\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    logger.info(f\"Summary statistics saved to: {summary_file}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # SAVE STUDENT PREDICTIONS\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Combine predictions from all outer folds\n",
    "    if output_dfs:  # Check if we have any DataFrames to concatenate\n",
    "        output_df = pd.concat(output_dfs, ignore_index=True)\n",
    "    else:\n",
    "        output_df = pd.DataFrame(columns=[\"index\", \"output\"])\n",
    "    output_df = output_df.sort_values(by=\"index\")\n",
    "    \n",
    "    # Create the full directory structure\n",
    "    sub_folder = \"hpo\" if use_hpo else \"default\"\n",
    "    output_dir = os.path.join(config[\"data\"][\"output_dir_path\"], sub_folder, \"student\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    output_file = os.path.join(output_dir, f\"{dataset_id}_{student_model_type}({teacher_model_type}).csv\")\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "    logger.info(f\"{config['model']['student_model']} outputs saved to: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
