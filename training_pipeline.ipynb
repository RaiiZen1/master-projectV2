{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3eaf3d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, roc_auc_score\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from tabpfn import TabPFNClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae3463",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7bf7565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    Set random seeds for reproducibility across Python, NumPy, PyTorch, and TensorFlow.\n",
    "\n",
    "    Args:\n",
    "        seed: Integer seed for random number generators\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "def setup_logging() -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Set up logging configuration.\n",
    "    This function configures the logging settings for the application, including\n",
    "    the logging level and format. It returns a logger instance that can be used\n",
    "    throughout the application.\n",
    "    Returns:\n",
    "        logging.Logger: Configured logger instance.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def check_GPU_availability():\n",
    "    \"\"\"\n",
    "    Checks if a GPU is available and configures TensorFlow to use it appropriately.\n",
    "\n",
    "    Sets up memory growth for TensorFlow GPU usage to avoid allocating all GPU memory at once.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: A torch device object ('cuda' if GPU is available, 'cpu' otherwise)\n",
    "    \"\"\"\n",
    "    logger = setup_logging()\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"Using GPU: {torch.cuda.get_device_name()}\")\n",
    "        device = torch.device(\"cuda\")\n",
    "        gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        logger.info(\"Using CPU for training.\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14164da",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3635ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from common.utils import setup_logging\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    LabelEncoder,\n",
    "    TargetEncoder,\n",
    ")\n",
    "\n",
    "\n",
    "def encode_target(y):\n",
    "    \"\"\"\n",
    "    Encode the target variable using LabelEncoder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array\n",
    "        The target variable to encode.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_encoded : np.array\n",
    "        The encoded target variable.\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    return y_encoded\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_val: pd.DataFrame,\n",
    "    cat_cols: list,\n",
    "    config: dict,\n",
    "    preprocessing_type: str,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Preprocess the training and validation datasets based on the specified model type.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : pandas.DataFrame\n",
    "        The training dataset features.\n",
    "    y_train : pandas.Series or np.array\n",
    "        The target variable for the training dataset.\n",
    "    X_val : pandas.DataFrame\n",
    "        The validation dataset features.\n",
    "    cat_cols : list or array of bool\n",
    "        Boolean mask indicating which columns in X_train and X_val are categorical (True).\n",
    "    config : dict\n",
    "        Configuration dictionary containing model and preprocessing details.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train : np.array\n",
    "        The preprocessed training dataset features.\n",
    "    X_val : np.array\n",
    "        The preprocessed validation dataset features.\n",
    "    \"\"\"\n",
    "    X_train, preprocessor_inner = _minimal_preprocess_train(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cat_cols,\n",
    "        preprocessing_type,\n",
    "        config,\n",
    "    )\n",
    "    X_val = _minimal_preprocess_test(X_val, preprocessor_inner)\n",
    "\n",
    "    return X_train, X_val\n",
    "\n",
    "\n",
    "def _minimal_preprocess_train(X, y, categorical_features, model_type, config):\n",
    "    \"\"\"\n",
    "    Perform minimal preprocessing on the input features for training.\n",
    "\n",
    "    This function applies preprocessing to both numeric and categorical features\n",
    "    based on the model type specified in the configuration. For Neural Networks,\n",
    "    it standardizes numeric features and one-hot encodes categorical features. For\n",
    "    tree-based models, it does not scale numeric features and applies different\n",
    "    encoding strategies for low- and high-cardinality categorical features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pandas.DataFrame\n",
    "        The input features.\n",
    "\n",
    "    categorical_features : list or array of bool\n",
    "        Boolean mask indicating which columns in X are categorical (True) and\n",
    "        which are numeric (False).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_preprocessed : np.array\n",
    "        The preprocessed features.\n",
    "    preprocessor : ColumnTransformer\n",
    "        The fitted preprocessor, which can be used to transform future datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the boolean mask to a NumPy array (if not already) for fast indexing.\n",
    "    cat_mask = np.asarray(categorical_features)\n",
    "\n",
    "    # Check that the mask length matches the number of columns in X.\n",
    "    if cat_mask.shape[0] != X.shape[1]:\n",
    "        raise ValueError(\n",
    "            \"Length of categorical_features mask must match the number of columns in X.\"\n",
    "        )\n",
    "\n",
    "    # Extract column names using boolean indexing.\n",
    "    categorical_feature_names = X.columns[cat_mask].tolist()\n",
    "    numeric_feature_names = X.columns[~cat_mask].tolist()\n",
    "\n",
    "    # Select pipeline based on config.\n",
    "    logger = setup_logging()\n",
    "\n",
    "    # Neural Networks: standardize numeric features and one-hot encode categoricals.\n",
    "    if model_type == \"nn\":\n",
    "\n",
    "        # Define the preprocessing pipeline for numeric features.\n",
    "        numeric_pipeline = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "                (\"scaler\", StandardScaler()),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Define the preprocessing pipeline for categorical features.\n",
    "        categorical_pipeline = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\n",
    "                    \"onehot\",\n",
    "                    OneHotEncoder(\n",
    "                        drop=\"first\", sparse_output=False, handle_unknown=\"ignore\"\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        transformers = [\n",
    "            (\"num\", numeric_pipeline, numeric_feature_names),\n",
    "            (\"cat\", categorical_pipeline, categorical_feature_names),\n",
    "        ]\n",
    "\n",
    "        logger.info(\"Preprocessing pipeline for Neural Network model.\")\n",
    "\n",
    "    elif model_type == \"tree\":\n",
    "\n",
    "        # Tree-based models: leave numeric features unscaled, and differentiate encoding for categoricals.\n",
    "        numeric_pipeline = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            ]\n",
    "        )\n",
    "        # Split categorical features into low- and high-cardinality groups.\n",
    "        threshold = config[\"preprocessing\"][\"threshold_high_cardinality\"]\n",
    "        low_card_features = []\n",
    "        high_card_features = []\n",
    "        for feature in categorical_feature_names:\n",
    "            if X[feature].nunique() <= threshold:\n",
    "                low_card_features.append(feature)\n",
    "            else:\n",
    "                high_card_features.append(feature)\n",
    "\n",
    "        # For low-cardinality categoricals, use one-hot encoding.\n",
    "        low_card_pipeline = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\n",
    "                    \"onehot\",\n",
    "                    OneHotEncoder(\n",
    "                        drop=\"first\", sparse_output=False, handle_unknown=\"ignore\"\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # For high-cardinality categoricals, use target encoding.\n",
    "        high_card_pipeline = Pipeline(\n",
    "            [\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"target_encode\", TargetEncoder()),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        transformers = [\n",
    "            (\"num\", numeric_pipeline, numeric_feature_names),\n",
    "            (\"low_card\", low_card_pipeline, low_card_features),\n",
    "            (\"high_card\", high_card_pipeline, high_card_features),\n",
    "        ]\n",
    "\n",
    "        logger.info(\"Preprocessing pipeline for Decision Tree model.\")\n",
    "\n",
    "    elif model_type == \"grande\":\n",
    "        logger.info(\"No preprocessing required for Grande model.\")\n",
    "        return X, None\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Unknown teacher type. Must be 'nn' for Neural Network, 'tree' for Decision Tree, or 'grande' for Gradient Based Tree.\"\n",
    "        )\n",
    "\n",
    "    # Create a ColumnTransformer to apply the transformations to the appropriate columns.\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        n_jobs=-2,\n",
    "    )\n",
    "\n",
    "    # Fit the preprocessor on the training data and transform it.\n",
    "    X_preprocessed = preprocessor.fit_transform(X, y)\n",
    "\n",
    "    return X_preprocessed, preprocessor\n",
    "\n",
    "\n",
    "def _minimal_preprocess_test(X, preprocessor):\n",
    "    \"\"\"\n",
    "    Transform the input features according to the preprocessor fitted on the training data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pandas.DataFrame\n",
    "        The input features.\n",
    "    preprocessor : ColumnTransformer\n",
    "        The preprocessor fitted on the training data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_preprocessed : np.array\n",
    "        The preprocessed features.\n",
    "    \"\"\"\n",
    "    if preprocessor is None:\n",
    "        return X\n",
    "    return preprocessor.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d700d2",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f642a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openml\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# from common.preprocessing import encode_target\n",
    "# from common.utils import setup_logging\n",
    "from typing import Literal\n",
    "\n",
    "def load_dataset(dataset_id: int, config: dict):\n",
    "    \"\"\"\n",
    "    Loads an OpenML dataset based on its ID and processes it according to the task type.\n",
    "\n",
    "    This function fetches the dataset using the _fetch_dataset helper, then processes\n",
    "    the target variable based on whether the task is binary classification or regression.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_id (int): The ID of the dataset on OpenML.\n",
    "        config (dict): Configuration dictionary containing data paths and settings.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - X: The feature data (pandas DataFrame)\n",
    "            - y: The processed target variable (encoded for binary classification or numpy array for regression)\n",
    "            - cat_cols: List of booleans indicating which features are categorical\n",
    "            - attribute_names: List of attribute names\n",
    "            - task_type: String indicating the task type ('binary' or 'regression')\n",
    "    \"\"\"\n",
    "    X, y, cat_cols, attribute_names, task_type = fetch_dataset(\n",
    "        dataset_id=dataset_id,\n",
    "        cache_dir=config[\"data\"][\"cache_dir_path\"],\n",
    "    )\n",
    "    if task_type == \"binary\":\n",
    "        # Encode the target variable if it's binary\n",
    "        y = encode_target(y)\n",
    "    else:\n",
    "        # Transform to np.array for regression\n",
    "        y = np.array(y, dtype=float)\n",
    "\n",
    "    return X, y, cat_cols, attribute_names, task_type\n",
    "\n",
    "\n",
    "def fetch_dataset(dataset_id: int, cache_dir: str = \"data/cache/\"):\n",
    "    \"\"\"\n",
    "    Downloads an OpenML dataset based on its id, caches the data locally, and returns the data\n",
    "    along with its metadata.\n",
    "\n",
    "    The five returned objects are:\n",
    "        - X: The feature data (typically a pandas DataFrame).\n",
    "        - y: The target variable.\n",
    "        - categorical_indicator: A list of booleans indicating which features are categorical.\n",
    "        - attribute_names: A list of attribute names.\n",
    "        - task_type: A string indicating the type of task ('binary' or 'regression').\n",
    "\n",
    "    If the dataset has been previously downloaded and stored in the cache directory,\n",
    "    it will be loaded from the local file instead of re-downloading.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_id (int): The id of the dataset on OpenML.\n",
    "        cache_dir (str): The directory to store the downloaded dataset. Defaults to \"openml_cache\".\n",
    "\n",
    "    Returns:\n",
    "        X, y, categorical_indicator, attribute_names, task_type\n",
    "    \"\"\"\n",
    "    logger = setup_logging()\n",
    "\n",
    "    # Ensure the cache directory exists\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    # Define a cache file name that is unique to the dataset id\n",
    "    cache_file = os.path.join(cache_dir, f\"openml_dataset_{dataset_id}.pkl\")\n",
    "\n",
    "    # If the cache file exists, load the data from the file.\n",
    "    if os.path.exists(cache_file):\n",
    "        logger.info(\n",
    "            f\"Loading dataset {dataset_id} from cache at '{cache_file}'...\"\n",
    "        )\n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        X = data[\"X\"]\n",
    "        y = data[\"y\"]\n",
    "        categorical_indicator = data[\"categorical_indicator\"]\n",
    "        attribute_names = data[\"attribute_names\"]\n",
    "        task_type = data[\"task_type\"]\n",
    "    else:\n",
    "        # Download the dataset from OpenML.\n",
    "        logger.info(f\"Downloading dataset {dataset_id} from OpenML...\")\n",
    "        dataset = openml.datasets.get_dataset(dataset_id)\n",
    "\n",
    "        # Use the default target attribute (if defined) when retrieving the data.\n",
    "        X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "            target=dataset.default_target_attribute, dataset_format=\"dataframe\"\n",
    "        )\n",
    "\n",
    "        # Determine the task type (binary classification or regression)\n",
    "        task_type = _determine_task_type(y)\n",
    "\n",
    "        # Store the data and metadata in a dictionary.\n",
    "        data = {\n",
    "            \"X\": X,\n",
    "            \"y\": y,\n",
    "            \"categorical_indicator\": categorical_indicator,\n",
    "            \"attribute_names\": attribute_names,\n",
    "            \"task_type\": task_type,\n",
    "        }\n",
    "\n",
    "        # Save the dictionary to a local file using pickle.\n",
    "        with open(cache_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        logger.info(f\"Dataset {dataset_id} stored locally at '{cache_file}'.\")\n",
    "\n",
    "    logger.info(f\"Dataset {dataset_id} loaded successfully with task type: {task_type}\")\n",
    "\n",
    "    return X, y, categorical_indicator, attribute_names, task_type\n",
    "\n",
    "\n",
    "def _determine_task_type(y):\n",
    "    \"\"\"\n",
    "    Determines if the target variable is for binary classification, or regression.\n",
    "\n",
    "    Parameters:\n",
    "        y: The target variable (pandas dataframe).\n",
    "\n",
    "    Returns:\n",
    "        str: 'binary', or 'regression'\n",
    "    \"\"\"\n",
    "\n",
    "    # Get unique values\n",
    "    unique_values = pd.unique(y)\n",
    "\n",
    "    # Check if it's binary (2 unique values)\n",
    "    if len(unique_values) == 2:\n",
    "        return \"binary\"\n",
    "    else:\n",
    "        return \"regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4579812",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ee7681de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "\n",
    "def evaluate_classification(y_prob, y_true):\n",
    "    \"\"\"\n",
    "    Evaluate classification performance using balanced accuracy, F1 score, and ROC AUC.\n",
    "\n",
    "    Parameters:\n",
    "        y_prob (np.array): Predicted probabilities for the positive class.\n",
    "        y_true (np.array): True labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    logger = setup_logging()\n",
    "    y_pred = (y_prob[:, 1] > 0.5).astype(int)\n",
    "\n",
    "    acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    roc_auc = roc_auc_score(y_true, y_prob[:, 1])\n",
    "\n",
    "    logger.info(f\"\\t Balanced Accuracy: {acc:.4f}, F1 Score: {f1:.4f}, ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"roc\": roc_auc,\n",
    "    }\n",
    "\n",
    "def evaluate_regression(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Evaluate regression performance using R^2 score.\n",
    "\n",
    "    Parameters:\n",
    "        y_pred (np.array): Predicted values.\n",
    "        y_true (np.array): True values.\n",
    "\n",
    "    Returns:\n",
    "        float: R^2 score.\n",
    "    \"\"\"\n",
    "    logger = setup_logging()\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    logger.info(f\"\\t MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, R^2: {r2:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"mae\": mae,\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\": r2,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8af1a5",
   "metadata": {},
   "source": [
    "# Model Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f9825c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "# from common.packages.tabm_reference import Model, make_parameter_groups\n",
    "from tabpfn import TabPFNClassifier, TabPFNRegressor\n",
    "from GRANDE import GRANDE\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "class TeacherModelBase(ABC):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.params = kwargs\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model on the given data.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.ndarray): The input features with shape (n_samples, n_features).\n",
    "            y (np.ndarray): The target labels with shape (n_samples,).\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Makes predictions on the provided data.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.ndarray): The input features with shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: An array of predicted probabilities with shape (n_samples, n_classes).\n",
    "                        (If the model returns logits, these should be post-processed to probabilities.)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, y_pred: np.ndarray, y_true: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluates the model's predictions against the true labels.\n",
    "\n",
    "        Parameters:\n",
    "            y_pred (np.ndarray): The predicted probabilities or values.\n",
    "            y_true (np.ndarray): The true labels.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: A dictionary containing evaluation metrics.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            return evaluate_classification(y_pred, y_true)\n",
    "        elif self.task_type == \"regression\":\n",
    "            return evaluate_regression(y_pred, y_true)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown task type: {self.task_type}. Must be 'binary' or 'regression'.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def count_parameters(self) -> int:\n",
    "        \"\"\"\n",
    "        Counts the number of trainable parameters in the model.\n",
    "\n",
    "        Returns:\n",
    "            int: The total number of trainable parameters.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class TabPFNTeacherModel(TeacherModelBase):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a TabPFN teacher model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments to pass to the base\n",
    "            class constructor.\n",
    "        \"\"\"\n",
    "        self.device = kwargs.pop(\"device\")\n",
    "        self.config = kwargs.pop(\"config\")\n",
    "        self.task_type = kwargs.pop(\"task_type\")\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def train(self, X, y, **training_params):\n",
    "        if self.task_type == \"binary\":\n",
    "            self.model = TabPFNClassifier()\n",
    "        else:\n",
    "            self.model = TabPFNRegressor()\n",
    "        # If X is bigger than 10000 samples, reduce data size to 10000\n",
    "        if X.shape[0] > 10000:\n",
    "            X = X[:10000]\n",
    "            y = y[:10000]\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.task_type == \"binary\":\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            return self.model.predict(X)\n",
    "\n",
    "    def count_parameters(self) -> int:\n",
    "        \"\"\"\n",
    "        Counts the number of trainable parameters in the underlying TabPFN model.\n",
    "\n",
    "        Returns:\n",
    "            int: Total number of trainable parameters.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the TabPFN model has not been fitted yet.\n",
    "        \"\"\"\n",
    "        if not hasattr(self.model, \"model_\"):\n",
    "            raise ValueError(\n",
    "                \"The TabPFN model is not fitted yet. Please call fit() first.\"\n",
    "            )\n",
    "        return sum(p.numel() for p in self.model.model_.parameters() if p.requires_grad)\n",
    "    \n",
    "class CatBoostTeacherModel(TeacherModelBase):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a CatBoost teacher model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments to pass to the base class constructor.\n",
    "        \"\"\"\n",
    "        self.device = kwargs.pop(\"device\")\n",
    "        self.config = kwargs.pop(\"config\")\n",
    "        self.task_type = kwargs.pop(\"task_type\")\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def train(self, X, y, **training_params):\n",
    "        \"\"\"\n",
    "        Train the CatBoost model using the provided data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target data.\n",
    "        **training_params : dict\n",
    "            Additional training parameters to pass to the CatBoost model's fit method.\n",
    "        \"\"\"\n",
    "        config = self.config\n",
    "        random_state = config[\"training\"][\"random_state\"]\n",
    "\n",
    "        # Initialize the CatBoost model\n",
    "        if self.task_type == \"binary\":\n",
    "            self.model = CatBoostClassifier(\n",
    "                random_seed=random_state,\n",
    "                thread_count=-1,  # Use all available CPU cores\n",
    "                logging_level=\"Silent\",  # Suppress CatBoost output\n",
    "                task_type=(\"GPU\" if str(self.device).lower() == \"cuda\" else \"CPU\"),\n",
    "                **self.params,  # Include the sampled hyperparameters\n",
    "            )\n",
    "        else:\n",
    "            self.model = CatBoostRegressor(\n",
    "                random_seed=random_state,\n",
    "                thread_count=-1,  # Use all available CPU cores\n",
    "                logging_level=\"Silent\",  # Suppress CatBoost output\n",
    "                task_type=(\"GPU\" if str(self.device).lower() == \"cuda\" else \"CPU\"),\n",
    "                **self.params,  # Include the sampled hyperparameters\n",
    "            )\n",
    "\n",
    "        # Split data into training and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        self.model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            **training_params,  # Pass additional training parameters\n",
    "        )\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on the given data using the trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array-like of shape (n_samples, n_classes)\n",
    "            The predicted probabilities for each class.\n",
    "        \"\"\"\n",
    "        if self.task_type == \"binary\":\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            return self.model.predict(X)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"\n",
    "        Count the number of parameters in the trained CatBoost model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The number of parameters in the model.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"model\") or self.model is None:\n",
    "            raise ValueError(\"The model has not been trained yet.\")\n",
    "\n",
    "        # Get the number of trees in the model\n",
    "        tree_count = self.model.tree_count_\n",
    "\n",
    "        # For each tree, count:\n",
    "        # - split values (one per non-leaf node)\n",
    "        # - leaf values (one per leaf node)\n",
    "        # - feature indices (one per non-leaf node)\n",
    "        # For a binary tree with depth d, there are 2^d - 1 non-leaf nodes and 2^d leaf nodes\n",
    "        # This is a simplified approximation\n",
    "        approx_depth = self.params.get(\"depth\", 6)  # Default depth in CatBoost is 6\n",
    "        non_leaf_nodes = 2**approx_depth - 1\n",
    "        leaf_nodes = 2**approx_depth\n",
    "\n",
    "        # Total parameters per tree\n",
    "        params_per_tree = (\n",
    "            non_leaf_nodes * 2 + leaf_nodes\n",
    "        )  # split values + feature indices + leaf values\n",
    "\n",
    "        return tree_count * params_per_tree\n",
    "    \n",
    "def get_teacher_model(config: dict, task_type: Literal[\"binary\", \"regression\"], device) -> TeacherModelBase:\n",
    "    \"\"\"\n",
    "    Returns an instance of the teacher model based on the configuration.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : dict\n",
    "        Configuration dictionary containing model and dataset details.\n",
    "    device : torch.device\n",
    "        The device to use for training (CPU or GPU).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TeacherModelBase\n",
    "        An instance of the teacher model.\n",
    "    \"\"\"\n",
    "    model_type = config[\"model\"][\"teacher_model\"]\n",
    "    if model_type == \"tabpfn\":\n",
    "        return TabPFNTeacherModel(config=config, task_type=task_type, device=device)\n",
    "    elif model_type == \"catboost\":\n",
    "        return CatBoostTeacherModel(config=config, task_type=task_type, device=device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown teacher type: {config['model']['teacher_model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02857a",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "59b406d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data\": {\n",
    "        \"datasets\": [\n",
    "                     23381, # Binary classification datasets\n",
    "                    #    197, # Regression datasets\n",
    "                     ],  \n",
    "        \"cache_dir_path\": \"data/cache/\",\n",
    "    },\n",
    "\n",
    "    \"preprocessing\": {\n",
    "        \"threshold_high_cardinality\": 10,  \n",
    "    },\n",
    "\n",
    "    \"model\": {\n",
    "        \"teacher_model\": \"tabpfn\",   # Options: 'tabpfn', \n",
    "        \"student_model\": \"catboost\", # Options: 'catboost', \n",
    "    },\n",
    "\n",
    "    \"training\": {\n",
    "        \"random_state\": 42,\n",
    "        \"outer_folds\": 5,\n",
    "        \"inner_folds\": 2,\n",
    "    },\n",
    "\n",
    "    \"teacher_models\": {\n",
    "        \"tabpfn\": {\n",
    "            \"preprocessing\": \"nn\",  \n",
    "        },\n",
    "        \"catboost\": {\n",
    "            \"preprocessing\": \"tree\", \n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17729927",
   "metadata": {},
   "source": [
    "# Train Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dce1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 11:04:29,824 - __main__ - INFO - Loading configuration...\n",
      "2025-06-04 11:04:29,827 - __main__ - INFO - Loading dataset 23381 from cache at 'data/cache/openml_dataset_23381.pkl'...\n",
      "2025-06-04 11:04:29,835 - __main__ - INFO - Dataset 23381 loaded successfully with task type: binary\n",
      "2025-06-04 11:04:29,840 - __main__ - INFO - Using GPU: NVIDIA RTX A6000\n",
      "2025-06-04 11:04:29,845 - __main__ - INFO - Random seed set to 42\n",
      "2025-06-04 11:04:29,855 - __main__ - INFO - -------------------- Outer Fold 1 --------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 11:04:29,865 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 2, 3, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-04 11:04:35,040 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 1...\n",
      "2025-06-04 11:04:36,403 - __main__ - INFO - \t Balanced Accuracy: 0.5837, F1 Score: 0.5766, ROC AUC: 0.6439\n",
      "2025-06-04 11:04:36,418 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 1, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-04 11:04:40,602 - __main__ - INFO - Training Model on Outer Fold 1, Inner Fold 2...\n",
      "2025-06-04 11:04:41,797 - __main__ - INFO - \t Balanced Accuracy: 0.5419, F1 Score: 0.4984, ROC AUC: 0.6085\n",
      "2025-06-04 11:04:41,804 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [4, 5, 7, 9] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-04 11:04:46,123 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-04 11:04:46,127 - __main__ - INFO - Retraining Model on Outer Fold 1\n",
      "2025-06-04 11:04:47,625 - __main__ - INFO - \t Inference Time: 0.50088 seconds\n",
      "2025-06-04 11:04:47,650 - __main__ - INFO - \t Balanced Accuracy: 0.5936, F1 Score: 0.5924, ROC AUC: 0.6695\n",
      "2025-06-04 11:04:47,656 - __main__ - INFO - -------------------- Outer Fold 2 --------------------\n",
      "2025-06-04 11:04:47,671 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [2, 3, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-04 11:04:47,791 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 1...\n",
      "2025-06-04 11:04:48,908 - __main__ - INFO - \t Balanced Accuracy: 0.5983, F1 Score: 0.5919, ROC AUC: 0.6613\n",
      "2025-06-04 11:04:48,920 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 2, 3, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-04 11:04:49,042 - __main__ - INFO - Training Model on Outer Fold 2, Inner Fold 2...\n",
      "2025-06-04 11:04:50,109 - __main__ - INFO - \t Balanced Accuracy: 0.6010, F1 Score: 0.5929, ROC AUC: 0.6606\n",
      "2025-06-04 11:04:50,116 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-04 11:04:50,256 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-04 11:04:50,260 - __main__ - INFO - Retraining Model on Outer Fold 2\n",
      "2025-06-04 11:04:51,689 - __main__ - INFO - \t Inference Time: 0.51690 seconds\n",
      "2025-06-04 11:04:51,711 - __main__ - INFO - \t Balanced Accuracy: 0.5308, F1 Score: 0.5175, ROC AUC: 0.4807\n",
      "2025-06-04 11:04:51,717 - __main__ - INFO - -------------------- Outer Fold 3 --------------------\n",
      "2025-06-04 11:04:51,732 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 1, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-04 11:04:51,842 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 1...\n",
      "2025-06-04 11:04:52,937 - __main__ - INFO - \t Balanced Accuracy: 0.5712, F1 Score: 0.5558, ROC AUC: 0.6298\n",
      "2025-06-04 11:04:52,949 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [3, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-04 11:04:53,082 - __main__ - INFO - Training Model on Outer Fold 3, Inner Fold 2...\n",
      "2025-06-04 11:04:54,174 - __main__ - INFO - \t Balanced Accuracy: 0.5532, F1 Score: 0.5028, ROC AUC: 0.6395\n",
      "2025-06-04 11:04:54,181 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [2, 5, 7, 8, 9] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-04 11:04:54,302 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-04 11:04:54,306 - __main__ - INFO - Retraining Model on Outer Fold 3\n",
      "2025-06-04 11:04:55,707 - __main__ - INFO - \t Inference Time: 0.49253 seconds\n",
      "2025-06-04 11:04:55,729 - __main__ - INFO - \t Balanced Accuracy: 0.6195, F1 Score: 0.6179, ROC AUC: 0.6486\n",
      "2025-06-04 11:04:55,735 - __main__ - INFO - -------------------- Outer Fold 4 --------------------\n",
      "2025-06-04 11:04:55,749 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 2, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-04 11:04:55,867 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 1...\n",
      "2025-06-04 11:04:56,954 - __main__ - INFO - \t Balanced Accuracy: 0.5326, F1 Score: 0.4770, ROC AUC: 0.5982\n",
      "2025-06-04 11:04:56,964 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-04 11:04:57,099 - __main__ - INFO - Training Model on Outer Fold 4, Inner Fold 2...\n",
      "2025-06-04 11:04:58,195 - __main__ - INFO - \t Balanced Accuracy: 0.5523, F1 Score: 0.5366, ROC AUC: 0.5722\n",
      "2025-06-04 11:04:58,202 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [3, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-04 11:04:58,322 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-04 11:04:58,326 - __main__ - INFO - Retraining Model on Outer Fold 4\n",
      "2025-06-04 11:04:59,728 - __main__ - INFO - \t Inference Time: 0.49145 seconds\n",
      "2025-06-04 11:04:59,751 - __main__ - INFO - \t Balanced Accuracy: 0.6486, F1 Score: 0.6484, ROC AUC: 0.7139\n",
      "2025-06-04 11:04:59,756 - __main__ - INFO - -------------------- Outer Fold 5 --------------------\n",
      "2025-06-04 11:04:59,770 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 4, 5, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-04 11:04:59,882 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 1...\n",
      "2025-06-04 11:05:00,997 - __main__ - INFO - \t Balanced Accuracy: 0.4957, F1 Score: 0.3651, ROC AUC: 0.6307\n",
      "2025-06-04 11:05:01,010 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [2, 3, 4, 5, 6, 7, 8, 9, 10] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-04 11:05:01,153 - __main__ - INFO - Training Model on Outer Fold 5, Inner Fold 2...\n",
      "2025-06-04 11:05:02,256 - __main__ - INFO - \t Balanced Accuracy: 0.5382, F1 Score: 0.5072, ROC AUC: 0.5904\n",
      "2025-06-04 11:05:02,262 - __main__ - INFO - Preprocessing pipeline for Neural Network model.\n",
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [0, 4, 5, 8, 9] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "2025-06-04 11:05:02,374 - __main__ - INFO - ------------------------------------------------------\n",
      "2025-06-04 11:05:02,378 - __main__ - INFO - Retraining Model on Outer Fold 5\n",
      "2025-06-04 11:05:03,834 - __main__ - INFO - \t Inference Time: 0.53472 seconds\n",
      "2025-06-04 11:05:03,858 - __main__ - INFO - \t Balanced Accuracy: 0.5965, F1 Score: 0.5706, ROC AUC: 0.6929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold indices saved to: data/cache/dataset_23381_fold_indices.json\n",
      "tabpfn outputs saved to: data/cache/teacher_outputs_23381_tabpfn.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import time\n",
    "\n",
    "# Get list of datasets to process from configuration\n",
    "datasets = config[\"data\"][\"datasets\"]\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINING LOOP - Process each dataset independently\n",
    "# =============================================================================\n",
    "for dataset_id in datasets:\n",
    "# -------------------------------------------------------------------------\n",
    "    # SETUP AND INITIALIZATION\n",
    "    # -------------------------------------------------------------------------\n",
    "    logger = setup_logging()\n",
    "    logger.info(\"Loading configuration...\")\n",
    "\n",
    "    # Extract model configuration for this run\n",
    "    model_type = config[\"model\"][\"teacher_model\"]\n",
    "    preprocessing_type = config[\"teacher_models\"][model_type][\"preprocessing\"] \n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 0: DATA LOADING\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Load dataset from OpenML with caching for efficiency\n",
    "    X, y, cat_cols, _, task_type = load_dataset(\n",
    "        dataset_id=dataset_id,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 1: INFRASTRUCTURE SETUP\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Note: Checking for existing results is placeholder for future implementation\n",
    "\n",
    "    # Configure GPU/CPU usage for training\n",
    "    device = check_GPU_availability()\n",
    "\n",
    "    # Set random seed for reproducibility across all libraries\n",
    "    set_seed(config[\"training\"][\"random_state\"])\n",
    "    logger.info(f\"Random seed set to {config['training']['random_state']}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 2: INITIALIZE DATA STRUCTURES\n",
    "    # -------------------------------------------------------------------------\n",
    "    # List to store predictions from each outer fold\n",
    "    output_dfs = []\n",
    "    \n",
    "    # Dictionary to store all fold indices for reproducibility and student training\n",
    "    # Structure: {\"outer_folds\": {fold_id: {train_idx, test_idx}},\n",
    "    #            \"inner_folds\": {outer_fold_id: {inner_fold_id: {train_idx, val_idx}}}}\n",
    "    fold_indices = {\n",
    "        \"outer_folds\": {},\n",
    "        \"inner_folds\": {}\n",
    "    }\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 3: OUTER CROSS-VALIDATION SETUP\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Choose appropriate CV strategy based on task type to maintain class balance\n",
    "    if task_type == \"binary\":\n",
    "        outer_cv = StratifiedKFold(\n",
    "            n_splits=config[\"training\"][\"outer_folds\"],\n",
    "            shuffle=True,\n",
    "            random_state=config[\"training\"][\"random_state\"],\n",
    "        )\n",
    "    else:\n",
    "        outer_cv = KFold(\n",
    "            n_splits=config[\"training\"][\"outer_folds\"],\n",
    "            shuffle=True,\n",
    "            random_state=config[\"training\"][\"random_state\"],\n",
    "        )\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEP 4: OUTER CROSS-VALIDATION LOOP\n",
    "    # =========================================================================\n",
    "    # Each iteration provides one unbiased performance estimate\n",
    "    for outer_fold, (train_idx, test_idx) in enumerate(outer_cv.split(X, y), start=1):\n",
    "\n",
    "        logger.info(f\"-------------------- Outer Fold {outer_fold} --------------------\")\n",
    "        \n",
    "        # ---------------------------------------------------------------------\n",
    "        # FOLD INDEX MANAGEMENT\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Store outer fold indices for later use in student training and validation\n",
    "        fold_indices[\"outer_folds\"][f\"fold_{outer_fold}\"] = {\n",
    "            \"train_idx\": train_idx.tolist(),\n",
    "            \"test_idx\": test_idx.tolist()\n",
    "        }\n",
    "\n",
    "        # Split data according to current outer fold\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # INNER CROSS-VALIDATION SETUP (for model validation)\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Choose appropriate CV strategy for inner folds\n",
    "        if task_type == \"binary\":\n",
    "            inner_cv = StratifiedKFold(\n",
    "                n_splits=config[\"training\"][\"inner_folds\"],\n",
    "                shuffle=True,\n",
    "                random_state=config[\"training\"][\"random_state\"],\n",
    "            )\n",
    "        else:\n",
    "            inner_cv = KFold(\n",
    "                n_splits=config[\"training\"][\"inner_folds\"],\n",
    "                shuffle=True,\n",
    "                random_state=config[\"training\"][\"random_state\"],\n",
    "            )\n",
    "        \n",
    "        # Initialize storage for inner fold indices within this outer fold\n",
    "        fold_indices[\"inner_folds\"][f\"outer_fold_{outer_fold}\"] = {}\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 5: INNER CROSS-VALIDATION LOOP (hyperparameter validation)\n",
    "        # =====================================================================\n",
    "        # This loop would typically be used for hyperparameter optimization\n",
    "        for inner_fold, (inner_train_index, inner_val_index) in enumerate(inner_cv.split(X_train, y_train), start=1):\n",
    "            \n",
    "            # -----------------------------------------------------------------\n",
    "            # INDEX MANAGEMENT (Critical for avoiding data leakage)\n",
    "            # -----------------------------------------------------------------\n",
    "            # Convert relative indices (within outer training set) to absolute indices\n",
    "            # This is crucial for maintaining consistency with teacher outputs\n",
    "            absolute_inner_train_idx = train_idx[inner_train_index]\n",
    "            absolute_inner_val_idx = train_idx[inner_val_index]\n",
    "            \n",
    "            # Store inner fold indices using absolute indices for consistency\n",
    "            fold_indices[\"inner_folds\"][f\"outer_fold_{outer_fold}\"][f\"inner_fold_{inner_fold}\"] = {\n",
    "                \"train_idx\": absolute_inner_train_idx.tolist(),\n",
    "                \"val_idx\": absolute_inner_val_idx.tolist()\n",
    "            }\n",
    "\n",
    "            # Split inner training data using relative indices\n",
    "            X_inner_train, X_inner_val = X_train.iloc[inner_train_index], X_train.iloc[inner_val_index]\n",
    "            y_inner_train, y_inner_val = y_train[inner_train_index], y_train[inner_val_index]\n",
    "\n",
    "            # -----------------------------------------------------------------\n",
    "            # PREPROCESSING: Apply model-specific data transformations\n",
    "            # -----------------------------------------------------------------\n",
    "            X_inner_train, X_inner_val = preprocess(\n",
    "                X_inner_train,\n",
    "                y_inner_train,\n",
    "                X_inner_val, \n",
    "                cat_cols,\n",
    "                config,\n",
    "                preprocessing_type=preprocessing_type,\n",
    "            )\n",
    "\n",
    "            # -----------------------------------------------------------------\n",
    "            # MODEL TRAINING: Train teacher model on inner training data\n",
    "            # -----------------------------------------------------------------\n",
    "            model = get_teacher_model(config=config, task_type=task_type, device=device)\n",
    "            logger.info(f\"Training Model on Outer Fold {outer_fold}, Inner Fold {inner_fold}...\")\n",
    "            model.train(X_inner_train, y_inner_train)\n",
    "\n",
    "            # -----------------------------------------------------------------\n",
    "            # VALIDATION: Evaluate model performance on inner validation set\n",
    "            # -----------------------------------------------------------------\n",
    "            val_preds = model.predict(X_inner_val)\n",
    "            val_metrics = model.evaluate(val_preds, y_inner_val)\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 6: FINAL MODEL TRAINING (on complete outer training set)\n",
    "        # =====================================================================\n",
    "        # After inner CV completes, train final model on all available training data\n",
    "        \n",
    "        # Apply same preprocessing to outer training and test sets\n",
    "        X_train, X_test = preprocess(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            cat_cols,\n",
    "            config,\n",
    "            preprocessing_type=preprocessing_type,  # Use 'nn' for TabPFN preprocessing\n",
    "        )\n",
    "        \n",
    "        # Train final model on complete outer training set\n",
    "        logger.info(\"------------------------------------------------------\")\n",
    "        logger.info(f\"Retraining Model on Outer Fold {outer_fold}\")\n",
    "        model.train(X_train, y_train)\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 7: FINAL EVALUATION (unbiased performance on outer test set)\n",
    "        # =====================================================================\n",
    "        # This provides the unbiased performance estimate for this fold\n",
    "        start_time = time.time()\n",
    "        test_preds = model.predict(X_test)\n",
    "        end_time = time.time() - start_time\n",
    "        logger.info(f\"\\t Inference Time: {end_time:.5f} seconds\")\n",
    "        test_metrics = model.evaluate(test_preds, y_test)\n",
    "\n",
    "        # =====================================================================\n",
    "        # STEP 8: STORE PREDICTIONS FOR STUDENT TRAINING\n",
    "        # =====================================================================\n",
    "        # Save predictions with their corresponding dataset indices\n",
    "        # These will be used as targets for training student models\n",
    "        output_dfs.append(pd.DataFrame({\n",
    "            \"index\": test_idx,\n",
    "            \"output\": test_preds[:, 1] if task_type == \"binary\" else test_preds\n",
    "        }))\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 9: SAVE RESULTS AND METADATA\n",
    "    # =========================================================================\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # SAVE FOLD INDICES (for reproducibility and student training)\n",
    "    # -------------------------------------------------------------------------\n",
    "    import json\n",
    "    fold_indices_file = os.path.join(config[\"data\"][\"cache_dir_path\"], f\"dataset_{dataset_id}_fold_indices.json\")\n",
    "    with open(fold_indices_file, 'w') as f:\n",
    "        json.dump(fold_indices, f, indent=2)\n",
    "    print(f\"Fold indices saved to: {fold_indices_file}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # SAVE TEACHER PREDICTIONS (targets for student training)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Combine predictions from all outer folds\n",
    "    if output_dfs:  # Check if we have any DataFrames to concatenate\n",
    "        output_df = pd.concat(output_dfs, ignore_index=True)\n",
    "    else:\n",
    "        output_df = pd.DataFrame(columns=[\"index\", \"output\"])\n",
    "    output_df = output_df.sort_values(by=\"index\")\n",
    "    output_file = os.path.join(config[\"data\"][\"cache_dir_path\"], f\"teacher_outputs_{dataset_id}_{model_type}.csv\")\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "    print(f\"{config['model']['teacher_model']} outputs saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c1e150",
   "metadata": {},
   "source": [
    "# Train Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289fc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 09:02:28,598 - __main__ - INFO - Dataset 23381 loaded successfully from cache at 'data/cache/openml_dataset_23381.pkl'...\n",
      "2025-06-04 09:02:28,887 - __main__ - INFO - Using GPU: NVIDIA RTX A6000\n",
      "2025-06-04 09:02:28,967 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded fold indices from: data/cache/dataset_23381_fold_indices.json\n",
      "Loaded TabPFN outputs from: data/cache/dataset_23381_tabpfn_outputs.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mherre/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:242: UserWarning: Found unknown categories in columns [1, 2, 3] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "TBB Warning: The number of workers is currently limited to 11. The request for 39 workers is ignored. Further requests for more workers will be silently ignored until the limit changes.\n",
      "\n",
      "2025-06-04 09:02:35,085 - __main__ - INFO - Preprocessing pipeline for Decision Tree model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Fold 1, Inner Fold 1 - Accuracy: 0.6238, F1 Score: 0.6221, ROC AUC: 0.6975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process LokyProcess-8:\n",
      "Process LokyProcess-7:\n",
      "Process LokyProcess-9:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mherre/miniconda3/envs/thesis/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mherre/miniconda3/envs/thesis/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mherre/.local/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 478, in _process_worker\n",
      "    _process_reference_size = _get_memory_usage(pid, force_gc=True)\n",
      "  File \"/home/mherre/.local/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 109, in _get_memory_usage\n",
      "    gc.collect()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mherre/miniconda3/envs/thesis/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mherre/miniconda3/envs/thesis/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mherre/.local/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 478, in _process_worker\n",
      "    _process_reference_size = _get_memory_usage(pid, force_gc=True)\n",
      "  File \"/home/mherre/.local/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 109, in _get_memory_usage\n",
      "    gc.collect()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mherre/miniconda3/envs/thesis/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mherre/miniconda3/envs/thesis/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mherre/.local/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 478, in _process_worker\n",
      "    _process_reference_size = _get_memory_usage(pid, force_gc=True)\n",
      "  File \"/home/mherre/.local/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 109, in _get_memory_usage\n",
      "    gc.collect()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m teacher_logits_inner_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([index_to_logits[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m absolute_inner_val_idx])\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Preprocess the data for tree-based model\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m X_inner_train, X_inner_val \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_inner_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_inner_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_inner_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcat_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocessing_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtree\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Train CatBoost regressor to predict teacher logits\u001b[39;00m\n\u001b[1;32m     75\u001b[0m catboost \u001b[38;5;241m=\u001b[39m CatBoostRegressor(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[3], line 73\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(X_train, y_train, X_val, cat_cols, config, preprocessing_type)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mPreprocess the training and validation datasets based on the specified model type.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    The preprocessed validation dataset features.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m X_train, preprocessor_inner \u001b[38;5;241m=\u001b[39m _minimal_preprocess_train(\n\u001b[1;32m     67\u001b[0m     X_train,\n\u001b[1;32m     68\u001b[0m     y_train,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     config,\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m X_val \u001b[38;5;241m=\u001b[39m \u001b[43m_minimal_preprocess_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor_inner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_train, X_val\n",
      "Cell \u001b[0;32mIn[3], line 238\u001b[0m, in \u001b[0;36m_minimal_preprocess_test\u001b[0;34m(X, preprocessor)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:1076\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1074\u001b[0m     routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_empty_routing()\n\u001b[0;32m-> 1076\u001b[0m Xs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_func_on_transformers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_transform_one\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_as_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_dataframe_and_transform_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_output(Xs)\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Xs:\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;66;03m# All transformers are None\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:885\u001b[0m, in \u001b[0;36mColumnTransformer._call_func_on_transformers\u001b[0;34m(self, X, y, func, column_as_labels, routed_params)\u001b[0m\n\u001b[1;32m    873\u001b[0m             extra_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    874\u001b[0m         jobs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    875\u001b[0m             delayed(func)(\n\u001b[1;32m    876\u001b[0m                 transformer\u001b[38;5;241m=\u001b[39mclone(trans) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted \u001b[38;5;28;01melse\u001b[39;00m trans,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    882\u001b[0m             )\n\u001b[1;32m    883\u001b[0m         )\n\u001b[0;32m--> 885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import json\n",
    "\n",
    "datasets = config[\"data\"][\"datasets\"]\n",
    "\n",
    "for dataset_id in datasets:\n",
    "    X, y, cat_cols, _, task_type = load_dataset(\n",
    "        dataset_id=dataset_id,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    device = check_GPU_availability()\n",
    "\n",
    "    set_seed(config[\"training\"][\"random_state\"])\n",
    "\n",
    "    # Load the saved fold indices \n",
    "    fold_indices_file = os.path.join(config[\"data\"][\"cache_dir_path\"], f\"dataset_{dataset_id}_fold_indices.json\")\n",
    "    with open(fold_indices_file, 'r') as f:\n",
    "        fold_indices = json.load(f)\n",
    "    print(f\"Loaded fold indices from: {fold_indices_file}\") \n",
    "\n",
    "    # Load the TabPFN outputs (teacher predictions)\n",
    "    tabpfn_outputs_file = os.path.join(config[\"data\"][\"cache_dir_path\"], f\"dataset_{dataset_id}_tabpfn_outputs.csv\")\n",
    "    teacher_outputs_df = pd.read_csv(tabpfn_outputs_file)\n",
    "    print(f\"Loaded TabPFN outputs from: {tabpfn_outputs_file}\")\n",
    "\n",
    "    # Convert probabilities to logits\n",
    "    # Clip probabilities to avoid log(0) or log(1)\n",
    "    eps = 1e-7\n",
    "    teacher_probs = np.clip(teacher_outputs_df['output'].values, eps, 1 - eps)\n",
    "    teacher_logits = np.log(teacher_probs / (1 - teacher_probs))\n",
    "    \n",
    "    # Create a mapping from index to logits for easy lookup\n",
    "    index_to_logits = dict(zip(teacher_outputs_df['index'].values, teacher_logits))\n",
    "\n",
    "    output_df = pd.DataFrame(columns=[\"index\", \"output\"])\n",
    "\n",
    "    for fold_key, fold_data in fold_indices[\"outer_folds\"].items():\n",
    "        outer_fold = int(fold_key.split('_')[1])\n",
    "        train_idx = np.array(fold_data[\"train_idx\"])\n",
    "        test_idx = np.array(fold_data[\"test_idx\"])\n",
    "\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        inner_folds_data = fold_indices[\"inner_folds\"][f\"outer_fold_{outer_fold}\"]\n",
    "\n",
    "        for inner_fold_key, inner_fold_data in inner_folds_data.items():\n",
    "            inner_fold = int(inner_fold_key.split('_')[2])\n",
    "            \n",
    "            # Get absolute indices from saved data\n",
    "            absolute_inner_train_idx = np.array(inner_fold_data[\"train_idx\"])\n",
    "            absolute_inner_val_idx = np.array(inner_fold_data[\"val_idx\"])\n",
    "            \n",
    "            # Convert absolute indices to relative indices for the current outer training set\n",
    "            inner_train_relative = np.where(np.isin(train_idx, absolute_inner_train_idx))[0]\n",
    "            inner_val_relative = np.where(np.isin(train_idx, absolute_inner_val_idx))[0]\n",
    "\n",
    "            X_inner_train, X_inner_val = X_train.iloc[inner_train_relative], X_train.iloc[inner_val_relative]\n",
    "            y_inner_train, y_inner_val = y_train[inner_train_relative], y_train[inner_val_relative]\n",
    "\n",
    "            # Get teacher logits for inner training and validation sets\n",
    "            teacher_logits_inner_train = np.array([index_to_logits[idx] for idx in absolute_inner_train_idx])\n",
    "            teacher_logits_inner_val = np.array([index_to_logits[idx] for idx in absolute_inner_val_idx])\n",
    "\n",
    "            # Preprocess the data for tree-based model\n",
    "            X_inner_train, X_inner_val = preprocess(\n",
    "                X_inner_train,\n",
    "                y_inner_train,\n",
    "                X_inner_val, \n",
    "                cat_cols,\n",
    "                config,\n",
    "                preprocessing_type=\"tree\",\n",
    "            )\n",
    "\n",
    "            # Train CatBoost regressor to predict teacher logits\n",
    "            catboost = CatBoostRegressor(verbose=False)\n",
    "            catboost.fit(X_inner_train, teacher_logits_inner_train)\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            val_pred_logits = catboost.predict(X_inner_val)\n",
    "            # Convert predicted logits back to probabilities for evaluation\n",
    "            val_probs = 1 / (1 + np.exp(-val_pred_logits))\n",
    "            val_preds = (val_probs > 0.5).astype(int)\n",
    "            \n",
    "            val_acc = balanced_accuracy_score(y_inner_val, val_preds)\n",
    "            val_f1 = f1_score(y_inner_val, val_preds, average=\"macro\")\n",
    "            val_roc_auc = roc_auc_score(y_inner_val, val_probs)\n",
    "            print(f\"Outer Fold {outer_fold}, Inner Fold {inner_fold} - Accuracy: {val_acc:.4f}, F1 Score: {val_f1:.4f}, ROC AUC: {val_roc_auc:.4f}\")\n",
    "\n",
    "        # Get teacher logits for outer training set\n",
    "        teacher_logits_train = np.array([index_to_logits[idx] for idx in train_idx])\n",
    "\n",
    "        # Preprocess the outer training and test sets\n",
    "        X_train, X_test = preprocess(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            cat_cols,\n",
    "            config,\n",
    "            preprocessing_type=\"tree\",\n",
    "        )\n",
    "        \n",
    "        # Retrain CatBoost regressor on the full training set for the outer fold\n",
    "        catboost = CatBoostRegressor(verbose=False)\n",
    "        catboost.fit(X_train, teacher_logits_train)\n",
    "\n",
    "        # Evaluate on the outer test set\n",
    "        test_pred_logits = catboost.predict(X_test)\n",
    "        # Convert predicted logits back to probabilities for evaluation\n",
    "        test_probs = 1 / (1 + np.exp(-test_pred_logits))\n",
    "        test_preds = (test_probs > 0.5).astype(int)\n",
    "        \n",
    "        test_acc = balanced_accuracy_score(y_test, test_preds)\n",
    "        test_f1 = f1_score(y_test, test_preds, average=\"macro\")\n",
    "        test_roc_auc = roc_auc_score(y_test, test_probs)\n",
    "        print(f\"Outer Fold {outer_fold} - Test Accuracy: {test_acc:.4f}, Test F1 Score: {test_f1:.4f}, Test ROC AUC: {test_roc_auc:.4f}\")\n",
    "        \n",
    "        # Save student probabilities with indices\n",
    "        output_df = pd.concat(\n",
    "            [\n",
    "                output_df,\n",
    "                pd.DataFrame({\n",
    "                    \"index\": test_idx,\n",
    "                    \"output\": test_probs  # Student probabilities (converted from predicted logits)\n",
    "                })\n",
    "            ],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "    # Export the output DataFrame to a CSV file\n",
    "    output_df = output_df.sort_values(by=\"index\")\n",
    "    output_file = os.path.join(config[\"data\"][\"cache_dir_path\"], f\"dataset_{dataset_id}_catboost_outputs_student.csv\")\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "    print(f\"CatBoost student outputs saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23cc6db",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b141aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Validating Fold Indices ===\n",
      "Total samples in dataset: 500\n",
      "Total test samples across all folds: 500\n",
      "Unique test samples: 500\n",
      " All samples appear exactly once in test sets across folds\n",
      " Teacher outputs cover all dataset indices\n",
      " Fold 1: No overlap between train/test\n",
      " Fold 1, Inner fold inner_fold_1: Indices are subset of outer train\n",
      " Fold 1, Inner fold inner_fold_2: Indices are subset of outer train\n",
      " Fold 1: Inner folds cover all outer training data\n",
      " Fold 2: No overlap between train/test\n",
      " Fold 2, Inner fold inner_fold_1: Indices are subset of outer train\n",
      " Fold 2, Inner fold inner_fold_2: Indices are subset of outer train\n",
      " Fold 2: Inner folds cover all outer training data\n",
      " Fold 3: No overlap between train/test\n",
      " Fold 3, Inner fold inner_fold_1: Indices are subset of outer train\n",
      " Fold 3, Inner fold inner_fold_2: Indices are subset of outer train\n",
      " Fold 3: Inner folds cover all outer training data\n",
      " Fold 4: No overlap between train/test\n",
      " Fold 4, Inner fold inner_fold_1: Indices are subset of outer train\n",
      " Fold 4, Inner fold inner_fold_2: Indices are subset of outer train\n",
      " Fold 4: Inner folds cover all outer training data\n",
      " Fold 5: No overlap between train/test\n",
      " Fold 5, Inner fold inner_fold_1: Indices are subset of outer train\n",
      " Fold 5, Inner fold inner_fold_2: Indices are subset of outer train\n",
      " Fold 5: Inner folds cover all outer training data\n",
      "=== End Validation ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add this validation code after loading the fold indices and before the main loop\n",
    "print(\"=== Validating Fold Indices ===\")\n",
    "\n",
    "# 1. Check that all indices are within valid range\n",
    "total_samples = len(X)\n",
    "print(f\"Total samples in dataset: {total_samples}\")\n",
    "\n",
    "# 2. Collect all test indices across outer folds\n",
    "all_test_indices = []\n",
    "for fold_key, fold_data in fold_indices[\"outer_folds\"].items():\n",
    "    test_idx = np.array(fold_data[\"test_idx\"])\n",
    "    all_test_indices.extend(test_idx)\n",
    "\n",
    "all_test_indices = np.array(all_test_indices)\n",
    "print(f\"Total test samples across all folds: {len(all_test_indices)}\")\n",
    "print(f\"Unique test samples: {len(np.unique(all_test_indices))}\")\n",
    "\n",
    "# 3. Check if all samples appear exactly once in test sets\n",
    "if len(all_test_indices) == len(np.unique(all_test_indices)) == total_samples:\n",
    "    print(\" All samples appear exactly once in test sets across folds\")\n",
    "else:\n",
    "    print(\" Issue with test set coverage!\")\n",
    "\n",
    "# 4. Check teacher outputs coverage\n",
    "teacher_indices = set(teacher_outputs_df['index'].values)\n",
    "dataset_indices = set(range(total_samples))\n",
    "if teacher_indices == dataset_indices:\n",
    "    print(\" Teacher outputs cover all dataset indices\")\n",
    "else:\n",
    "    missing = dataset_indices - teacher_indices\n",
    "    extra = teacher_indices - dataset_indices\n",
    "    print(f\" Teacher outputs mismatch - Missing: {missing}, Extra: {extra}\")\n",
    "\n",
    "# 5. Validate fold structure\n",
    "for fold_key, fold_data in fold_indices[\"outer_folds\"].items():\n",
    "    outer_fold = int(fold_key.split('_')[1])\n",
    "    train_idx = np.array(fold_data[\"train_idx\"])\n",
    "    test_idx = np.array(fold_data[\"test_idx\"])\n",
    "    \n",
    "    # Check no overlap between train and test\n",
    "    overlap = np.intersect1d(train_idx, test_idx)\n",
    "    if len(overlap) == 0:\n",
    "        print(f\" Fold {outer_fold}: No overlap between train/test\")\n",
    "    else:\n",
    "        print(f\" Fold {outer_fold}: Found {len(overlap)} overlapping indices!\")\n",
    "    \n",
    "    # Check inner folds\n",
    "    inner_folds_data = fold_indices[\"inner_folds\"][f\"outer_fold_{outer_fold}\"]\n",
    "    inner_train_indices = []\n",
    "    inner_val_indices = []\n",
    "    \n",
    "    for inner_fold_key, inner_fold_data in inner_folds_data.items():\n",
    "        inner_train_idx = np.array(inner_fold_data[\"train_idx\"])\n",
    "        inner_val_idx = np.array(inner_fold_data[\"val_idx\"])\n",
    "        \n",
    "        # Check inner indices are subset of outer train indices\n",
    "        if np.all(np.isin(inner_train_idx, train_idx)) and np.all(np.isin(inner_val_idx, train_idx)):\n",
    "            print(f\" Fold {outer_fold}, Inner fold {inner_fold_key}: Indices are subset of outer train\")\n",
    "        else:\n",
    "            print(f\" Fold {outer_fold}, Inner fold {inner_fold_key}: Indices not subset of outer train!\")\n",
    "        \n",
    "        inner_train_indices.extend(inner_train_idx)\n",
    "        inner_val_indices.extend(inner_val_idx)\n",
    "    \n",
    "    # Check inner folds cover all outer training data\n",
    "    inner_all = np.unique(np.concatenate([inner_train_indices, inner_val_indices]))\n",
    "    if len(np.setdiff1d(train_idx, inner_all)) == 0:\n",
    "        print(f\" Fold {outer_fold}: Inner folds cover all outer training data\")\n",
    "    else:\n",
    "        missing_in_inner = np.setdiff1d(train_idx, inner_all)\n",
    "        print(f\" Fold {outer_fold}: {len(missing_in_inner)} samples missing from inner folds\")\n",
    "\n",
    "print(\"=== End Validation ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc9aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validating Teacher Logits for Fold 5 ===\n",
      " All 400 training indices have teacher outputs\n",
      " WARNING: 100 test indices found in teacher outputs - possible data leakage!\n",
      "Sample teacher logits: [-0.5883296581435397, -0.79494873035179, -0.49716914708297155, -0.6096161248627843, -0.4098342731792005]\n",
      "Converted to probabilities: [0.3570182  0.31110707 0.37820616 0.35214677 0.39895186]\n",
      "=== End Teacher Logits Validation ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add this inside the main loop, after creating index_to_logits mapping\n",
    "print(f\"\\n=== Validating Teacher Logits for Fold {outer_fold} ===\")\n",
    "\n",
    "# Check if all training indices have corresponding teacher outputs\n",
    "missing_teacher_outputs = []\n",
    "for idx in train_idx:\n",
    "    if idx not in index_to_logits:\n",
    "        missing_teacher_outputs.append(idx)\n",
    "\n",
    "if len(missing_teacher_outputs) == 0:\n",
    "    print(f\" All {len(train_idx)} training indices have teacher outputs\")\n",
    "else:\n",
    "    print(f\" Missing teacher outputs for {len(missing_teacher_outputs)} indices: {missing_teacher_outputs[:10]}...\")\n",
    "\n",
    "# Check if test indices have teacher outputs (they shouldn't for proper validation)\n",
    "test_in_teacher = []\n",
    "for idx in test_idx:\n",
    "    if idx in index_to_logits:\n",
    "        test_in_teacher.append(idx)\n",
    "\n",
    "if len(test_in_teacher) == 0:\n",
    "    print(f\" No test indices found in teacher outputs (proper data leakage prevention)\")\n",
    "else:\n",
    "    print(f\" WARNING: {len(test_in_teacher)} test indices found in teacher outputs - possible data leakage!\")\n",
    "\n",
    "# Sample a few logits to check they're reasonable\n",
    "sample_indices = train_idx[:5]\n",
    "sample_logits = [index_to_logits[idx] for idx in sample_indices]\n",
    "sample_probs = 1 / (1 + np.exp(-np.array(sample_logits)))\n",
    "print(f\"Sample teacher logits: {sample_logits}\")\n",
    "print(f\"Converted to probabilities: {sample_probs}\")\n",
    "print(\"=== End Teacher Logits Validation ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b55ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Validation ===\n",
      " Student output file created with 500 predictions\n",
      " Student predictions cover exactly the expected test indices\n",
      " All student outputs are valid probabilities [0,1]\n",
      "  Range: [0.2589, 0.7412]\n",
      "  Mean: 0.4127\n",
      "=== End Final Validation ===\n"
     ]
    }
   ],
   "source": [
    "# Add this at the very end to verify your student outputs match expectations\n",
    "print(\"=== Final Validation ===\")\n",
    "\n",
    "# Check output file was created and has expected structure\n",
    "if os.path.exists(output_file):\n",
    "    student_df = pd.read_csv(output_file)\n",
    "    print(f\" Student output file created with {len(student_df)} predictions\")\n",
    "    \n",
    "    # Check all test indices are covered\n",
    "    predicted_indices = set(student_df['index'].values)\n",
    "    expected_test_indices = set(all_test_indices)\n",
    "    \n",
    "    if predicted_indices == expected_test_indices:\n",
    "        print(\" Student predictions cover exactly the expected test indices\")\n",
    "    else:\n",
    "        missing = expected_test_indices - predicted_indices\n",
    "        extra = predicted_indices - expected_test_indices\n",
    "        print(f\" Prediction coverage mismatch - Missing: {len(missing)}, Extra: {len(extra)}\")\n",
    "    \n",
    "    # Check output values are reasonable probabilities\n",
    "    outputs = student_df['output'].values\n",
    "    if np.all((outputs >= 0) & (outputs <= 1)):\n",
    "        print(f\" All student outputs are valid probabilities [0,1]\")\n",
    "        print(f\"  Range: [{outputs.min():.4f}, {outputs.max():.4f}]\")\n",
    "        print(f\"  Mean: {outputs.mean():.4f}\")\n",
    "    else:\n",
    "        invalid_count = np.sum((outputs < 0) | (outputs > 1))\n",
    "        print(f\" {invalid_count} invalid probability values found!\")\n",
    "        \n",
    "else:\n",
    "    print(\" Student output file was not created!\")\n",
    "\n",
    "print(\"=== End Final Validation ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
