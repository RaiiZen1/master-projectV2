{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "464337e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.09482451769664475\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def KL(P,Q):\n",
    "     epsilon = 0.00001\n",
    "\n",
    "     # You may want to instead make copies to avoid changing the np arrays.\n",
    "     P = P+epsilon\n",
    "     Q = Q+epsilon\n",
    "\n",
    "     divergence = np.sum(P*np.log(P/Q))\n",
    "     return divergence\n",
    "\n",
    "# Should be normalized though\n",
    "values1 = np.asarray([0.9,0,1])\n",
    "values2 = np.asarray([1,0,1])\n",
    "\n",
    "# Note slight difference in the final result compared to Dawny33\n",
    "print(KL(values1, values2)) # 0.775278939433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46db69a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence: 0.2284\n"
     ]
    }
   ],
   "source": [
    "def calculate_kl_divergence_binary(teacher_probs, y_true):\n",
    "    \"\"\"\n",
    "    Calculate KL divergence between teacher predictions and ground truth for binary classification.\n",
    "    \n",
    "    Parameters:\n",
    "        teacher_probs: 2D array with shape (n_samples, 2) - [prob_class_0, prob_class_1]\n",
    "        y_true: 1D array with true labels (0 or 1)\n",
    "    \"\"\"\n",
    "    eps = 1e-8\n",
    "    \n",
    "    # Create one-hot encoded ground truth\n",
    "    y_true_onehot = np.zeros((len(y_true), 2))\n",
    "    y_true_onehot[np.arange(len(y_true)), y_true] = 1\n",
    "    \n",
    "    # Clip teacher probabilities to avoid log(0)\n",
    "    teacher_probs_safe = np.clip(teacher_probs, eps, 1 - eps)\n",
    "    \n",
    "    # Calculate KL divergence for each sample\n",
    "    kl_divs = []\n",
    "    for i in range(len(y_true)):\n",
    "        # KL(true || predicted) = sum(true * log(true / predicted))\n",
    "        kl = np.sum(y_true_onehot[i] * np.log(y_true_onehot[i] + eps) - \n",
    "                    y_true_onehot[i] * np.log(teacher_probs_safe[i]))\n",
    "        kl_divs.append(kl)\n",
    "    \n",
    "    return np.mean(kl_divs)\n",
    "\n",
    "# Test with binary classification data\n",
    "teacher_probs = np.array([[0.3, 0.7], [0.8, 0.2], [0.1, 0.9]])  # Teacher predictions\n",
    "y_true = np.array([1, 0, 1])  # Ground truth labels\n",
    "\n",
    "kl_div = calculate_kl_divergence_binary(teacher_probs, y_true)\n",
    "print(f\"KL Divergence: {kl_div:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c3bd9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence: 0.2284\n",
      "KL Divergence (simple): 0.2284\n"
     ]
    }
   ],
   "source": [
    "def calculate_kl_divergence_binary_single_prob(teacher_probs_pos, y_true):\n",
    "    \"\"\"\n",
    "    Calculate KL divergence between teacher predictions and ground truth for binary classification.\n",
    "    \n",
    "    Parameters:\n",
    "        teacher_probs_pos: 1D array with probabilities for positive class (shape: n_samples,)\n",
    "        y_true: 1D array with true labels (0 or 1)\n",
    "    \"\"\"\n",
    "    eps = 1e-8\n",
    "    \n",
    "    # Clip probabilities to avoid log(0)\n",
    "    p_pos = np.clip(teacher_probs_pos, eps, 1 - eps)\n",
    "    p_neg = 1 - p_pos\n",
    "    \n",
    "    # Calculate KL divergence for each sample\n",
    "    kl_divs = []\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == 1:  # True label is positive class\n",
    "            # KL(true || predicted) = 1 * log(1 / p_pos) = -log(p_pos)\n",
    "            kl = -np.log(p_pos[i])\n",
    "        else:  # True label is negative class\n",
    "            # KL(true || predicted) = 1 * log(1 / p_neg) = -log(p_neg)\n",
    "            kl = -np.log(p_neg[i])\n",
    "        kl_divs.append(kl)\n",
    "    \n",
    "    return np.mean(kl_divs)\n",
    "\n",
    "# Test with your data format\n",
    "teacher_probs_pos = np.array([0.7, 0.2, 0.9])  # Just positive class probabilities\n",
    "y_true = np.array([1, 0, 1])  # Ground truth labels\n",
    "\n",
    "kl_div = calculate_kl_divergence_binary_single_prob(teacher_probs_pos, y_true)\n",
    "print(f\"KL Divergence: {kl_div:.4f}\")\n",
    "\n",
    "# This is equivalent to negative log-likelihood!\n",
    "# You can also use this even simpler version:\n",
    "def simple_kl_binary(probs_pos, y_true):\n",
    "    eps = 1e-8\n",
    "    probs_pos = np.clip(probs_pos, eps, 1 - eps)\n",
    "    \n",
    "    # For binary classification with one-hot ground truth, KL divergence = negative log-likelihood\n",
    "    log_likelihood = y_true * np.log(probs_pos) + (1 - y_true) * np.log(1 - probs_pos)\n",
    "    return -np.mean(log_likelihood)\n",
    "\n",
    "kl_simple = simple_kl_binary(teacher_probs_pos, y_true)\n",
    "print(f\"KL Divergence (simple): {kl_simple:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
